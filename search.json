[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "All Blog Posts",
    "section": "",
    "text": "Subscribe to the RSS feed to get updates on new posts.\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\ncreatelang.rs: Six Years Later, It‚Äôs Done\n\n\nAfter almost six years, my book on creating programming languages with Rust is finally complete. Here‚Äôs what that journey looked like.\n\n\n\n\n\nDec 31, 2025\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\nA Geometric Framework for Undefined Behavior: From a 2019 Tweet to an Actual Paper\n\n\nHow a random thought about using algebraic topology to understand undefined behavior turned into a real paper.\n\n\n\n\n\nDec 24, 2025\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\nMojo GPU Puzzles Edition 1\n\n\nA hands-on guide that teaches GPU programming through 34 progressive challenges - learn by doing, not lectures.\n\n\n\n\n\nOct 29, 2025\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\nPaged Attention & Prefix Caching Now Available in MAX Serve\n\n\nAnnouncing state-of-the-art optimizations for LLM inference in MAX Serve - originally published on Modular‚Äôs blog.\n\n\n\n\n\nFeb 6, 2025\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\nAgentic Building Blocks: Creating AI Agents with MAX Serve and OpenAI Function Calling\n\n\nLearn how to build AI agents by integrating MAX Serve with OpenAI‚Äôs function calling capabilities - originally published on Modular‚Äôs blog.\n\n\n\n\n\nJan 30, 2025\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\nHands-on with Mojo 24.6\n\n\nA practical walkthrough of the new features in Mojo 24.6 - originally published on Modular‚Äôs blog.\n\n\n\n\n\nJan 21, 2025\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\nBuild a Continuous Chat Interface with Llama 3 and MAX Serve\n\n\nA step-by-step guide to building a chat application using Llama 3 and MAX Serve - originally published on Modular‚Äôs blog.\n\n\n\n\n\nDec 17, 2024\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\nPaged Attention & Prefix Caching Now Available in MAX Serve\n\n\nAnnouncing state-of-the-art optimizations for LLM inference in MAX Serve - originally published on Modular‚Äôs blog.\n\n\n\n\n\nNov 1, 2024\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\nHands-on with Mojo 24.5\n\n\nA practical walkthrough of the new features in Mojo 24.5 - originally published on Modular‚Äôs blog.\n\n\n\n\n\nOct 1, 2024\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\nWhat‚Äôs New in MAX 24.4? MAX on macOS, Fast Local Llama3, Native Quantization and GGUF Support\n\n\nExploring the new features in MAX 24.4 including macOS support and Llama3 - originally published on Modular‚Äôs blog.\n\n\n\n\n\nJun 25, 2024\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\nWhat Ownership is Really About: A Mental Model Approach\n\n\nA deep dive into understanding ownership in programming through mental models - originally published on Modular‚Äôs blog.\n\n\n\n\n\nMay 29, 2024\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\nSemantic Search with MAX Engine\n\n\nBuilding semantic search applications using MAX Engine - originally published on Modular‚Äôs blog.\n\n\n\n\n\nMar 21, 2024\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\nModal Labs Deep Dive\n\n\n\n\n\n\n\n\nDec 8, 2023\n\n26 min\n\n\n\n\n\n\n\n\n\n\n\nThe Core of Attention is Communication\n\n\n\n\n\n\n\n\nNov 23, 2023\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\nRust and Node.js: Harmonizing Performance and Safety\n\n\n\n\n\n\n\n\nNov 19, 2023\n\n13 min\n\n\n\n\n\n\n\n\n\n\n\nNotes on the Current State of LLM Frameworks\n\n\n\n\n\n\n\n\nAug 13, 2023\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\nAnnouncement üì¢ Releasing dlpackrs\n\n\n\n\n\n\n\n\nSep 20, 2022\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\nAnnouncement üì¢ Releasing smartalloc\n\n\n\n\n\n\n\n\nSep 7, 2022\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\nAnnouncement üì¢ Create your own programming language with Rust\n\n\n\n\n\n\n\n\nJun 8, 2020\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\nRust std study series: Pin\n\n\n\n\n\n\n\n\nAug 16, 2019\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\nRust std study series: alloc\n\n\n\n\n\n\n\n\nJul 3, 2019\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\nRust std study series: Interior mutability\n\n\n\n\n\n\n\n\nJun 18, 2019\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\nThe State of Machine Learning in Rust\n\n\n\n\n\n\n\n\nMay 13, 2019\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\nRust std study series: VecDeque\n\n\n\n\n\n\n\n\nApr 28, 2019\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\nRust std study series: LinkedList\n\n\n\n\n\n\n\n\nMar 25, 2019\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\nVariance in Rust: An intuitive explanation\n\n\n\n\n\n\n\n\nMar 16, 2019\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\nRust std study series: Vec\n\n\n\n\n\n\n\n\nMar 10, 2019\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\nNIPS, AI hype and the lost rigor\n\n\n\n\n\n\n\n\nDec 6, 2017\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\nWhat‚Äôs up with word embedding?\n\n\n\n\n\n\n\n\nJul 12, 2017\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\nFrom Machine Learning to Formal Math and Type Theory\n\n\n\n\n\n\n\n\nJun 30, 2017\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\nMy M.Sc. Thesis: Distributed Linear Programming with Apache Spark\n\n\nI finally defended my M.Sc. thesis on solving large-scale linear programming problems using Apache Spark. Here‚Äôs the full thesis and some thoughts on the journey.\n\n\n\n\n\nJan 21, 2017\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\nGeneral Monty Hall Simulation\n\n\n\n\n\n\n\n\nMay 5, 2016\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\nRestaurant Revenue Prediction with BART Machine\n\n\n\n\n\n\n\n\nSep 13, 2015\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\nMy Math M.Sc. Thesis: Hilbert Schemes and the Derived McKay Correspondence\n\n\nMy first M.Sc. thesis on the behavior of the Hilbert scheme of points under the derived McKay correspondence. Pure math, derived categories, and beautiful combinatorics.\n\n\n\n\n\nSep 9, 2013\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\nVector Bundles, Locally Free Sheaves and Divisors on a Curve\n\n\n\n\n\n\n\n\nDec 6, 2012\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\nClassification of Vector Bundles on Elliptic Curves\n\n\n\n\n\n\n\n\nNov 30, 2012\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\nSome Homological Algebra Computations\n\n\n\n\n\n\n\n\nNov 26, 2012\n\n4 min\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2015-09-13-restaurant-revenue-prediction-with-bart-machine/index.html",
    "href": "posts/2015-09-13-restaurant-revenue-prediction-with-bart-machine/index.html",
    "title": "Restaurant Revenue Prediction with BART Machine",
    "section": "",
    "text": "In this post, I‚Äôd like to show you how to use the newly written package onBayesian Additive Regression Treesi.e.BART Machine for Restaurant Revenue Prediction withR. The datasets are part of the passed Kaggle competition that can be found here.What is BART?BART is the Bayesian sibling of Random Forest. For some applications, Bayesian approaches with probabilistic assumptions work better than pure algorithmic ensemble of trees due to underlyingprior assumptions. BART utilizes prior assumptions for parameters estimate and splits that occur in classic tree models. There are multiple distinctions between BART and Random Forest. One is the user defined ability to set generalized Bernoulli priors on features to be selected for each split, whereas in Random Forest, this prior is uniform on features. The other ones are having multiple hyperparameters controlling the growth of trees more than Random Forest and the ability to findexplicit interactionsamong features, which can hardly be found in general Machine/Statistical learning algorithms/models.BART in actionSpeaking of competition, TFI has been one of the most unusual Kaggle competitions so far and has confused and hurt many contestants as well as myself because of the following reasons:\nGreat disparity among the number of training and testing samples. The training set has 137 samples with 37 obfuscated features out of 42 with therevenueas the response variable. The test set has 100,000 samples (poisoned data and robust model indications!).\nIt was very hard to get consistent cross-validation scores even with repeated CV.\nAlmost all of the features are weak and some are incompatible in train and test sets.Manyof the top seed Kaggle Masters did poorly in this competition which adds to its strangeness and perhaps if you knew less, you could have been more successful in this competition! So, let‚Äôs briefly look at the data first;\nAs you go through exploringalldataset, you will figure out many odd cases, like the featureCity, the number of unique cities used in training set length(unique(data$City)) is 34 whereas the number of unique cities used in test set is 57. You can check the forum to see more odd cases. Of course, these cases could happen in real data science and everyone should now how to handle them. This post, however, does not deal with that kind of behaviors. By plotting the histogram ofrevenue, one can see its skewness easily, so the natural log(revenue) better demonstrates its (nearly) log normal distribution and perhaps suggests existence ofoutliers.\nTo plot the correlations among (numeric) features and the response, we can use the corrplot package\nBefore going further, I‚Äôd like to emphasize that when the data is small, it‚Äôs very likely to find some seemingly good complicatedinteractionsamong features, for example, by looking at the p-values for a simple linear regression and by tweaking couple of significant ones, I found a (good-turn-out-to-be-useless) interaction between logRevenue~I(P6*P20)+P28+I(P28^2)+Age+I(Age^2) , includingAgeof the restaurant, which at the time, I thought it might be a good feature to add, but the deserved winner argued the opposite! Now, let‚Äôs dig into bartMachine. First, I‚Äôm going to throw out the incompatible features includingOpen.Date, City, City.Group, Typeas well asIdto make our training and testing sets unified and initialize the bartMachine as follows;\nThe results are &gt; bartMachine v1.2.0 for regression &gt; training data n = 137 and p = 37 &gt; built in 0.3 secs on 4 cores, 10 trees, 250 burn-in and 1000 post. samples &gt; sigsq est for y beforehand: 0.159 &gt; avg sigsq estimate after burn-in: 0.19178 &gt; in-sample statistics: &gt; L1 = 41.5 &gt; L2 = 22.15 &gt; rmse = 0.4 &gt; Pseudo-Rsq = 0.2953 &gt; p-val for shapiro-wilk test of normality of residuals: 0.08099 &gt; p-val for zero-mean noise: 0.97545\nWe can also see the effect of changing the num_trees parameter by\nSo perhaps num_trees=20 works better with less out-of-sample error. As,bartMachineis essentially doing MCMC and heavily using Gibbs sampler, checking convergence seems necessary (see the above paper for more details);\nWe can also check mean-centeredness assumption and heteroskedasticity of the data by the QQ-plot\nOne of the main features of bartMachine is showing variable importance via permutation tests\nMoreover, bartMachine is capable of finding explicit interactions among feature\nHowever, since we want a simple model to start and score well in the poisoned test set, focusing too much on interactions can hurt a lot, as happened to me by inevitable overfitting. Now, I‚Äôm going to pickP20, P17, P6and retrain my model and perform a CV and finally submit it.\nThe result of this submission, is 1731437.14026 in public and 1839841.77388 (ranks around700out of 2257 teams) in private leaderboards."
  },
  {
    "objectID": "posts/2015-09-13-restaurant-revenue-prediction-with-bart-machine/index.html#rank-below-100-out-of-2257",
    "href": "posts/2015-09-13-restaurant-revenue-prediction-with-bart-machine/index.html#rank-below-100-out-of-2257",
    "title": "Restaurant Revenue Prediction with BART Machine",
    "section": "Rank below 100 out of 2257:",
    "text": "Rank below 100 out of 2257:\nTo improve our model and make it more robust, we should remove outliers by deciding what threshold to use perhaps with some trial-and-error first, though there‚Äôre many methods that can be helpful, which is not within the scope of this post. Looking at the log(revenue) distribution should suggest where you should put the threshold. Doing so, it can hugely boost your private leaderboard ranking to position around80with score 1786605.94364 which is a fairly good incentive to explore bartMachine more. You can find the codes including theimproved modeland plots in my github. Enjoy! P.S. During the competition, the BAYZ,M.D team managed to make aperfect overfiton the public leaderboard with interesting and creative methods. You can read more about that here."
  },
  {
    "objectID": "posts/2012-12-06-vector-bundles-locally-free-sheaves-and-divisors-on-a-curve/index.html",
    "href": "posts/2012-12-06-vector-bundles-locally-free-sheaves-and-divisors-on-a-curve/index.html",
    "title": "Vector Bundles, Locally Free Sheaves and Divisors on a Curve",
    "section": "",
    "text": "In this post, I‚Äôll be summarizing the basics of the correspondence between vector bundles, locally free sheaves and divisors on a smooth curve (defined over an algebraically closed field \\(k\\) of characteristic zero) together with some of their individual properties."
  },
  {
    "objectID": "posts/2012-12-06-vector-bundles-locally-free-sheaves-and-divisors-on-a-curve/index.html#locally-free-sheaves-and-vector-bundles",
    "href": "posts/2012-12-06-vector-bundles-locally-free-sheaves-and-divisors-on-a-curve/index.html#locally-free-sheaves-and-vector-bundles",
    "title": "Vector Bundles, Locally Free Sheaves and Divisors on a Curve",
    "section": "Locally Free Sheaves and Vector Bundles",
    "text": "Locally Free Sheaves and Vector Bundles\n\n\n\n\n\n\nNoteProposition 1\n\n\n\n\nA coherent sheaf \\(\\mathcal{E}\\) on a curve \\(C\\) is locally free \\(\\iff\\) the fibers (stalks) \\(\\mathcal{E}_p\\) are free at every point \\(p \\in C.\\) (Note that the statement is local.)\nA subsheaf of a locally free sheaf is locally free. (Use the fact that local rings of a smooth curve are DVR, hence PID and a submodule of a finitely generated free module is free.)\nA non-zero map from a rank one locally free sheaf to a locally free sheaf is injective. (If there is a non-zero kernel, by (b) it is locally free of rank one, then the cokernel will be a torsion sheaf injecting to a locally free sheaf!)\nLet \\(\\mathcal{E}\\) be a locally free sheaf of rank \\(r\\) and \\(\\mathcal{E}'\\) a subsheaf of rank \\(r'.\\) There exists a subsheaf \\(\\mathcal{E}''\\) of rank \\(r''\\) containing \\(\\mathcal{E}'\\) s.t. \\(\\mathcal{E}/\\mathcal{E}''\\) is locally free. In particular, if \\(\\mathcal{E}'\\) is maximal (w.r.t inclusion) the quotient is already locally free. (Think how to kill the torsion!)\n\n\n\nTheorem: There is a natural one-to-one correspondence between vector bundles (of rank \\(r\\)) and locally free sheaves (of rank \\(r\\)) on \\(C.\\)\nIdea: Given a vector bundle, the sheaf of sections is the corresponding sheaf. The converse is a little tricky. Given a locally free sheaf \\(\\mathcal{E},\\) one can define a vector bundle as follows\n\\[E:= \\{ (p,t)\\mid p \\in C, \\;\\; t \\in \\mathcal{E}_p/{\\mathfrak{m}_p \\mathcal{E}_p} \\}\\]\nwhere \\(\\mathfrak{m}_p\\) is the unique maximal ideal of the local ring \\(\\mathcal{O}_p.\\)\nRemark: Subsheaves of a locally free sheaf do not necessarily correspond to the subbundles of its associated bundle. The point is that injectivity of the map of locally free sheaves may fail to be injective when it is reduced modulo the maximal ideal of some point.\nLemma 1: A non-trivial global section of a vector bundle \\(E\\) corresponds to a non-zero map of sheaves \\(\\mathcal{O} \\to \\mathcal{E}\\) where \\(\\mathcal{E}\\) is the sheaf associated to \\(E.\\)"
  },
  {
    "objectID": "posts/2012-12-06-vector-bundles-locally-free-sheaves-and-divisors-on-a-curve/index.html#divisors",
    "href": "posts/2012-12-06-vector-bundles-locally-free-sheaves-and-divisors-on-a-curve/index.html#divisors",
    "title": "Vector Bundles, Locally Free Sheaves and Divisors on a Curve",
    "section": "Divisors",
    "text": "Divisors\nA Weil divisor on \\(C\\) is a finite formal sum of points with integral multiplicities. If \\(f\\) is a rational function on \\(C\\) the divisor corresponding to \\(f\\) is defined as \\((f)=\\sum \\text{ord}_p(f) \\cdot p.\\)\nA Cartier divisor on \\(C\\) is given by a covering \\(U_i\\) of \\(C\\) together with functions \\(f_i \\in \\mathcal{O}(U_i)\\) s.t. \\(f_i/f_j\\) are invertible on \\(U_i \\cap U_j.\\) A principal divisor is given by the cover consisting of \\(C\\) alone and a function on \\(C\\) (global section of \\(\\mathcal{O}_C\\)).\nGiven a Cartier divisor, one can associate a Weil divisor to it by considering on each open set \\(U_i\\) the zeros minus the poles of \\((f_i)\\) and this is well-defined, since \\(f_i/f_j\\) is invertible on \\(U_i \\cap U_j.\\) Conversely, given a Weil divisor \\(D,\\) one can construct a Cartier divisor by choosing open sets that contain at most one of the points on the support of \\(D\\) and functions that vanish at these points with the assigned multiplicities.\nTherefore, these two seemingly different notions are equivalent over a smooth curve \\(C.\\)"
  },
  {
    "objectID": "posts/2012-12-06-vector-bundles-locally-free-sheaves-and-divisors-on-a-curve/index.html#line-bundles-locally-free-sheaves-of-rank-one-and-divisors",
    "href": "posts/2012-12-06-vector-bundles-locally-free-sheaves-and-divisors-on-a-curve/index.html#line-bundles-locally-free-sheaves-of-rank-one-and-divisors",
    "title": "Vector Bundles, Locally Free Sheaves and Divisors on a Curve",
    "section": "Line Bundles (locally free sheaves of rank one) and Divisors",
    "text": "Line Bundles (locally free sheaves of rank one) and Divisors\nGiven a Cartier divisor, one can define a locally free sheaf of rank one by taking the trivial sheaf \\(\\mathcal{O}(U_i)\\) and gluing them by the isomorphisms \\(f_i/f_j\\) on \\(U_i \\cap U_j.\\)\nConversely, given an invertible sheaf and a trivialization \\(\\{ U_i, f_{i,j} \\},\\) one can define a Cartier divisor as follows: take an arbitrary open set, say \\(U_0\\) and define a Cartier divisor with \\((U_i,f_{i,0}),\\) as \\(f_{j,0}=f^{-1}_{0,j}\\) and \\(f_{0,j}f_{i,0}=f_{i,j}\\) which is a unit on \\(U_i \\cap U_j.\\)\nDefinition: Let \\(D=\\sum n_p p\\) be an effective divisor. Define the sheaf \\(\\mathcal{F}\\) on the support of \\(D\\) by \\(\\mathcal{F}_p=k^{n_p}.\\) Now, define the skyscraper sheaf \\(k_D\\) as the extension by zero outside of \\(D.\\)\nLemma 2: A line bundle \\(\\mathcal{L}\\) corresponds to an effective divisor \\(D\\) if and only if \\(\\mathcal{L}\\) has a non-zero global section (by Lemma 1, there is a non-zero map of sheaves \\(\\mathcal{O} \\to \\mathcal{L}\\) which is injective by Proposition 1(c)). In this case, one has the following short exact sequence of sheaves\n\\[0 \\longrightarrow \\mathcal{O} \\longrightarrow \\mathcal{L} \\longrightarrow k_D \\longrightarrow 0\\]\nOne then writes \\(\\mathcal{L}=\\mathcal{O}(D).\\)"
  },
  {
    "objectID": "posts/2012-12-06-vector-bundles-locally-free-sheaves-and-divisors-on-a-curve/index.html#riemann-roch-theorem",
    "href": "posts/2012-12-06-vector-bundles-locally-free-sheaves-and-divisors-on-a-curve/index.html#riemann-roch-theorem",
    "title": "Vector Bundles, Locally Free Sheaves and Divisors on a Curve",
    "section": "Riemann-Roch Theorem",
    "text": "Riemann-Roch Theorem\nIf \\(\\mathcal{L}\\) is an invertible sheaf (line bundle), then the Euler-Poincar√© characteristic can be computed as\n\\[\\chi(\\mathcal{L})=\\deg(\\mathcal{L})+\\chi(\\mathcal{O})=\\deg(\\mathcal{L})+1-g\\]\nwhere \\(g=h^1(\\mathcal{O})\\) is the genus of the curve.\nProof: First, assume that \\(\\mathcal{L}\\) corresponds to an effective divisor \\(D,\\) then by Lemma 2, there is a short exact sequence, thus the associated long exact sequence gives rise to \\(\\chi(\\mathcal{O}(D))= \\chi (\\mathcal{O})+ \\chi (k_D).\\) Since \\(k_D\\) is a skyscraper sheaf, its support is a zero-dimensional set, hence \\(h^1(k_D)=0\\) and \\(\\chi(k_D)=h^0(k_D)=\\deg D,\\) so the result follows.\nIn the general case, assume that \\(\\mathcal{L}\\) corresponds to \\(D-D'\\) with both \\(D,D'\\) effective. Then \\(\\mathcal{O}(D-D')=\\mathcal{O}(D) \\otimes \\mathcal{O}(-D')\\) where \\(\\mathcal{O}(-D')=\\mathcal{O}(D')^*.\\) Indeed, \\(\\mathcal{O}(-D')\\) is a locally free sheaf so is flat and tensoring the above short exact sequence with \\(\\mathcal{O}(-D')\\) one obtains\n\\[0\\longrightarrow \\mathcal{O}(-D') \\longrightarrow \\mathcal{L} \\longrightarrow k_D \\otimes \\mathcal{O}(-D') \\longrightarrow 0\\]\nTherefore, \\(\\chi(\\mathcal{L})= \\chi (\\mathcal{O}(-D'))+ \\chi(k_D \\otimes \\mathcal{O}(-D')).\\) On the other hand, replacing \\(D\\) by \\(D'\\) in the above sequence and tensoring with \\(\\mathcal{O}(-D')\\) and doing the same thing as above leads to the desired formula.\n\nLemma 3: Given a locally free sheaf \\(\\mathcal{E}\\) of rank \\(r\\) there exists a short exact sequence of sheaves\n\\[0 \\longrightarrow \\mathcal{L} \\longrightarrow \\mathcal{E} \\longrightarrow \\mathcal{E}' \\longrightarrow 0\\]\nwhere \\(\\mathcal{L}\\) is an invertible sheaf and \\(\\mathcal{E}'\\) a locally free sheaf of rank \\(r-1.\\)\nUsing Lemma 3 and induction, one can prove the following formula for the calculation of \\(\\deg(\\mathcal{E}_1 \\otimes \\mathcal{E}_2)\\) where \\(\\mathcal{E}_1, \\mathcal{E}_2\\) are locally free sheaves of rank \\(r_1, r_2\\) respectively.\n\\[\\deg(\\mathcal{E}_1 \\otimes \\mathcal{E}_2)=r_1 \\deg(\\mathcal{E}_2) + r_2 \\deg(\\mathcal{E}_1)\\]\nwhere the degree of a locally free sheaf \\(\\mathcal{E}\\) of rank \\(r\\) is defined by\n\\[\\deg(\\mathcal{E})=\\chi(\\mathcal{E})-r \\chi (\\mathcal{O})\\]\n\nDefinition: A sheaf \\(\\mathcal{F}\\) is said to be generated by global sections if the natural map\n\\[H^0(\\mathcal{F}) \\otimes \\mathcal{O} \\longrightarrow \\mathcal{F}\\]\nis onto.\n\n\n\n\n\n\nNoteProposition 2\n\n\n\nIf \\(\\mathcal{E}\\) is a locally free sheaf on \\(C,\\) there exists a positive divisor \\(D\\) on \\(C\\) s.t. \\(\\mathcal{E}(D):=\\mathcal{E} \\otimes \\mathcal{O}(D)\\) is generated by global sections."
  },
  {
    "objectID": "posts/2012-12-06-vector-bundles-locally-free-sheaves-and-divisors-on-a-curve/index.html#atiyahs-theorem",
    "href": "posts/2012-12-06-vector-bundles-locally-free-sheaves-and-divisors-on-a-curve/index.html#atiyahs-theorem",
    "title": "Vector Bundles, Locally Free Sheaves and Divisors on a Curve",
    "section": "Atiyah‚Äôs Theorem",
    "text": "Atiyah‚Äôs Theorem\nGiven a locally free sheaf \\(\\mathcal{E}\\) of rank \\(r\\) generated by global sections, there exists an exact sequence\n\\[0 \\longrightarrow \\mathcal{O}^{r-1} \\longrightarrow \\mathcal{E} \\longrightarrow \\mathcal{L} \\longrightarrow 0\\]\nwhere \\(\\mathcal{L}\\) is an invertible sheaf."
  },
  {
    "objectID": "posts/2019-06-18-rust-std-study-series-interior-mutability/index.html",
    "href": "posts/2019-06-18-rust-std-study-series-interior-mutability/index.html",
    "title": "Rust std study series: Interior mutability",
    "section": "",
    "text": "Continuing the standard library study, it‚Äôs time for Cell&lt;T&gt;!\nRust compiler enforcesmultiple readsaccess and asingle writeaccess mutually exclusive, i.e.¬†either multiple shared references & or one and only one mutable reference & mut. So essentially, Rust prevents the evil ofaliasing and mutation between multiple threads.\nCell&lt;T&gt; is asharable mutable containerdesigned carefully to prevent stepping into the UB land. Note thatunsafe castof & to & mut isimmediate UB, so Cell was designed to manage/catch such UB at compile time. This container behaves like & allowing & mut. However, there‚Äôs a distinction between a single threaded behavior and multi-threaded one:Single threaded: Cell&lt;T&gt; (ownsa values) and RefCell&lt;T&gt; (borrowsa value withruntimecost).Multi-threaded: Mutex&lt;T&gt;, RwLock&lt;T&gt;, etc. (synchronization primitives)\nThe essence of Cell can be understood in terms ofVarianceand the compiler support which we explored before. As usual, std has a great explanations in particular, an enlightening example in mutating implementations of Clone that manages the side-effect of Rc&lt;T&gt; via Cell. In other words, animmutable structwith a Cell ed field allows mutation for that field.\nThere core primitive for interior mutability is UnsafeCell&lt;T&gt; which to my knowledge is the onlyinvariant container."
  },
  {
    "objectID": "posts/2019-06-18-rust-std-study-series-interior-mutability/index.html#cell",
    "href": "posts/2019-06-18-rust-std-study-series-interior-mutability/index.html#cell",
    "title": "Rust std study series: Interior mutability",
    "section": "Cell",
    "text": "Cell\n#[lang = \"unsafe_cell\"] // &lt;-- invariant type support\n#[repr(transparent)] // &lt;-- enforces the same type repr as the underlying `T`\npub struct UnsafeCell&lt;T: ?Sized&gt; {\n    value: T,\n}\n// UnsafeCell is not Sync, hence \"single-threadedness\".\n// In other words, it cannot share references between multiple threads \nimpl&lt;T: ?Sized&gt; !Sync for UnsafeCell&lt;T&gt; {}\n\nimpl&lt;T: ?Sized&gt; UnsafeCell&lt;T&gt; {\n    pub const fn get(&self) -&gt; *mut T {\n        // We can just cast the pointer from `UnsafeCell&lt;T&gt;` to `T` because of\n        // #[repr(transparent)]\n        self as *const UnsafeCell&lt;T&gt; as *const T as *mut T\n    }\n}\n#[repr(transparent)] // &lt;-- so the final type repr is the same as `T`\npub struct Cell&lt;T: ?Sized&gt; {\n    value: UnsafeCell&lt;T&gt;,\n}\n\nimpl&lt;T: Copy&gt; Cell&lt;T&gt; { // &lt;-- note that `T` must be `Copy`\n    pub fn get(&self) -&gt; T {\n        unsafe{ *self.value.get() }\n    }\n}\n\nimpl&lt;T&gt; Cell&lt;T&gt; {\n    // compile time guarantee that we process the ONLY one reference\n    pub fn get_mut(&mut self) -&gt; &mut T {\n        unsafe {\n            &mut *self.value.get()\n        }\n    }\n    \n    pub fn set(&self, val: T) {\n        let old = self.replace(val);\n        drop(old); // &lt;-- drops the old value\n    }\n}\n\n// Uphold the assumption of the wrapped `UnsafeCell`\nimpl&lt;T: ?Sized&gt; !Sync for Cell&lt;T&gt; {}\n// If `T` can be transferred across thread boundaries, so does `Cell`\nunsafe impl&lt;T: ?Sized&gt; Send for Cell&lt;T&gt; where T: Send {}\n\nNotable propertiesCell‚Äôs get method is definedonlyfor Copy i.e.bit-wise copyable, otherwise cannot move of the borrowed &self.Only when the underlying type is Copy it is possible to clone the Cell.Cell‚Äôs swap method is just a pointer swap and the difference between this and mem::swap is Cell‚Äôs swap doesn‚Äôt need & mut.Cell‚Äôs take method, takes the value of the cell and leaves Default::default value in its place.Cell‚Äôs replace method uses mem::replace to replace the new value and return the old value.Cell&lt;T&gt; is Send + !Sync (given T: Send), meaning it is safe to transfer Cell between multiple threads (when the underlying value allows us) butreferencesof the Cellcannot be sharedbetween multiple threads.\n\n\nRefCell\n\nA mutable memory location with dynamically checked borrow rules\nRust std doc\n\n// Positive values shows the number of active `Ref`\n// Negative values shows the number of active `RefMut`\ntype BorrowFlag = isize;\n\npub struct RefCell&lt;T: ?Sized&gt; {\n    borrow: Cell&lt;BorrowFlag&gt;, // &lt;-- subtle\n    value: UnsafeCell&lt;T&gt;,\n}\n\nunsafe impl&lt;T: ?Sized&gt; Send for RefCell&lt;T&gt; where T: Send {}\nimpl&lt;T: ?Sized&gt; !Sync for RefCell&lt;T&gt; {}\n\nstruct BorrowRef&lt;'b&gt; {\n    borrow: &'b Cell&lt;BorrowFlag&gt;,\n}\n\n/// A wrapper type for an immutably borrowed value from a `RefCell&lt;T&gt;`\npub struct Ref&lt;'b, T: ?Sized + 'b&gt; {\n    value: &'b T,\n    borrow: BorrowRef&lt;'b&gt;,\n}\n\nstruct BorrowRefMut&lt;'b&gt; {\n    borrow: &'b Cell&lt;BorrowFlag&gt;,\n}\n\n/// A wrapper type for a mutably borrowed value from a `RefCell&lt;T&gt;`\npub struct RefMut&lt;'b, T: ?Sized + 'b&gt; {\n    value: &'b mut T,\n    borrow: BorrowRefMut&lt;'b&gt;,\n}\n\nRef and RefMut are both two words in size, and so there will likely never be enough Refs or RefMuts in existence to overflow half of the usize range. Thus, a BorrowFlag will probably never overflow or underflow. However, this is not a guarantee, as a pathological program could repeatedly create and then mem::forget Refs or RefMuts. Thus, all code must explicitly check for overflow and underflow in order to avoid unsafety, or at least behave correctly in the event that overflow or underflow happens.\nInternal std doc of BorrowFlag\n\nOne important distinction between RefCell and Cell is the ability totry_borrow (and the panicking versionborrow):It immutably borrows the wrapped value, returning an error if the value is currently mutably borrowed.The borrow lasts until the returned Ref exits scope. try_borrow_mut (panicking version borrow_mut):Mutably borrows the wrapped value, returning an error if the value is currently borrowed. The borrow lasts until the returned RefMut or all RefMuts derived from it exit scope. The value cannot be borrowed while this borrow is active.We‚Äôve covered most of details and important points of how managed interior mutability is possible in Rust."
  },
  {
    "objectID": "posts/2024-10-01-hands-on-with-mojo-24-5/index.html",
    "href": "posts/2024-10-01-hands-on-with-mojo-24-5/index.html",
    "title": "Hands-on with Mojo 24.5",
    "section": "",
    "text": "I wrote a blog post on the Modular blog exploring the features introduced in Mojo 24.5.\nThis article provides practical insights into applying the new language features in your code to improve performance and maintainability.\nKey topics covered:\n\nOverview of Mojo 24.5 release highlights\nNew language features and syntax improvements\nMemory and performance enhancements\nPractical examples and best practices\nMigration tips from previous versions\n\nRead the full article: Hands-on with Mojo 24.5"
  },
  {
    "objectID": "posts/2025-02-06-paged-attention-prefix-caching-max-serve/index.html",
    "href": "posts/2025-02-06-paged-attention-prefix-caching-max-serve/index.html",
    "title": "Paged Attention & Prefix Caching Now Available in MAX Serve",
    "section": "",
    "text": "I wrote a blog post on the Modular blog announcing the integration of Paged Attention and Prefix Caching into MAX Serve.\nThese features bring state-of-the-art optimizations to LLM inference, significantly improving computational efficiency and memory management.\nKey topics covered:\n\nWhat is Paged Attention and why it matters\nHow Prefix Caching improves inference performance\nAddressing computational challenges in Multi-Head Attention\nInstallation and setup instructions\nCommands for enabling these optimizations\nPerformance benchmarks and improvements\n\nRead the full article: Paged Attention & Prefix Caching Now Available in MAX Serve"
  },
  {
    "objectID": "posts/2024-11-01-paged-attention-prefix-caching-max-serve/index.html",
    "href": "posts/2024-11-01-paged-attention-prefix-caching-max-serve/index.html",
    "title": "Paged Attention & Prefix Caching Now Available in MAX Serve",
    "section": "",
    "text": "I wrote a blog post on the Modular blog announcing the integration of Paged Attention and Prefix Caching into MAX Serve.\nThese features bring state-of-the-art optimizations to LLM inference, significantly improving computational efficiency and memory management.\nKey topics covered:\n\nWhat is Paged Attention and why it matters\nHow Prefix Caching improves inference performance\nAddressing computational challenges in Multi-Head Attention\nInstallation and setup instructions\nCommands for enabling these optimizations\nPerformance benchmarks and improvements\n\nRead the full article: Paged Attention & Prefix Caching Now Available in MAX Serve"
  },
  {
    "objectID": "posts/2019-04-28-rust-std-study-series-vecdeque/index.html",
    "href": "posts/2019-04-28-rust-std-study-series-vecdeque/index.html",
    "title": "Rust std study series: VecDeque",
    "section": "",
    "text": "Continuing from Rust standard library study series, it‚Äôs time for VecDeque. Because of its similarity to Vec, there isn‚Äôt much to say.\n\nAdouble-endedqueueimplemented with agrowable ring buffer.\nThe ‚Äúdefault‚Äù usage of this type as a queue is to use push_back toaddto the queue, and pop_front toremovefrom the queue. extend and append push onto the back in this manner, and iterating over VecDeque goes front to back.\nRust std docSimilar to Vec, VecDeque has amortized \\(O(1)\\) insert to both ends of the container, but unlike Vec, it has amortized \\(O(1)\\) removal from both ends. (Recalling from Vec study, removal is strictly \\(O(1)\\) with no shrink factor involved)Similar to Vec, indexing is \\(O(1).\\)\n\nSimilar to Vec study, here‚Äôs the stripped down definition of VecDeque&lt;T&gt;\nstruct VecDeque&lt;T&gt; {\n    // tail and head are pointers into the buffer. Tail always points\n    // to the first element that could be read, Head always points\n    // to where data should be written.\n    // If tail == head the buffer is empty. The length of the ringbuffer\n    // is defined as the distance between the two.\n    tail: usize,\n    head: usize,\n    buf: RawVec&lt;T&gt;,\n}\n\n// Default Global allocator\nstruct RawVec&lt;T, A: Alloc = Global&gt; {\n    ptr: Unique&lt;T&gt;,\n    cap: usize,\n    a: A,\n}\n\n#[repr(transparent)]\nstruct Unique&lt;T: ?Sized&gt; {\n    pointer: *const T,\n    _marker: PhantomData&lt;T&gt;,\n}\nThe same Vec methods, such as with_capacity (note that ring buffer alwaysleaves one space empty), truncate, shrink_to etc. exist and follow the same observations as in Vec study. The notable methods arepush_back and pop_back which involve moving the head pointer and push_front and pop_front which involve moving the tail pointer.retain which acts as the filter method.resize_with with a generator: impl FnMut() -&gt; Tas_slices (and the mut one) which contains, in order the content of the VecDeque.\nThat‚Äôs it for now!"
  },
  {
    "objectID": "posts/2023-11-19-rust-and-node-js-harmonizing-performance-and-safety/index.html",
    "href": "posts/2023-11-19-rust-and-node-js-harmonizing-performance-and-safety/index.html",
    "title": "Rust and Node.js: Harmonizing Performance and Safety",
    "section": "",
    "text": "In the Rust world, the interaction between Python and Rust is very well-known through the amazing PyO3 ecosystem. There is a similar relation between Python and Javascript in particular Node.js that I‚Äôm going to describe in this post. All the code is available here.\nMost programming language interactions happen through C layer ABI i.e.¬†FFI. However, interacting Rust with JavaScript is commonly achieved through WebAssembly (WASM). Furthermore, Node.js (written in C++) addon-api offers writing extending Node functionalities through C++ (FFI) without stepping into the WASM and the Rust ecosystem has created two frameworks on topnapi-rsneon\nWe are going to explore neon as well since it is also the more mature alternative.\nAs a quick recap, companies such as 1Password and Signal have adopted Rust in their Node applications, and more recently, a number of other companies like LogRocket and RisingStack have supercharged their Node apps. They‚Äôve achieved this by delegating critical parts to Rust where Node.js falls short. Consequently, Rust enhances these applications with its memory and type safety, while also being more efficient in CPU and memory usage. This leads to orders of magnitude higher Requests Per Second (RPS), showcasing Rust‚Äôs robust capabilities in optimizing performance.\nI‚Äôm assuming you have working Rust toolchain , NPM, Node.js. Then install neon module with npm i -g neon-cli. First, the ‚Äúhello, world!‚Äù\n\nnpm init neon hello creates\n\n.\n‚îú‚îÄ‚îÄ Cargo.toml\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ package.json\n‚îî‚îÄ‚îÄ src\n    ‚îî‚îÄ‚îÄ lib.rs\n2 directories, 4 files\nThe content of the preloaded src/lib.rs is\nuse neon::prelude::*;\nfn hello(mut cx: FunctionContext) -&gt; JsResult&lt;JsString&gt; {\n    Ok(cx.string(\"hello node\"))\n}\n#[neon::main]\nfn main(mut cx: ModuleContext) -&gt; NeonResult&lt;()&gt; {\n    cx.export_function(\"hello\", hello)?;\n\nnpm install and\nRun node prompt and\n\n&gt; require('.').hello()\n'hello node'\nSuper easy! All the compilation dependencies are included that calls for high DevX. For more details, check out the official neon documentation."
  },
  {
    "objectID": "posts/2023-11-19-rust-and-node-js-harmonizing-performance-and-safety/index.html#prelude",
    "href": "posts/2023-11-19-rust-and-node-js-harmonizing-performance-and-safety/index.html#prelude",
    "title": "Rust and Node.js: Harmonizing Performance and Safety",
    "section": "",
    "text": "In the Rust world, the interaction between Python and Rust is very well-known through the amazing PyO3 ecosystem. There is a similar relation between Python and Javascript in particular Node.js that I‚Äôm going to describe in this post. All the code is available here.\nMost programming language interactions happen through C layer ABI i.e.¬†FFI. However, interacting Rust with JavaScript is commonly achieved through WebAssembly (WASM). Furthermore, Node.js (written in C++) addon-api offers writing extending Node functionalities through C++ (FFI) without stepping into the WASM and the Rust ecosystem has created two frameworks on topnapi-rsneon\nWe are going to explore neon as well since it is also the more mature alternative.\nAs a quick recap, companies such as 1Password and Signal have adopted Rust in their Node applications, and more recently, a number of other companies like LogRocket and RisingStack have supercharged their Node apps. They‚Äôve achieved this by delegating critical parts to Rust where Node.js falls short. Consequently, Rust enhances these applications with its memory and type safety, while also being more efficient in CPU and memory usage. This leads to orders of magnitude higher Requests Per Second (RPS), showcasing Rust‚Äôs robust capabilities in optimizing performance.\nI‚Äôm assuming you have working Rust toolchain , NPM, Node.js. Then install neon module with npm i -g neon-cli. First, the ‚Äúhello, world!‚Äù\n\nnpm init neon hello creates\n\n.\n‚îú‚îÄ‚îÄ Cargo.toml\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ package.json\n‚îî‚îÄ‚îÄ src\n    ‚îî‚îÄ‚îÄ lib.rs\n2 directories, 4 files\nThe content of the preloaded src/lib.rs is\nuse neon::prelude::*;\nfn hello(mut cx: FunctionContext) -&gt; JsResult&lt;JsString&gt; {\n    Ok(cx.string(\"hello node\"))\n}\n#[neon::main]\nfn main(mut cx: ModuleContext) -&gt; NeonResult&lt;()&gt; {\n    cx.export_function(\"hello\", hello)?;\n\nnpm install and\nRun node prompt and\n\n&gt; require('.').hello()\n'hello node'\nSuper easy! All the compilation dependencies are included that calls for high DevX. For more details, check out the official neon documentation."
  },
  {
    "objectID": "posts/2023-11-19-rust-and-node-js-harmonizing-performance-and-safety/index.html#cheat-table",
    "href": "posts/2023-11-19-rust-and-node-js-harmonizing-performance-and-safety/index.html#cheat-table",
    "title": "Rust and Node.js: Harmonizing Performance and Safety",
    "section": "Cheat Table",
    "text": "Cheat Table\ntable { border-collapse: collapse; width: 100%; } th, td { border: 1px solid black; padding: 8px; text-align: left; }\n\n\n\n\n\n\n\n\nRust Neon Construct\nDescription\nExample Usage\n\n\n\n\nneon::prelude::*\nImports the essential traits and types for Neon modules.\nuse neon::prelude::*;\n\n\nFunctionContext&lt;'a&gt;\nRepresents the execution context of a JavaScript function call.\nfn my_function(mut cx: FunctionContext) -&gt; JsResult&lt;JsValue&gt; { ... }\n\n\nJsResult&lt;T&gt;\nA result type for Neon functions, either Ok(T) or Err(Throw).\nfn my_function(...) -&gt; JsResult&lt;JsString&gt; { ... }\n\n\nJsValue\nRepresents any JavaScript value.\nlet js_value: Handle&lt;JsValue&gt; = cx.argument(0)?;\n\n\nJsString, JsNumber, etc.\nSpecific JavaScript value types.\nlet js_string: Handle&lt;JsString&gt; = cx.argument(0)?;\n\n\nHandle&lt;T&gt;\nA handle to a JavaScript value, keeping it alive across the JS-Rust boundary.\nlet handle: Handle&lt;JsString&gt; = cx.argument(0)?;\n\n\nModuleContext&lt;'a&gt;\nRepresents the context of a module during initialization.\nfn neon_module_init(cx: ModuleContext) -&gt; NeonResult&lt;()?&gt; { ... }\n\n\nregister_module!\nA macro to register the module with Node.js.\nregister_module!(cx, neon_module_init);\n\n\nJsArray, JsObject\nTypes for JavaScript arrays and objects.\nlet js_array: Handle&lt;JsArray&gt; = JsArray::new(&mut cx, 3);\n\n\n.to_string(), .to_number(), etc.\nMethods to convert Neon types to JavaScript types.\nlet js_num = cx.number(42.0).upcast&lt;JsValue&gt;();\n\n\ncx.argument&lt;T&gt;(i)\nRetrieves the ith argument of a function call.\nlet arg0: Handle&lt;JsString&gt; = cx.argument&lt;JsString&gt;(0)?;\n\n\ncx.throw_error()\nThrows a JavaScript error from Rust.\ncx.throw_error(\"Something went wrong\")?;\n\n\ncx.borrow(), cx.borrow_mut()\nBorrows a reference to a JavaScript value.\nlet guard = cx.borrow(&js_array);"
  },
  {
    "objectID": "posts/2023-11-19-rust-and-node-js-harmonizing-performance-and-safety/index.html#resize-image-example",
    "href": "posts/2023-11-19-rust-and-node-js-harmonizing-performance-and-safety/index.html#resize-image-example",
    "title": "Rust and Node.js: Harmonizing Performance and Safety",
    "section": "Resize image example",
    "text": "Resize image example\nFor another example, let‚Äôs say you want to enhance resizing jpeg image.\n\n\nnpm init neon image-resize and cd image-resize\nThen cargo add image and\nInclude the following in src/lib.rs. Check out the comments below\n\nuse std::io::Cursor;\nuse image;\nuse neon::{prelude::*, types::buffer::TypedArray};\nfn resize_image(mut cx: FunctionContext) -&gt; JsResult&lt;JsBuffer&gt; {\n    // Retrieve image buffer and dimensions from JavaScript arguments\n    let buffer = cx.argument::&lt;JsBuffer&gt;(0)?; // &lt;- gets the first argument in node\n    let width = cx.argument::&lt;JsNumber&gt;(1)?.value(&mut cx) as u32;\n    let height = cx.argument::&lt;JsNumber&gt;(2)?.value(&mut cx) as u32;\n    // Convert JS Buffer to a byte slice\n    let image_data: &[u8] = buffer.as_slice(&cx);\n    // Perform image resizing in rust\n    let img = image::load_from_memory(&image_data).expect(\"Failed to load image from memory\");\n    let resized = img.resize(width, height, image::imageops::FilterType::Nearest);\n    let mut resized_buffer = Cursor::new(Vec::new());\n    resized\n        .write_to(&mut resized_buffer, image::ImageOutputFormat::Jpeg(100))\n        .expect(\"Failed to write image to buffer\");\n    let img_data = resized_buffer.into_inner();\n    // Convert the byte vector back to a JS Buffer\n    let js_buffer = JsBuffer::external(&mut cx, img_data);\n    Ok(js_buffer)\n}\n// finally export the function as a module\nregister_module!(mut cx, { cx.export_function(\"resizeImage\", resize_image) });\n\nnpm install\nRun node prompt and test with an image\n\nconst nativeModule = require('.');\nconst fs = require('fs');\nlet imageBuf = fs.readFileSync('cat.jpeg');\nconst resizedBuf = nativeModule.resizeImage(imageBuf, 50, 50);\nfs.writeFileSync('resized_cat.jpeg', resizedBuf);\nneon also provides a lot of ‚Äúpromise-api‚Äù, ‚Äútask-api‚Äù to handle js promise and node worker pool jobs."
  },
  {
    "objectID": "posts/2023-11-19-rust-and-node-js-harmonizing-performance-and-safety/index.html#segment-anything-in-node",
    "href": "posts/2023-11-19-rust-and-node-js-harmonizing-performance-and-safety/index.html#segment-anything-in-node",
    "title": "Rust and Node.js: Harmonizing Performance and Safety",
    "section": "Segment-Anything in Node",
    "text": "Segment-Anything in Node\nOur examples won‚Äôt be complete with some deep learning heavy computation. We use Segment-Anything which is a state-of-the-art segmentation model by Meta. It produces high quality object masks from input prompts such as points or boxes, and it can be used to generate masks for all objects in an image. We will using Huggingface Candle Rust Deep Learning library as the final example that delegates heavy lifting to Rust.\n\nnpm init neon sam-node and cd sam-node\nInclude the dependencies in Cargo.toml as follows\n\n[dependencies]\nanyhow = \"1.0.75\"\ncandle-core = { git = \"https://github.com/huggingface/candle.git\", version = \"0.3.1\", feautures = [\n    \"accelerate\",\n] }\ncandle-examples = { git = \"https://github.com/huggingface/candle.git\", version = \"0.3.1\", feautures = [\n    \"accelerate\",\n] }\ncandle-nn = { git = \"https://github.com/huggingface/candle.git\", version = \"0.3.1\", feautures = [\n    \"accelerate\",\n] }\ncandle-transformers = { git = \"https://github.com/huggingface/candle.git\", version = \"0.3.1\", feautures = [\n    \"accelerate\",\n] }\nhf-hub = \"0.3.2\"\nimage = \"0.24.7\"\nimageproc = \"0.23.0\"\nand make sure you may need to set Huggingface API key if you haven‚Äôt.\n\nNow in src/lib.rs\n\nfn generate_sam(mut cx: FunctionContext) -&gt; JsResult&lt;JsString&gt; {\n    let image_path = cx.argument::&lt;JsString&gt;(0)?.value(&mut cx);\n    let points = cx.argument::&lt;JsArray&gt;(1)?;\n    let neg_points = cx.argument::&lt;JsArray&gt;(2)?;\n    let points = get_points(&mut cx, points);\n    let neg_points = get_points(&mut cx, neg_points);\n    _generate_sam(image_path, points, neg_points).expect(\"error generating sam\");\n    Ok(cx.string(\"ok\"))\n}\nfn get_points(cx: &mut FunctionContext, handle: Handle&lt;JsArray&gt;) -&gt; Vec&lt;String&gt; {\n    let points = handle.to_vec(cx).expect(\"error converting to vec\");\n    let mut ret: Vec&lt;String&gt; = Vec::new();\n    for point in points {\n        let point_string = point\n            .downcast::&lt;JsString, FunctionContext&gt;(cx)\n            .or_else(|_| cx.throw_error(\"Array element is not a string\"))\n            .unwrap()\n            .value(cx);\n        ret.push(point_string);\n    }\n    // return as a vec but a single contiguous string\n    ret = vec![ret.join(\",\")];\n    ret\n}\nwhere\nfn _generate_sam(\n    image_path: String,\n    points: Vec&lt;String&gt;,\n    neg_points: Vec&lt;String&gt;,\n) -&gt; anyhow::Result&lt;()&gt; {\n    let device = candle_examples::device(true)?; // use CPU\n    let (image, initial_h, initial_w) =\n        candle_examples::load_image(&image_path, Some(sam::IMAGE_SIZE))?;\n    let image = image.to_device(&device)?;\n    println!(\"loaded image {image:?}\");\n    let api = hf_hub::api::sync::Api::new()?;\n    let api = api.model(\"lmz/candle-sam\".to_string());\n    let filename = \"mobile_sam-tiny-vitt.safetensors\";\n    let model = api.get(filename)?;\n    let vb = unsafe { VarBuilder::from_mmaped_safetensors(&[model], DType::F32, &device)? };\n    let sam = sam::Sam::new_tiny(vb)?;\n    // Default options similar to the Python version.\n    let bboxes = sam.generate_masks(\n        &image,\n        /* points_per_side */ 32,\n        /* crop_n_layer */ 0,\n        /* crop_overlap_ratio */ 512. / 1500.,\n        /* crop_n_points_downscale_factor */ 1,\n    )?;\n    for (idx, bbox) in bboxes.iter().enumerate() {\n        println!(\"{idx} {bbox:?}\");\n        let mask = (&bbox.data.to_dtype(DType::U8)? * 255.)?;\n        let (h, w) = mask.dims2()?;\n        let mask = mask.broadcast_as((3, h, w))?;\n        candle_examples::save_image_resize(\n            &mask,\n            format!(\"sam_mask{idx}.png\"),\n            initial_h,\n            initial_w,\n        )?;\n    }\n    let iter_points = points.iter().map(|p| (p, true));\n    let iter_neg_points = neg_points.iter().map(|p| (p, false));\n    let points = iter_points\n        .chain(iter_neg_points)\n        .map(|(point, b)| {\n            use std::str::FromStr;\n            let xy = point.split(',').collect::&lt;Vec&lt;_&gt;&gt;();\n            if xy.len() != 2 {\n                anyhow::bail!(\"expected format for points is 0.4,0.2\")\n            }\n            Ok((f64::from_str(xy[0])?, f64::from_str(xy[1])?, b))\n        })\n        .collect::&lt;anyhow::Result&lt;Vec&lt;_&gt;&gt;&gt;()?;\n    let start_time = std::time::Instant::now();\n    let (mask, iou_predictions) = sam.forward(&image, &points, false)?;\n    println!(\n        \"mask generated in {:.2}s\",\n        start_time.elapsed().as_secs_f32()\n    );\n    println!(\"mask:\\n{mask}\");\n    println!(\"iou_predictions: {iou_predictions}\");\n    let mask = (mask.ge(0.)? * 255.)?;\n    let (_one, h, w) = mask.dims3()?;\n    let mask = mask.expand((3, h, w))?;\n    let mut img = image::io::Reader::open(&image_path)?\n        .decode()\n        .map_err(candle::Error::wrap)?;\n    let mask_pixels = mask.permute((1, 2, 0))?.flatten_all()?.to_vec1::&lt;u8&gt;()?;\n    let mask_img: image::ImageBuffer&lt;image::Rgb&lt;u8&gt;, Vec&lt;u8&gt;&gt; =\n        match image::ImageBuffer::from_raw(w as u32, h as u32, mask_pixels) {\n            Some(image) =&gt; image,\n            None =&gt; anyhow::bail!(\"error saving merged image\"),\n        };\n    let mask_img = image::DynamicImage::from(mask_img).resize_to_fill(\n        img.width(),\n        img.height(),\n        image::imageops::FilterType::CatmullRom,\n    );\n    for x in 0..img.width() {\n        for y in 0..img.height() {\n            let mask_p = imageproc::drawing::Canvas::get_pixel(&mask_img, x, y);\n            if mask_p.0[0] &gt; 100 {\n                let mut img_p = imageproc::drawing::Canvas::get_pixel(&img, x, y);\n                img_p.0[2] = 255 - (255 - img_p.0[2]) / 2;\n                img_p.0[1] /= 2;\n                img_p.0[0] /= 2;\n                imageproc::drawing::Canvas::draw_pixel(&mut img, x, y, img_p)\n            }\n        }\n    }\n    for (x, y, b) in points {\n        let x = (x * img.width() as f64) as i32;\n        let y = (y * img.height() as f64) as i32;\n        let color = if b {\n            image::Rgba([255, 0, 0, 200])\n        } else {\n            image::Rgba([0, 255, 0, 200])\n        };\n        imageproc::drawing::draw_filled_circle_mut(&mut img, (x, y), 3, color);\n    }\n    img.save(\"sam_merged.jpg\")?;\n    Ok(())\n}\n// finally register as node module\nregister_module!(mut cx, { cx.export_function(\"generateSam\", generate_sam) });\nNext, after sam-node/index.js we add simple Node.js express server. (Note the dependencies npm install axios express)\nconst express = require('express');\nconst nativeModule = require('.');\nconst app = express();\nconst port = 3000;\nconst axios = require('axios');\nconst fs = require('fs');\nconst path = require('path');\nasync function downloadImage(url, filePath) {\n  try {\n    const response = await axios({\n      method: 'GET',\n      url: url,\n      responseType: 'stream'\n    });\n    const writer = fs.createWriteStream(filePath);\n    response.data.pipe(writer);\n    return new Promise((resolve, reject) =&gt; {\n      writer.on('finish', resolve);\n      writer.on('error', reject);\n    });\n  } catch (error) {\n    console.error('Error downloading the image:', error);\n    throw error;\n  }\n}\napp.use(express.json());\napp.listen(port, () =&gt; console.log(`Listening on port ${port}`));\napp.post(\"/generate-sam\", async (req, res) =&gt; {\n    try {\n        const { imagUrl, points, negPoints } = req.body;\n        const name = imagUrl.split('/').pop();\n        const filePath = path.join(__dirname, name);\n        await downloadImage(imagUrl, filePath);\n        await nativeModule.generateSam(filePath, points, negPoints);\n    } catch (error) {\n        console.log(error);\n        res.status(500).send(error);\n    }\n});\nThen node index.js and can test with npm install node-fetch script in test.js\nthat downloads the sample JPG image\n\n// testing using node-fetch to send a POST request to the server\nconst url = 'http://localhost:3000/generate-sam';\nconst imageUrl = 'https://githubraw.com/huggingface/candle/main/candle-examples/examples/yolo-v8/assets/bike.jpg';\nconst points = ['0.6', '0.6'];\nconst negPoints = ['0.6', '0.55'];\nfetch(url, {\n    method: 'POST',\n    headers: {\n        'Content-Type': 'application/json',\n    },\n    body: JSON.stringify({\n        imagUrl: imageUrl,\n        points: points,\n        negPoints: negPoints\n    })\n})\n.then(response =&gt; {\n    if (!response.ok) {\n        throw new Error('Network response was not ok');\n    }\n    return response.blob();\n})\n.then(blob =&gt; {\n    console.log('Image received:', blob);\n    const imageUrl = URL.createObjectURL(blob);\n    const img = document.createElement('img');\n    img.src = imageUrl;\n    document.body.appendChild(img);\n})\n.catch(error =&gt; {\n    console.error('Fetch error:', error);\n});\n\nwhere segmentation was applied to the right-most cyclist‚Äôs right foot.\nHope this post has ignited some spark to explore neon and the Rust-Node.js interactions."
  },
  {
    "objectID": "posts/2017-12-06-nips-ai-hype-and-the-lost-rigor/index.html",
    "href": "posts/2017-12-06-nips-ai-hype-and-the-lost-rigor/index.html",
    "title": "NIPS, AI hype and the lost rigor",
    "section": "",
    "text": "Warning: This post contains a mixture of excitements, frustrations and rants!\nToday, Machine Learning/Deep Learning people have been sharing their great excitements over Ali Rahimi‚Äôs talk at NIPS (from min 57 onwards). Undoubtedly, it‚Äôs a great talk and you should check it out if you care about fundamental issues and the lost rigor in Deep Learning results. His talk has resonated a lot with me as well and for more reasons that I‚Äôll try to explain in my own way while these tweets sum up the hype part well  ¬†   ¬† While there‚Äôs no doubt Deep Learning has been an incredible enabler butAI hype is realand you can feel its bittersweet taste. It is too naive to think that at this stage Deep Learning will bring usArtificial General Intelligenceand portraying ML/DL like in terminator movie coming to extinct the human race is irresponsible and idiotic. At this stage Deep Learning is like a bundle of techniques and since it is led by (empirical part of) Computer Science community, ‚Äúworking code‚Äù and seeing the empirical results is somehowthe proof. Moreover, apparently in order to get into ML/DL, you‚Äôd only need to know calculus and coding tobe the revolutionandchange the world!¬†¬Ø\\__(„ÉÑ)__/¬Ø Mypessimistic sideis saying what is happening now is that Deep Learning is growing exponentially fast with big army of enthusiasts ready to code up something quickly, generate results, publish papers and attract many attentions. These results somehow contribute to building systems for health care for example, which is sensitive enough and we cannot give more power to Deep Learning until there‚Äôs a solid,realfoundation. One such example of these serious issues is reflected in this tweet ¬†  ¬† What is baffling to me is there are many faculties who seem happy about this situation and are not addressing the real problems and are somehow becoming the enemy of science. Their ignorance is mind blowing! Myoptimistic sideis pointing me towards¬†the efforts and initiatives for addressing these issues. ¬†Some examples include introduction of google‚Äôs colaboratory project¬†and nowreproducibilityof results are much easier than ever before. Well-known distill.pub initiative aims at clearing up the air of current DL research. Stanford DAWNbench competition. From research sides,Bayesian Neural Networksis getting more attention and abundant of probabilistic programming frameworks such as PyMC3, Edward, ZhuSuan, Pyro and ProbTorch help a lot. Information theory of DL talk by Naftali Tishby at Yandex school is addressing some of the fundamental issues. Hinton‚Äôs talk on what‚Äôs wrong with convolutional neural nets¬†which led to the very recent Capsule paper. Moreover, his another recent paper on¬†Distilling a Neural Network into a Soft Decision Tree. To finish off, I think another important part is the realm of optimization algorithms more towards Unit Tests for Stochastic Optimization¬†type of research."
  },
  {
    "objectID": "posts/2025-12-31-createlang-rs-complete/index.html",
    "href": "posts/2025-12-31-createlang-rs-complete/index.html",
    "title": "createlang.rs: Six Years Later, It‚Äôs Done",
    "section": "",
    "text": "After almost six years, createlang.rs is complete. The book on creating your own programming language with Rust is finished. Not ‚Äúmostly done‚Äù or ‚Äújust needs polish.‚Äù Actually finished.\nI can barely believe I‚Äôm writing this."
  },
  {
    "objectID": "posts/2025-12-31-createlang-rs-complete/index.html#its-finally-done",
    "href": "posts/2025-12-31-createlang-rs-complete/index.html#its-finally-done",
    "title": "createlang.rs: Six Years Later, It‚Äôs Done",
    "section": "",
    "text": "After almost six years, createlang.rs is complete. The book on creating your own programming language with Rust is finished. Not ‚Äúmostly done‚Äù or ‚Äújust needs polish.‚Äù Actually finished.\nI can barely believe I‚Äôm writing this."
  },
  {
    "objectID": "posts/2025-12-31-createlang-rs-complete/index.html#how-it-started",
    "href": "posts/2025-12-31-createlang-rs-complete/index.html#how-it-started",
    "title": "createlang.rs: Six Years Later, It‚Äôs Done",
    "section": "How It Started",
    "text": "How It Started\nBack in 2020, I tweeted about the project:\n\n\n\n\nCan‚Äôt see the embed? View the tweet on X ‚Üí\nAt the time, I was excited. I had this vision of a comprehensive guide that would take someone from zero to building a real compiler in Rust. Not a toy language, but something with a proper parser, type system, and code generation.\nThe early version got some attention, people seemed interested, and I thought: ‚ÄúGreat! I‚Äôll wrap this up in a few months.‚Äù\nSix years later, here we are."
  },
  {
    "objectID": "posts/2025-12-31-createlang-rs-complete/index.html#what-took-so-long",
    "href": "posts/2025-12-31-createlang-rs-complete/index.html#what-took-so-long",
    "title": "createlang.rs: Six Years Later, It‚Äôs Done",
    "section": "What Took So Long?",
    "text": "What Took So Long?\nHonestly? Everything.\nLife happened. Jobs changed. Priorities shifted. I moved across the country. Some months I‚Äôd write every day, other months I wouldn‚Äôt touch it at all.\nThe scope kept growing. Every time I thought I was done, I‚Äôd realize there was another important concept I hadn‚Äôt covered. Lifetime semantics. Borrow checking. LLVM integration. Optimization passes. The list kept expanding.\nI didn‚Äôt want to write a typical compiler book. You know the ones. 1000 pages about parser theory that put you to sleep by chapter 3. I wanted something practical, something you could actually follow and build with. That meant finding the right balance between theory and practice, which is harder than it sounds.\nPerfectionism is real. I kept rewriting chapters. ‚ÄúThis explanation isn‚Äôt clear enough.‚Äù ‚ÄúThis example is too simple.‚Äù ‚ÄúThis code could be better.‚Äù I probably rewrote the parser chapter five times.\nRust kept evolving. When I started, we didn‚Äôt have const generics. No async/await. The module system was different. Every Rust edition meant going back and updating examples.\nBut you know what? All of that is okay. Because the book that exists now is so much better than what I envisioned in 2020."
  },
  {
    "objectID": "posts/2025-12-31-createlang-rs-complete/index.html#whats-actually-in-it",
    "href": "posts/2025-12-31-createlang-rs-complete/index.html#whats-actually-in-it",
    "title": "createlang.rs: Six Years Later, It‚Äôs Done",
    "section": "What‚Äôs Actually In It",
    "text": "What‚Äôs Actually In It\nThe book walks through building a complete programming language from scratch:\nPart I: Foundations - Lexical analysis and tokenization - Parsing with recursive descent and Pratt parsing - Abstract syntax trees and intermediate representations\nPart II: Type Systems - Type checking and inference - Polymorphism and generics - Lifetimes and ownership (the Rust way)\nPart III: Code Generation - LLVM basics and IR generation - Optimization passes - Linking and producing executables\nPart IV: Advanced Topics - Garbage collection and memory management - Concurrency primitives - Interop with C and other languages\nEvery chapter has complete, working code. Not pseudo-code, not simplified examples. Real Rust code that compiles and runs. The full implementation is on GitHub, and it‚Äôs a legitimate compiler for a small but complete language."
  },
  {
    "objectID": "posts/2025-12-31-createlang-rs-complete/index.html#what-i-learned",
    "href": "posts/2025-12-31-createlang-rs-complete/index.html#what-i-learned",
    "title": "createlang.rs: Six Years Later, It‚Äôs Done",
    "section": "What I Learned",
    "text": "What I Learned\nWriting this book taught me more than I expected.\nWriting forces clarity. When you have to explain something to someone else, you realize what you actually understand and what you just thought you understood. I learned Rust more deeply by teaching it.\nIncremental progress works. Some weeks I only wrote 200 words. That‚Äôs fine. 200 words a week is 10,000 words a year. It adds up.\nDone is better than perfect. At some point, you have to ship. There are probably typos in the book. Some explanations could be clearer. Some examples could be more elegant. But if I waited until it was perfect, I‚Äôd never release it.\nCommunity matters. The people who read early drafts, opened issues on GitHub, asked questions on Discord, they kept me going. Every piece of feedback made the book better."
  },
  {
    "objectID": "posts/2025-12-31-createlang-rs-complete/index.html#why-rust",
    "href": "posts/2025-12-31-createlang-rs-complete/index.html#why-rust",
    "title": "createlang.rs: Six Years Later, It‚Äôs Done",
    "section": "Why Rust?",
    "text": "Why Rust?\nRust‚Äôs safety guarantees, performance, and modern tooling made it the obvious choice. But honestly, the bigger reason I kept at this was that I really don‚Äôt like having a half-baked project hanging over my head. You know the feeling? That nagging sense of something unfinished, something you started with excitement that‚Äôs just sitting there incomplete.\nSix years is a long time to carry that around. But now it‚Äôs done, and that feels good."
  },
  {
    "objectID": "posts/2025-12-31-createlang-rs-complete/index.html#whats-next",
    "href": "posts/2025-12-31-createlang-rs-complete/index.html#whats-next",
    "title": "createlang.rs: Six Years Later, It‚Äôs Done",
    "section": "What‚Äôs Next?",
    "text": "What‚Äôs Next?\nThe book is done, but the project isn‚Äôt over.\nI want to add more examples. Maybe a chapter on implementing a small standard library. Perhaps a section on JIT compilation. There‚Äôs always more to explore.\nI‚Äôm also planning a series of blog posts diving deeper into specific topics from the book. Type inference algorithms. Register allocation. Optimization strategies. All the stuff that didn‚Äôt quite fit in the main narrative.\nAnd honestly? I‚Äôm just excited to see what people build with it. The whole point was to lower the barrier to creating languages. If even a few people use this to build something cool, that‚Äôs worth six years."
  },
  {
    "objectID": "posts/2025-12-31-createlang-rs-complete/index.html#a-note-on-mojo",
    "href": "posts/2025-12-31-createlang-rs-complete/index.html#a-note-on-mojo",
    "title": "createlang.rs: Six Years Later, It‚Äôs Done",
    "section": "A Note on Mojoüî•",
    "text": "A Note on Mojoüî•\nSpeaking of creating languages, I should mention that I‚Äôm now part of the team at Modular. Mojo is a new programming language for AI that combines Python‚Äôs usability with C-level performance. It‚Äôs a serious language design effort, and if you‚Äôre interested in systems programming, high-performance computing, or just how modern languages are built, check out the Mojo documentation.\nHonestly? If Mojo existed when I started this book, that would have been my choice. It‚Äôs much more powerful than Rust for the kind of low-level systems work compilers need, but also simpler. That‚Äôs the sweet spot. a language that gives you more power without the complexity overhead. But that‚Äôs the thing about language design, timing matters. In 2020, Rust was the right tool. Now we have even better options.\nWe‚Äôre on the path to Mojo 1.0, and it‚Äôs been incredible to see how the language has matured. The experience of writing this book definitely influenced how I think about language design. And working on Mojo has deepened my appreciation for the tradeoffs and decisions that go into creating a production language. It‚Äôs one thing to teach compiler concepts, another thing entirely to build a language that people will actually use in production."
  },
  {
    "objectID": "posts/2025-12-31-createlang-rs-complete/index.html#final-thoughts",
    "href": "posts/2025-12-31-createlang-rs-complete/index.html#final-thoughts",
    "title": "createlang.rs: Six Years Later, It‚Äôs Done",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nSix years feels like a long time. But looking back, it doesn‚Äôt feel wasted. Every chapter I struggled with, every example I rewrote, every concept I had to learn properly, it all contributed to something I‚Äôm genuinely proud of.\nIf you‚Äôre working on something that‚Äôs taking longer than expected, whether it‚Äôs a book, a project, or just learning something new, that‚Äôs okay. Progress isn‚Äôt always linear. Some things just take time.\nAnd when you‚Äôre done, whenever that is, it‚Äôll feel worth it.\nThe book is live at createlang.rs. The code is on GitHub. And if you have questions, find bugs, or just want to chat about compilers or language design, open an issue on the GitHub repository.\nThanks for following along on this journey. Here‚Äôs to the next one.\nHappy New Year. üéâ"
  },
  {
    "objectID": "posts/2017-01-21-distributed-lp-spark-thesis/index.html",
    "href": "posts/2017-01-21-distributed-lp-spark-thesis/index.html",
    "title": "My M.Sc. Thesis: Distributed Linear Programming with Apache Spark",
    "section": "",
    "text": "I defended my M.Sc. thesis at UBC! The thesis is titled ‚ÄúDistributed Linear Programming with Apache Spark‚Äù and it‚Äôs now available in the UBC Library Open Collections."
  },
  {
    "objectID": "posts/2017-01-21-distributed-lp-spark-thesis/index.html#its-official",
    "href": "posts/2017-01-21-distributed-lp-spark-thesis/index.html#its-official",
    "title": "My M.Sc. Thesis: Distributed Linear Programming with Apache Spark",
    "section": "",
    "text": "I defended my M.Sc. thesis at UBC! The thesis is titled ‚ÄúDistributed Linear Programming with Apache Spark‚Äù and it‚Äôs now available in the UBC Library Open Collections."
  },
  {
    "objectID": "posts/2017-01-21-distributed-lp-spark-thesis/index.html#the-thesis",
    "href": "posts/2017-01-21-distributed-lp-spark-thesis/index.html#the-thesis",
    "title": "My M.Sc. Thesis: Distributed Linear Programming with Apache Spark",
    "section": "The Thesis",
    "text": "The Thesis\nHere‚Äôs the full document:\n\n\n\n\nCan‚Äôt see the embed? View the thesis on UBC Library ‚Üí"
  },
  {
    "objectID": "posts/2017-01-21-distributed-lp-spark-thesis/index.html#whats-it-about",
    "href": "posts/2017-01-21-distributed-lp-spark-thesis/index.html#whats-it-about",
    "title": "My M.Sc. Thesis: Distributed Linear Programming with Apache Spark",
    "section": "What‚Äôs It About?",
    "text": "What‚Äôs It About?\nThe core idea is pretty straightforward: linear programming (LP) is fundamental to optimization, but when you have massive problems with millions of variables and constraints, traditional solvers on a single machine just don‚Äôt cut it.\nSo I implemented Mehrotra‚Äôs predictor-corrector interior point algorithm on top of Apache Spark. The result is Spark-LP, an open-source, fault-tolerant solver that can run on commodity clusters.\n\nWhy This Matters\n\nScale: You can solve LP problems that are too big for a single machine\nCost: Commodity clusters (like AWS EC2) are way cheaper than specialized hardware\nFault-tolerance: Spark handles node failures gracefully, so long-running jobs don‚Äôt die\nOpen-source: Anyone can use it, extend it, improve it\n\n\n\nThe Experiments\nI tested Spark-LP on clusters of 16 to 64 Amazon EC2 r3.xlarge instances. We threw both sparse and dense large-scale problems at it and measured convergence and performance.\nThe results were encouraging. For problems that fit the distributed paradigm well, Spark-LP performs competitively. There are limitations, of course. The overhead of distributed computing means it‚Äôs not always faster than a well-tuned single-machine solver for medium-sized problems. But for truly large-scale stuff? That‚Äôs where it shines."
  },
  {
    "objectID": "posts/2017-01-21-distributed-lp-spark-thesis/index.html#what-i-learned",
    "href": "posts/2017-01-21-distributed-lp-spark-thesis/index.html#what-i-learned",
    "title": "My M.Sc. Thesis: Distributed Linear Programming with Apache Spark",
    "section": "What I Learned",
    "text": "What I Learned\nWriting a thesis is an exercise in depth. You pick one thing and you go deep. Really deep. I learned more about interior point methods, numerical linear algebra, and distributed systems than I ever expected.\nI also learned that research is messy. Things don‚Äôt work the first time. Or the tenth time. You spend weeks debugging numerical instability issues, only to realize you had a sign error somewhere.\nBut when it finally works? That feeling is worth it."
  },
  {
    "objectID": "posts/2017-01-21-distributed-lp-spark-thesis/index.html#whats-next",
    "href": "posts/2017-01-21-distributed-lp-spark-thesis/index.html#whats-next",
    "title": "My M.Sc. Thesis: Distributed Linear Programming with Apache Spark",
    "section": "What‚Äôs Next?",
    "text": "What‚Äôs Next?\nThe thesis includes suggestions for future work:\n\nGPU acceleration: The JVM limits numerical performance. Integrating GPUs could dramatically speed things up.\nHeterogeneous clusters: Mixing CPUs and GPUs in the same cluster for different parts of the computation.\nBetter linear algebra: The numerical overhead on JVM is real. Native libraries could help.\n\nI‚Äôm moving on to other things now, but I hope someone picks this up and takes it further. The code is out there. The ideas are documented. Build on it."
  },
  {
    "objectID": "posts/2017-01-21-distributed-lp-spark-thesis/index.html#acknowledgments",
    "href": "posts/2017-01-21-distributed-lp-spark-thesis/index.html#acknowledgments",
    "title": "My M.Sc. Thesis: Distributed Linear Programming with Apache Spark",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nHuge thanks to my supervisor and committee for their guidance and patience. And to the UBC CS department for being a great place to do research.\nIf you‚Äôre interested in distributed optimization, large-scale computing, or just want to chat about Spark, feel free to reach out."
  },
  {
    "objectID": "posts/2017-01-21-distributed-lp-spark-thesis/index.html#links",
    "href": "posts/2017-01-21-distributed-lp-spark-thesis/index.html#links",
    "title": "My M.Sc. Thesis: Distributed Linear Programming with Apache Spark",
    "section": "Links",
    "text": "Links\n\nFull thesis on UBC Library\nDOI: 10.14288/1.0340337"
  },
  {
    "objectID": "posts/2022-09-20-announcing-releasing-dlpackrs/index.html",
    "href": "posts/2022-09-20-announcing-releasing-dlpackrs/index.html",
    "title": "Announcement üì¢ Releasing dlpackrs",
    "section": "",
    "text": "DLPack is the standard in-memory data format that facilitateszero-costtensor transfer across major Deep Learning frameworks (PyTorch, TensorFlow and TVM) and the supported Python Array processing frameworks such as Numpy, CuPy. The dlpackrs provides a safe idiomatic Rust binding where Rust ndarray and tensor frameworks can use it to gain the same kind of benefits (if not done already) across the supported hardware types.\nIn Python, such conversion goes through PyCapsule (minimal example for TVM - numpy conversion is here) but Rust needs no such restrictions or workaround. In fact, the PyO3/numpy conversion to Rust ndarray isalready zero-costthrough pointers (taken from here) which is\nimpl&lt;S, D, A&gt; ToPyArray for ArrayBase&lt;S, D&gt;\nwhere\n    S: Data&lt;Elem = A&gt;,\n    D: Dimension,\n    A: Element,\n{\n    type Item = A;\n    type Dim = D;\n\n    fn to_pyarray&lt;'py&gt;(&self, py: Python&lt;'py&gt;) -&gt; &'py PyArray&lt;Self::Item, Self::Dim&gt; {\n        let len = self.len();\n        match self.order() {\n            Some(flag) if A::IS_COPY =&gt; {\n                // if the array is contiguous, copy it by `copy_nonoverlapping`.\n                let strides = self.npy_strides();\n                unsafe {\n                    let array = PyArray::new_uninit(py, self.raw_dim(), strides.as_ptr(), flag);\n                    ptr::copy_nonoverlapping(self.as_ptr(), array.data(), len); // &lt;-- pointer copy\n                    array\n                }\n            }\n            _ =&gt; {\n                // if the array is not contiguous, copy all elements by `ArrayBase::iter`.\n                let dim = self.raw_dim();\n                unsafe {\n                    let array = PyArray::&lt;A, _&gt;::new(py, dim, false);\n                    let mut data_ptr = array.data();\n                    for item in self.iter() { // &lt;-- pointer copy starts\n                        data_ptr.write(item.clone());\n                        data_ptr = data_ptr.add(1);\n                    }\n                    array\n                }\n            }\n        }\n    }\n}"
  },
  {
    "objectID": "posts/2024-12-17-build-continuous-chat-interface-llama3/index.html",
    "href": "posts/2024-12-17-build-continuous-chat-interface-llama3/index.html",
    "title": "Build a Continuous Chat Interface with Llama 3 and MAX Serve",
    "section": "",
    "text": "I wrote a blog post on the Modular blog providing a comprehensive guide to building a continuous chat interface using Llama 3 and MAX Serve.\nThis tutorial walks through the entire process of creating a responsive chat application, from initial setup to deployment.\nKey topics covered:\n\nSetting up MAX Serve for LLM inference\nConfiguring Llama 3 for chat applications\nBuilding a continuous conversation interface\nManaging chat context and history\nDeployment strategies and best practices\n\nRead the full article: Build a Continuous Chat Interface with Llama 3 and MAX Serve"
  },
  {
    "objectID": "posts/2019-03-10-rust-std-study-series-vec/index.html",
    "href": "posts/2019-03-10-rust-std-study-series-vec/index.html",
    "title": "Rust std study series: Vec",
    "section": "",
    "text": "The upcoming series of blog posts will contain my study of Rust standard library. I‚Äôve partially written some parts of the series in scattered places and want to gather them in one place for better and easier access. I intend to update whenever I discover something interesting/important to remember.\nI‚Äôm referring to implementations inRust stable v1.33.0.\nThis post covers some details of std::vec that I found important."
  },
  {
    "objectID": "posts/2019-03-10-rust-std-study-series-vec/index.html#vec",
    "href": "posts/2019-03-10-rust-std-study-series-vec/index.html#vec",
    "title": "Rust std study series: Vec",
    "section": "Vec",
    "text": "Vec\nVec&lt;T&gt; is a dynamic array which only grows and never shrinks automatically.\n\nAcontiguousgrowable arraytype withheap-allocatedcontents.\nRust std doc\n\nNotice the difference between its counterpartstack-allocated fixed-sizearray [T; N] (where at this time N needs to be a specified non-negative integer. Constant generic hopefully will come soon).\nOk! let‚Äôs dig in to it. Vec&lt;T&gt; contains(pointer, capacity, length).\n\nVec pointer\n\nThe pointer willnever be null, so it enjoysnull-pointer-optimization.\nThe pointer maynotactually point to allocated memory for example inVec::new(), vec![] or Vec::with_capacity(0).\n\n\n\nVec capacity and length\n\nThe capacity of a vector is the amount of space allocated for any future elements that will be added onto the vector.\nThe length is the number of actual elements pushed/inserted in the vector.\nVec allocates memory iff mem::size_of::&lt;T&gt;() * capacity() &gt; 0. So it doesnotallocate for a Zero-Sized-Type (ZST) even with positive capacity.\nWhen length matches the capacity, Vec will (re)allocate by a certain growth factor. This makes insertionamortized\\(O(1)\\). Right now the growth factor is 2. However, comparing to other languages such as C++, Java, etc. it doesn‚Äôt seem to be optimal given any global first-fit allocator. Heuristically,1.5or a number a bit less than golden ratio is considered optimal. Here‚Äôs the related issue that‚Äôs currently open. I found it interesting to dig in!\nHow about shrink factor? for example, if we pop half of the elements out, would a quarter of the memory be freed? No, actually! That‚Äôs if poping all the elements the capacity won‚Äôt change leaving a hole on the heap. Therefore, pop (from back) is \\(O(1)\\) andnotamortized \\(O(1).\\) If you need to free up some memory, use shrink_to_fit.\nIf need to use Vec for FFI or as a memory-backed collection, be sure to deallocate memory with from_raw_parts and then drop explicitly.\nIf used in FFI and need to pass as pointer, for safety remember to call shrink_to_fit or truncate by the length prior to passing the (as_mut_ptr() or as_ptr()) to not pass uninitialized memory buffer.\nThe order of elements is always guaranteed to be the same if coerced into slice.\n\nHere‚Äôs the stripped down definition\nstruct Vec&lt;T&gt; {\n    buf: RawVec&lt;T&gt;,\n    len: usize,\n}\n\n// Default Global allocator\nstruct RawVec&lt;T, A: Alloc = Global&gt; {\n    ptr: Unique&lt;T&gt;,\n    cap: usize,\n    a: A,\n}\n\n#[repr(transparent)]\nstruct Unique&lt;T: ?Sized&gt; {\n    pointer: *const T,\n    _marker: PhantomData&lt;T&gt;,\n}\n\n\nUnique\n\n#[repr(transparent)] enforces that Unique&lt;T&gt;type representation is the same as *const T.\nUnique&lt;T&gt; is the covariant version of *mut T (wrt T) and hasstrongersemantic than NonNull&lt;T&gt;.\nUnlike *mut T, the pointer mustalways be non-null.\nIn fact, Box&lt;T&gt; wraps Unique&lt;T&gt; i.e.¬†struct Box&lt;T: ?Sized&gt;(Unique&lt;T&gt;).\nIt can be accessed in nightly through #![feature(ptr_internals)] and core::ptr::Unique.\nIf Tis Send/Sync then Unique&lt;T&gt; is Send/Sync.\nThe presence of PhantomData&lt;T&gt; marker is only important forrustc dropckto understand that we logically own a T causing the main difference between Unique&lt;T&gt; and NonNull&lt;T&gt; where it‚Äôs defined as\n\n// NonNull&lt;T&gt; doesn't own the referent whereas Unique&lt;T&gt; does\n#[repr(transparent)]\nstruct NonNull&lt;T: ?Sized&gt; {\n    pointer: *const T,\n}"
  },
  {
    "objectID": "posts/2017-06-30-from-machine-learning-to-type-theory/index.html",
    "href": "posts/2017-06-30-from-machine-learning-to-type-theory/index.html",
    "title": "From Machine Learning to Formal Math and Type Theory",
    "section": "",
    "text": "The idea of this post was sparkled from the new paperDeveloping Bug-Free Machine Learning Systems with Formal Mathematics.Meanwhile, I have had the idea of writing about what you‚Äôre going to readfor a long time and this paper happily forced me to do it finally! The first and final parts are about my journey and thoughts I want to write about. They may not seem directly related. If you only want to know about the paper, you can read that part."
  },
  {
    "objectID": "posts/2017-06-30-from-machine-learning-to-type-theory/index.html#brief-history",
    "href": "posts/2017-06-30-from-machine-learning-to-type-theory/index.html#brief-history",
    "title": "From Machine Learning to Formal Math and Type Theory",
    "section": "Brief history",
    "text": "Brief history\nWhen I was studying Algebraic Geometry the language of category, object, morphism, functors, natural-morphism, etc. were essential in understanding modern aspects of the field as well as modern Mathematics, in general. When I switched to Computer Science I stumbled upon Functional Language and Functional Programming (FP) through Scala programming language (mainly because of Apache Spark) and I loved it. I clearly remember the moment when I realized the connections between category theory language that I was using in my Math-time and then in Computer Science, with the implementations of Functor, Monad, Moniod, etc. It was fascinating and sweet, indeed. I quickly found out that Scala has a purely functional extension called Scalaz¬†that even has Yoneda object¬†defined. It was a WOW moment! I really enjoyed understanding and applying Yoneda lemma in my Math-time."
  },
  {
    "objectID": "posts/2017-06-30-from-machine-learning-to-type-theory/index.html#the-paper",
    "href": "posts/2017-06-30-from-machine-learning-to-type-theory/index.html#the-paper",
    "title": "From Machine Learning to Formal Math and Type Theory",
    "section": "The paper",
    "text": "The paper\nIt discusses about the a way for developers to &gt; use aninteractive proof assistantto bothimplement their systemand tostate a formal theoremdefining what it means for their system to be correct.\nIt is a way to close the gap between implementation errors and the formal Mathematical theory behind the implementation. The mechanism is important because when you design a machine learning system that enables you to &gt; find implementation errorssystematicallywithout recourse to empirical testing.\nBasically, it gives you formal verification of your program. It does so by using Lean programming language¬†which is aninteractive proof assistantas well as a programming language (talk by the creator of Lean). I‚Äôll explain more about what interactive proof assistant does. As a case study, the author developed a system called Centigrad &gt; which allows users to construct arbitrary stochastic computation graphs.Stochastic computation graphis a DAG (computation graph) having deterministic or stochastic computational units. Itsloss functionis defined as theexpected valueof sum of the leaf nodesover stochastic choices. Here‚Äôs an example of a simple variational autoencoder (VAE) (which is a generative model with stochastic computation graph introduced by the normal sampling layer in the middle. For more about VAE, see the nice post by fastforwardlabs)  Assume that you want to develop a backpropagation algorithm for such loss function. You may also have a sketch of ¬†the final derivation. Also assume that we don‚Äôt have access to Tensorflow or other deeplearning frameworks that do automatic differentiation. You may even have no data!It‚Äôs you and your computerand you want to make a formal Mathematical system that does all the things for you. You can be sure that the derivation is correct formally, when your system doesn‚Äôt fail and is thenbug-free. To do so, you‚Äôd need to define a long series of things in your system so that it can understand what the above loss function mean and how to compute backpropagration. Basically, you‚Äôd need to define real number \\(\\mathbb{R}\\) type, tensors, function, differentiation operator \\(\\nabla\\), differentiability, integration operator \\(\\int\\), integrability, conditions such that differentiation commutes with integration, notion of distribution, sampling from a distribution, computing the expectation (monadic operation!) and more details so that the system can understand the meaning the loss function and what backpropagation supposed to do.The novelty is that in Lean you can do so and it even can lead you interactively from in between steps / lemmas (aka¬†tactics in Lean) to the final machine-checkable formal derivation. In conclusion, the authors wrote &gt; ‚Ä¶ we were able to achieve extremely high confidence that our system was bug-free without needing to think about how all the pieces of the system fit together. In our approach, the computer‚Äî not the human‚Äîis responsible for ensuring that all the local properties that the developer establishes imply that the overall system is correct."
  },
  {
    "objectID": "posts/2017-06-30-from-machine-learning-to-type-theory/index.html#thoughts",
    "href": "posts/2017-06-30-from-machine-learning-to-type-theory/index.html#thoughts",
    "title": "From Machine Learning to Formal Math and Type Theory",
    "section": "Thoughts",
    "text": "Thoughts\nGiven the initial background\n\nScala is really a good fit, if one wants to create next Lean programming language.\nThis paper lead me to read and think about¬†type theory, decidability and homotopy type theory.¬†Their applications and contributions to the foundations of modern Mathematics. It is truly fascinating that such abstractions not only paves the road for Mathematicians in various fields such as Algebraic Geometry, Algebraic Topology, etc. but also enables creation of computer languages to assist us in constructing proofs and verify computations formally."
  },
  {
    "objectID": "posts/2019-03-25-rust-std-study-linkedlist/index.html",
    "href": "posts/2019-03-25-rust-std-study-linkedlist/index.html",
    "title": "Rust std study series: LinkedList",
    "section": "",
    "text": "Continuing from Rust standard library study series, it‚Äôs time for LinkedList&lt;T&gt;. Note that implementation are taken fromRust stable v1.33.0.\n\nAdoubly-linkedlist withowned nodes.\nThe LinkedList allowspushing andpopping elements at either end inconstant time.\nAlmost always it is better to use Vec or VecDeque instead of LinkedList. In general, array-based containers arefaster, more memory efficientand make better use of CPUcache**.\nRust std doc\n\nNote that unlike Vec&lt;T&gt;\n\naccessing an element through index is \\(O(n)\\) i.e.¬†needs to iterate linearly over the list.\nappend is \\(O(1).\\)\nInteresting how linked_list::Iter is different from linked_list::IterMut. Invariant of IterMut is enforced with &mut and PhantomData&lt;&'a Node&lt;T&gt;&gt; ensures soundness (more on PhantomData, dropck later).\n\nThere‚Äôs an entire book (which I highly recommend to go through details if you haven‚Äôt already) convincing the reader why it‚Äôs tricky to implement evensingly-linked list and most probably not a good idea for new Rust users!\nBecause of Rust‚Äôs affine type system / ownership, it‚Äôs actually tricky to implementdoubly-linked list. The main reason is it seems a node needs to havetwo ownersfrom adjacent nodes. However, that‚Äôs possible with NonNull&lt;T&gt; which we talked about in Vec&lt;T&gt; study.\nHere‚Äôs the stripped down definition\n// Note: NonNull&lt;T&gt; does NOT own the referent\n#[repr(transparent)] // &lt;-- enforces the same type representation as *const T\nstruct NonNull&lt;T: ?Sized&gt; {\n    pointer: *const T,\n}\n\nstruct Node&lt;T&gt; {\n    next: Option&lt;NonNull&lt;Node&lt;T&gt;&gt;&gt;, // Not Option&lt;Box&lt;Node&lt;T&gt;&gt;&gt;\n    prev: Option&lt;NonNull&lt;Node&lt;T&gt;&gt;&gt;,\n    element: T,\n}\n\nstruct LinkedList&lt;T&gt; {\n    head: Option&lt;NonNull&lt;Node&lt;T&gt;&gt;&gt;,\n    tail: Option&lt;NonNull&lt;Node&lt;T&gt;&gt;&gt;,\n    len: usize,\n    marker: PhantomData&lt;Box&lt;Node&lt;T&gt;&gt;&gt;, // &lt;-- sound dropck\n}\n\nWhy not Option&lt;Box&lt;Node&lt;T&gt;&gt;&gt; ?\nIt‚Äôs probably a good idea to see what would be the difference if we use Box&lt;Node&lt;T&gt;&gt; instead. We discussed what Unique&lt;T&gt; is and how it‚Äôs different from NonNull&lt;T&gt; previously, but as a quick reminder, Unique&lt;T&gt; owns the referent whereas NonNull&lt;T&gt; does not and in fact Box&lt;T&gt; (apointer typeforheap allocation) just wraps Unique&lt;T&gt; and provides new interface for interacting with Unique&lt;T&gt;.\nLet‚Äôs consider Node&lt;T&gt; with Box as follows (playpen link)\nstruct Node&lt;T&gt; {\n    prev: Option&lt;Box&lt;Node&lt;T&gt;&gt;&gt;,\n    next: Option&lt;Box&lt;Node&lt;T&gt;&gt;&gt;,\n    element: T,\n}\n\nfn main() {\n    let mut head_node = Node {\n        prev: None,\n        next: None,\n        element: 1,\n    };\n    \n    let next_node = Node {\n        prev: Some(Box::new(head_node)), // &lt;-- head_node is moved here\n        next: None,\n        element: 2,\n    };\n    \n    head_node.next = Some(Box::new(next_node)); // Not good!\n}\nThis begs forUse-After-Free(UAF) soUndefined Behavior(UB) which we know we shouldn‚Äôt push further. However, using a non-owning NonNull&lt;T&gt; can solve the problem as follows (playpen link)\nuse std::ptr::NonNull;\n\nstruct Node&lt;T&gt; {\n    prev: Option&lt;NonNull&lt;Node&lt;T&gt;&gt;&gt;,\n    next: Option&lt;NonNull&lt;Node&lt;T&gt;&gt;&gt;,\n    element: T,\n}\n\nfn main() {\n    let mut head_node = Node {\n        prev: None,\n        next: None,\n        element: 1,\n    };\n    \n    let next_node = Node {\n        prev: NonNull::new(&mut head_node as *mut _),\n        next: None,\n        element: 2,\n    };\n    \n    head_node.next = NonNull::new(&next_node as *const _ as *mut _);\n}\nBut how can we make sure this is sound esp.¬†when using it in LinkedList&lt;T&gt;. More precisely\n\n\nWhat‚Äôs the relation between PhantomData and dropck ?\nI‚Äôve been trying to understand the deeper relation between PhantomData and what makes dropck sound (so does LinkedList&lt;T&gt; sound), but couldn‚Äôt find any clear explanations so I asked it in the user channel and got an amazing thorough answer which can be generalized to Vec, LinkedList etc."
  },
  {
    "objectID": "posts/2019-08-16-rust-std-study-series-pin/index.html",
    "href": "posts/2019-08-16-rust-std-study-series-pin/index.html",
    "title": "Rust std study series: Pin",
    "section": "",
    "text": "This time we dive into std::pin which has a dense documentation."
  },
  {
    "objectID": "posts/2019-08-16-rust-std-study-series-pin/index.html#pin-and-unpin",
    "href": "posts/2019-08-16-rust-std-study-series-pin/index.html#pin-and-unpin",
    "title": "Rust std study series: Pin",
    "section": "Pin and Unpin",
    "text": "Pin and Unpin\n#[fundamental] // compiler thing\n#[repr(transparent)] // --&gt; ensures same memory repr as P\npub struct Pin&lt;P&gt; {\n    pointer: P,\n}\npub auto trait Unpin { }  // marker trait\nTo construct a Pin safely via Pin::new(pointer: P), we need to make sure that P: Deref (pointer-like) and P::Target: Unpin.\nNote that Unpin ensures\n\nTypes which can besafely moved after being pinned.\nSince Rust itself has no notion of immovable types, and considers moves (e.g.¬†through assignment or mem::replace) to always be safe, this trait cannot prevent types from moving by itself.\nInstead it is used toprevent moves through the type system, by controlling the behavior of pointers P wrapped in the Pin&lt;P&gt; wrapper, which ‚Äúpin‚Äù the type in place by not allowing it to be moved out of them.\nRust std doc\n\nBasically, the vast majority of types are ‚Äúmovable‚Äù i.e.¬†Unpin, so for most cases,Pin&lt;Pointer&lt;T&gt;&gt;is exactly likePointer&lt;T&gt;whenT: Unpin. However, if the referent (referenced type T) is ‚Äúimmovable‚Äù i.e.¬†T: !Unpin, then the pinned wrapper won‚Äôt let the underlying value to move.*Intuitively it‚Äôs like having**&mut* but with & access safety and immovability invariant. It is in fact, Unpin that controls how Pin to work.\nTo iterate; Pin&lt;P&gt; ensures that theunderlying data behind a pointer typePhas a stable location in memoryandits memory cannot be deallocated until it is dropped. So Pin pins the pointee (the data behind the pointer)not the pointer itself. Moreover,Pin&lt;P&gt;does not let clients actually obtain aBox&lt;T&gt;or&mut Tto the underlying pinned data, which implies that youcannotuse operations such as mem::swap.\nFurthermore,\n\nFor correctness, Pin&lt;P&gt; relies on the Deref and DerefMut implementations tonot move out of theirselfparameter, and to only ever return a pointer to pinned data when they are called on a pinned pointer.\nRust std doc\n\nTo implement !Unpin for a type directly, (up to this write up) we need nightly with ![feature(optin_builtin_traits)]. The stable way is through the marker type PhantomPinned which already has impl !Unpin for PhantomPinned {}"
  },
  {
    "objectID": "posts/2019-08-16-rust-std-study-series-pin/index.html#closer-look",
    "href": "posts/2019-08-16-rust-std-study-series-pin/index.html#closer-look",
    "title": "Rust std study series: Pin",
    "section": "Closer look",
    "text": "Closer look\nWe cannot create Pin for a Pointer&lt;T&gt; where T: !Unpin safely\npub struct Pin&lt;P&gt; {\n    pointer: P,\n}\nimpl&lt;P: Deref&gt; Pin&lt;P&gt; \nwhere P::Target: Unpin // --&gt; underlying data is movable\n{\n    pub fn new(pointer: P) -&gt; Pin&lt;P&gt; {\n        // Safety: the value pointed to is `Unpin`, and so has no requirements\n        // around pinning.\n        unsafe { Pin::new_unchecked(pointer) }\n    }\n}\nimpl&lt;P: Deref&gt; Pin&lt;P&gt; {\n   pub unsafe fn new_unchecked(pointer: P) -&gt; Pin&lt;P&gt; {\n        Pin { pointer }\n    }\n}\n\nSo how to pin a T: !Unpin safely?\nWe can use Box::pin (similarity with Rc::pin or Arc::pin) where\nimpl&lt;T&gt; Box&lt;T&gt; {\n    #[inline(always)]\n    pub fn pin(x: T) -&gt; Pin&lt;Box&lt;T&gt;&gt; {\n        (box x).into()\n    }\n}\nimpl&lt;T: ?Sized&gt; From&lt;Box&lt;T&gt;&gt; for Pin&lt;Box&lt;T&gt;&gt; {\n    /// This conversion does not allocate on the heap and happens in place.\n    fn from(boxed: Box&lt;T&gt;) -&gt; Self {\n        Box::into_pin(boxed)\n    }\n}\nimpl&lt;T: ?Sized&gt; Box&lt;T&gt; {\n    pub fn into_pin(boxed: Box&lt;T&gt;) -&gt; Pin&lt;Box&lt;T&gt;&gt; {\n        // It's not possible to move or replace the insides of a `Pin&lt;Box&lt;T&gt;&gt;`\n        // when `T: !Unpin`,  so it's safe to pin it directly without any\n        // additional requirements.\n        unsafe { Pin::new_unchecked(boxed) }\n    }\n}\nSome notable methods of Pin areas_ref: &Pin&lt;Pointer&lt;T&gt;&gt; ---&gt; Pin&lt;&T&gt;as_mut: &mut Pin&lt;Pointer&lt;T&gt;&gt; ---&gt; Pin&lt;&mut T&gt;get_ref: Pin&lt;Pointer&lt;T&gt;&gt; ---&gt; &Tget_mut: Pin&lt;Pointer&lt;T&gt;&gt; ---&gt; &mut T (given T: Unpin)\nThere is also a set method which overwrites the pinned data and it might seem is invalidating the pinned construct but the subtle point is that before assigning the new value, the destructor of the old data is run so it‚Äôs ok.\nimpl&lt;P: DerefMut&gt; Pin&lt;P&gt; {\n    pub fn set(self: &mut Pin&lt;P&gt;, value: P::Target)\n    where\n        P::Target: Sized,\n    {\n        *(self.pointer) = value; // --&gt; before assignment the pointee *self.pointer is dropped\n    }\n}\nSo the whole point is, we can change the pointee Pointer::Target of a mutable pinned pointer but cannot get a &mut T out of a Pin&lt;Pointer&lt;T&gt;&gt; (Pointer::Target = T in here) when T: !Unpin.\nPin has been introduced to resolve the issues around Future and async/.await (and more general, Generator). To get a better idea, I recommend looking into async book pin section to see, for example, how an async block is turned into structs (like closures) with potentiallyself-referential mutable fieldsand how Pin helps there.\nHope we have entangled some of the complications for understanding what Pin is and what it does. For more details, checkout the complete documentation as well as more discussion on structural pinning and the safety issues around it."
  },
  {
    "objectID": "posts/2025-12-24-geometric-ub-framework/index.html",
    "href": "posts/2025-12-24-geometric-ub-framework/index.html",
    "title": "A Geometric Framework for Undefined Behavior: From a 2019 Tweet to an Actual Paper",
    "section": "",
    "text": "Alright, so I finally finished writing this up. Here‚Äôs the paper if you want to dive straight in (I like coffee!):\n\n\n  Your browser doesn't support embedded PDFs.\n     Download the PDF here\n\n\n\n  \n    üì• Download Paper (PDF)"
  },
  {
    "objectID": "posts/2025-12-24-geometric-ub-framework/index.html#the-paper",
    "href": "posts/2025-12-24-geometric-ub-framework/index.html#the-paper",
    "title": "A Geometric Framework for Undefined Behavior: From a 2019 Tweet to an Actual Paper",
    "section": "",
    "text": "Alright, so I finally finished writing this up. Here‚Äôs the paper if you want to dive straight in (I like coffee!):\n\n\n  Your browser doesn't support embedded PDFs.\n     Download the PDF here\n\n\n\n  \n    üì• Download Paper (PDF)"
  },
  {
    "objectID": "posts/2025-12-24-geometric-ub-framework/index.html#acknowledgments",
    "href": "posts/2025-12-24-geometric-ub-framework/index.html#acknowledgments",
    "title": "A Geometric Framework for Undefined Behavior: From a 2019 Tweet to an Actual Paper",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nHuge thanks to Chris Lattner and Abdul Dakkak for taking the time to review drafts of this paper. Your feedback was incredibly helpful and pushed me to clarify and strengthen the ideas. I‚Äôm really grateful for your encouragement and thoughtful comments."
  },
  {
    "objectID": "posts/2025-12-24-geometric-ub-framework/index.html#how-this-started",
    "href": "posts/2025-12-24-geometric-ub-framework/index.html#how-this-started",
    "title": "A Geometric Framework for Undefined Behavior: From a 2019 Tweet to an Actual Paper",
    "section": "How This Started",
    "text": "How This Started\nSo this whole thing started back in 2019 with this tweet thread about using category theory to understand Rust‚Äôs lifetime system. I was thinking about lifetimes as categories, references as functors, that kind of thing.\n\n\n\n\n\n\nNoteThe Original 2019 Tweet Thread\n\n\n\n\n\n\n\nCan‚Äôt see the embed? View the thread on X ‚Üí\n\n\nI was working with Rust a lot and thinking about borrow checking and lifetime systems, and the categorical structure felt important. Also the origin goes back to my earlier blog on Variance in Rust. But later I started wondering: what if we could geometrize this? What if undefined behavior isn‚Äôt just a type system violation, but has actual geometric structure? Like, what if memory safety violations are holes in some space of program states?\nFast forward to 2025, and I started playing around with the idea more seriously. I was using ChatGPT 5.1 to bounce ideas around, and it made a suggestion that ended up being crucial: use the Alexandroff topology on a poset of abstract program states. That insight was the key that made everything click. Once I had that, the rest of the framework started falling into place."
  },
  {
    "objectID": "posts/2025-12-24-geometric-ub-framework/index.html#the-basic-idea",
    "href": "posts/2025-12-24-geometric-ub-framework/index.html#the-basic-idea",
    "title": "A Geometric Framework for Undefined Behavior: From a 2019 Tweet to an Actual Paper",
    "section": "The Basic Idea",
    "text": "The Basic Idea\nHere‚Äôs the pitch in plain English:\nSystems languages like Rust, C++, and Mojo try to give you low-level control while keeping you safe from things like use-after-free, iterator invalidation, and all the other ways you can shoot yourself in the foot with memory.\nThe standard way to think about this is through type systems (Rust‚Äôs borrow checker), logical models (RustBelt), or operational semantics (Stacked Borrows). These are great, but they‚Äôre often language-specific and pretty complex.\nMy approach is different. I‚Äôm treating program states as points in a geometric space, and undefined behavior as topological holes in that space. Specifically:\n1. States form a partially ordered set (poset)\nThink of each state as representing what your program knows about memory at some point. If state w can safely evolve into state w', we say w ‚â§ w'. So the ‚Äúsafe extension‚Äù relation gives you a poset.\n2. The Alexandroff topology\nThis was the key insight GPT 5.1 gave me. In the Alexandroff topology, a set is ‚Äúopen‚Äù if it‚Äôs upward-closed: if a state is in the set, all safe extensions of it are also in the set. This perfectly captures the idea that safety is monotonic. Once you‚Äôre in a safe state, staying safe means continuing upward.\n3. Build a simplicial complex\nFrom the poset, you can build a simplicial complex. Vertices are states, edges are safe transitions, 2-simplices are pairs of operations that commute safely, and so on. This complex describes the shape of the safe state space.\n4. Homology finds the holes\nHere‚Äôs where it gets cool. If there‚Äôs a cycle in your state space that you can‚Äôt fill in (like trying to use a reference after it‚Äôs been invalidated), that shows up as nontrivial homology. Specifically, it‚Äôs a loop that doesn‚Äôt bound a 2-dimensional disk, which means you have a hole in \\(H_1\\) (the first homology group).\nUndefined behavior is those holes."
  },
  {
    "objectID": "posts/2025-12-24-geometric-ub-framework/index.html#a-concrete-example",
    "href": "posts/2025-12-24-geometric-ub-framework/index.html#a-concrete-example",
    "title": "A Geometric Framework for Undefined Behavior: From a 2019 Tweet to an Actual Paper",
    "section": "A Concrete Example",
    "text": "A Concrete Example\nLet‚Äôs do Rust. Consider this pattern (which thankfully doesn‚Äôt compile):\nlet mut v = vec![1, 2, 3];\nlet r = &v[0];      // borrow v\nv.push(4);          // realloc may move v (UB if r is used)\nprintln!(\"{}\", *r); // use-after-free\nIn the paper, I model this as a state space with four states:\n\nw‚ÇÄ: v exists\nw‚ÇÅ: reference r to v[0] is created\nw‚ÇÇ: v gets reallocated (invalidates r)\nw‚ÇÉ: r is dereferenced (UB!)\n\nThe problem is that there‚Äôs a cycle w‚ÇÅ ‚Üí w‚ÇÇ (reallocate while borrowed) and w‚ÇÇ ‚Üí w‚ÇÅ (somehow un-reallocate?) that doesn‚Äôt bound a 2-simplex. There‚Äôs no safe ‚Äúcommuting square‚Äù where you can do both operations in either order. This cycle is a generator of \\(H_1\\), a 1-dimensional hole.\nRust‚Äôs borrow checker prevents this statically by not letting you do v.push(4) while r is live. But in C++ or unsafe Rust, the hole is there. And you can detect it by computing homology."
  },
  {
    "objectID": "posts/2025-12-24-geometric-ub-framework/index.html#wait-lifetimes-are-a-category",
    "href": "posts/2025-12-24-geometric-ub-framework/index.html#wait-lifetimes-are-a-category",
    "title": "A Geometric Framework for Undefined Behavior: From a 2019 Tweet to an Actual Paper",
    "section": "Wait, Lifetimes Are a Category?",
    "text": "Wait, Lifetimes Are a Category?\nYeah, this part is kind of fun. In addition to the topological view, the paper gives a categorical semantics for Rust‚Äôs lifetime system:\n\nLifetimes form a category where objects are lifetime parameters ('a, 'b, etc.) and morphisms are outlives relations ('a: 'b).\nReference types are bifunctors from (Lifetimes √ó Types) to Types. So &'a T is functorial in both the lifetime 'a and the type T.\nReborrows are natural transformations. When you reborrow &'a T to &'b T with 'a: 'b, that‚Äôs a natural transformation between bifunctors.\n\nThis is pretty standard category theory (in the sense of Mac Lane‚Äôs Categories for the Working Mathematician), but I think it‚Äôs cool that Rust‚Äôs lifetime system actually is a functorial construction. It‚Äôs not just an analogy."
  },
  {
    "objectID": "posts/2025-12-24-geometric-ub-framework/index.html#c-and-mojo-too",
    "href": "posts/2025-12-24-geometric-ub-framework/index.html#c-and-mojo-too",
    "title": "A Geometric Framework for Undefined Behavior: From a 2019 Tweet to an Actual Paper",
    "section": "C++ and Mojo Too",
    "text": "C++ and Mojo Too\nThe framework isn‚Äôt Rust-specific. In the paper I also work through:\n\nC++: I build a toy example with raw pointers and iterator invalidation happening in the same function. The complex has \\(H_1 \\cong \\mathbb{Z}^2\\), meaning there are two independent UB holes. You can trigger one, the other, or both.\nMojo: Mojo has this cool system of origins that parameterize lifetimes. I sketch how origins fit into the same geometric picture, and how you can use cohomology (specifically, sheaf cohomology on the poset of origins) to detect global inconsistencies.\n\nThe details are in the paper. The point is that once you have the geometric layer, you can apply it to any language where you can define abstract states and safe transitions."
  },
  {
    "objectID": "posts/2025-12-24-geometric-ub-framework/index.html#can-you-actually-compute-this",
    "href": "posts/2025-12-24-geometric-ub-framework/index.html#can-you-actually-compute-this",
    "title": "A Geometric Framework for Undefined Behavior: From a 2019 Tweet to an Actual Paper",
    "section": "Can You Actually Compute This?",
    "text": "Can You Actually Compute This?\nShort answer: yes, on small scopes.\nHomology is just linear algebra. If you have \\(N\\) abstract states and \\(M\\) transitions, you build a simplicial complex with \\(N\\) vertices and at most \\(M\\) edges, and computing \\(H_1\\) reduces to finding the kernel and image of some matrices over \\(\\mathbb{Z}\\). For intraprocedural analysis (like checking a single function or loop), this is totally doable.\nThe paper includes a high-level algorithm:\n\nExtract abstract states and safe transitions from your program (using existing static analysis).\nBuild the simplicial complex.\nCompute \\(H_1\\).\nReport any nontrivial cycles as candidate UB patterns.\n\nThis isn‚Äôt a replacement for the borrow checker or sanitizers. It‚Äôs a complementary technique. You‚Äôd run it on suspicious regions (like around unsafe blocks) to get a geometric sanity check."
  },
  {
    "objectID": "posts/2025-12-24-geometric-ub-framework/index.html#why-i-think-this-matters",
    "href": "posts/2025-12-24-geometric-ub-framework/index.html#why-i-think-this-matters",
    "title": "A Geometric Framework for Undefined Behavior: From a 2019 Tweet to an Actual Paper",
    "section": "Why I Think This Matters",
    "text": "Why I Think This Matters\nFor language designers: Thinking about UB geometrically gives you design principles. If you can design your type system so that certain complexes are contractible (no holes), you‚Äôve ruled out whole classes of bugs by construction.\nFor static analysis: You get algebraic invariants (homology groups) that are language-independent. Any tool that can extract abstract states can plug into this framework.\nFor programmers: It‚Äôs a mental model. When you write unsafe code, you‚Äôre navigating a space that has holes in it. The borrow checker is trying to keep you in the safe, contractible regions. Understanding that helps you reason about what‚Äôs going on."
  },
  {
    "objectID": "posts/2025-12-24-geometric-ub-framework/index.html#whats-next",
    "href": "posts/2025-12-24-geometric-ub-framework/index.html#whats-next",
    "title": "A Geometric Framework for Undefined Behavior: From a 2019 Tweet to an Actual Paper",
    "section": "What‚Äôs Next?",
    "text": "What‚Äôs Next?\nI want to build a prototype. Take Rust MIR or some C++ intermediate representation, extract small state complexes around uses of unsafe or raw pointers, compute \\(H_1\\), and see if it finds known bugs. If it does, that‚Äôs validation. If it finds new ones, that‚Äôs even better.\nThere are also some deeper directions:\n\nConcurrency: Extend the poset to include thread interleavings. Use directed topology or directed homology to detect data races as higher-dimensional holes.\nType system integration: Use homological properties as meta-invariants. Design type rules that guarantee contractibility.\nSheaf cohomology: Treat types and invariants as sheaves over the state poset and use cohomology to find global inconsistencies.\n\nBut honestly, I‚Äôm just excited that the basic idea works. It‚Äôs been six years since that tweet, and turning a vague intuition into actual math felt really satisfying."
  },
  {
    "objectID": "posts/2025-12-24-geometric-ub-framework/index.html#final-thoughts",
    "href": "posts/2025-12-24-geometric-ub-framework/index.html#final-thoughts",
    "title": "A Geometric Framework for Undefined Behavior: From a 2019 Tweet to an Actual Paper",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nFrom a random tweet in 2019 to a finished paper in 2025, this has been a weird journey. The idea that you can think about memory safety bugs as literal holes in a topological space still feels kind of magical to me."
  },
  {
    "objectID": "posts/2025-12-24-geometric-ub-framework/index.html#a-note-on-publication",
    "href": "posts/2025-12-24-geometric-ub-framework/index.html#a-note-on-publication",
    "title": "A Geometric Framework for Undefined Behavior: From a 2019 Tweet to an Actual Paper",
    "section": "A Note on Publication",
    "text": "A Note on Publication\nI tried to submit this to arXiv, but couldn‚Äôt get an endorsement. My old grad school advisors and colleagues aren‚Äôt eligible due to arXiv‚Äôs strict policies on endorser recency and paper count. The endorsement system makes sense for preventing spam, but it also creates a barrier for researchers who aren‚Äôt currently in academia or don‚Äôt have recent connections to active arXiv users. It‚Äôs a bit of a catch-22: you need endorsement to build a publication record, but you need a publication record (or the right connections) to get endorsed.\nSo for now, the paper lives here. The web is a perfectly fine place for ideas to exist.\nThanks for reading."
  },
  {
    "objectID": "posts/2024-05-29-what-ownership-is-really-about/index.html",
    "href": "posts/2024-05-29-what-ownership-is-really-about/index.html",
    "title": "What Ownership is Really About: A Mental Model Approach",
    "section": "",
    "text": "I wrote a blog post on the Modular blog exploring the concept of ownership in programming languages, particularly in the context of Mojo.\nOwnership is one of the most important concepts in systems programming, yet it‚Äôs often misunderstood. In this article, I present a mental model approach to help developers build intuition around ownership semantics.\nKey topics covered:\n\nWhat ownership really means beyond the syntax\nHow to think about ownership as a mental model\nPractical implications for writing safer code\nHow Mojo‚Äôs ownership system relates to other languages\n\nRead the full article: What Ownership is Really About: A Mental Model Approach"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ehsan's Blog",
    "section": "",
    "text": "createlang.rs: Six Years Later, It‚Äôs Done\n\n\nAfter almost six years, my book on creating programming languages with Rust is finally complete. Here‚Äôs what that journey looked like.\n\n\n\n\n\nDec 31, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nA Geometric Framework for Undefined Behavior: From a 2019 Tweet to an Actual Paper\n\n\nHow a random thought about using algebraic topology to understand undefined behavior turned into a real paper.\n\n\n\n\n\nDec 24, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMojo GPU Puzzles Edition 1\n\n\nA hands-on guide that teaches GPU programming through 34 progressive challenges - learn by doing, not lectures.\n\n\n\n\n\nOct 29, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPaged Attention & Prefix Caching Now Available in MAX Serve\n\n\nAnnouncing state-of-the-art optimizations for LLM inference in MAX Serve - originally published on Modular‚Äôs blog.\n\n\n\n\n\nFeb 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAgentic Building Blocks: Creating AI Agents with MAX Serve and OpenAI Function Calling\n\n\nLearn how to build AI agents by integrating MAX Serve with OpenAI‚Äôs function calling capabilities - originally published on Modular‚Äôs blog.\n\n\n\n\n\nJan 30, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nHands-on with Mojo 24.6\n\n\nA practical walkthrough of the new features in Mojo 24.6 - originally published on Modular‚Äôs blog.\n\n\n\n\n\nJan 21, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBuild a Continuous Chat Interface with Llama 3 and MAX Serve\n\n\nA step-by-step guide to building a chat application using Llama 3 and MAX Serve - originally published on Modular‚Äôs blog.\n\n\n\n\n\nDec 17, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPaged Attention & Prefix Caching Now Available in MAX Serve\n\n\nAnnouncing state-of-the-art optimizations for LLM inference in MAX Serve - originally published on Modular‚Äôs blog.\n\n\n\n\n\nNov 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHands-on with Mojo 24.5\n\n\nA practical walkthrough of the new features in Mojo 24.5 - originally published on Modular‚Äôs blog.\n\n\n\n\n\nOct 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWhat‚Äôs New in MAX 24.4? MAX on macOS, Fast Local Llama3, Native Quantization and GGUF Support\n\n\nExploring the new features in MAX 24.4 including macOS support and Llama3 - originally published on Modular‚Äôs blog.\n\n\n\n\n\nJun 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWhat Ownership is Really About: A Mental Model Approach\n\n\nA deep dive into understanding ownership in programming through mental models - originally published on Modular‚Äôs blog.\n\n\n\n\n\nMay 29, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSemantic Search with MAX Engine\n\n\nBuilding semantic search applications using MAX Engine - originally published on Modular‚Äôs blog.\n\n\n\n\n\nMar 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nModal Labs Deep Dive\n\n\n\n\n\n\n\n\nDec 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nThe Core of Attention is Communication\n\n\n\n\n\n\n\n\nNov 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nRust and Node.js: Harmonizing Performance and Safety\n\n\n\n\n\n\n\n\nNov 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nNotes on the Current State of LLM Frameworks\n\n\n\n\n\n\n\n\nAug 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nAnnouncement üì¢ Releasing dlpackrs\n\n\n\n\n\n\n\n\nSep 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nAnnouncement üì¢ Releasing smartalloc\n\n\n\n\n\n\n\n\nSep 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nAnnouncement üì¢ Create your own programming language with Rust\n\n\n\n\n\n\n\n\nJun 8, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nRust std study series: Pin\n\n\n\n\n\n\n\n\nAug 16, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nRust std study series: alloc\n\n\n\n\n\n\n\n\nJul 3, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nRust std study series: Interior mutability\n\n\n\n\n\n\n\n\nJun 18, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nThe State of Machine Learning in Rust\n\n\n\n\n\n\n\n\nMay 13, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nRust std study series: VecDeque\n\n\n\n\n\n\n\n\nApr 28, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nRust std study series: LinkedList\n\n\n\n\n\n\n\n\nMar 25, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nVariance in Rust: An intuitive explanation\n\n\n\n\n\n\n\n\nMar 16, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nRust std study series: Vec\n\n\n\n\n\n\n\n\nMar 10, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nNIPS, AI hype and the lost rigor\n\n\n\n\n\n\n\n\nDec 6, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nWhat‚Äôs up with word embedding?\n\n\n\n\n\n\n\n\nJul 12, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nFrom Machine Learning to Formal Math and Type Theory\n\n\n\n\n\n\n\n\nJun 30, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nMy M.Sc. Thesis: Distributed Linear Programming with Apache Spark\n\n\nI finally defended my M.Sc. thesis on solving large-scale linear programming problems using Apache Spark. Here‚Äôs the full thesis and some thoughts on the journey.\n\n\n\n\n\nJan 21, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nGeneral Monty Hall Simulation\n\n\n\n\n\n\n\n\nMay 5, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nRestaurant Revenue Prediction with BART Machine\n\n\n\n\n\n\n\n\nSep 13, 2015\n\n\n\n\n\n\n\n\n\n\n\n\nMy Math M.Sc. Thesis: Hilbert Schemes and the Derived McKay Correspondence\n\n\nMy first M.Sc. thesis on the behavior of the Hilbert scheme of points under the derived McKay correspondence. Pure math, derived categories, and beautiful combinatorics.\n\n\n\n\n\nSep 9, 2013\n\n\n\n\n\n\n\n\n\n\n\n\nVector Bundles, Locally Free Sheaves and Divisors on a Curve\n\n\n\n\n\n\n\n\nDec 6, 2012\n\n\n\n\n\n\n\n\n\n\n\n\nClassification of Vector Bundles on Elliptic Curves\n\n\n\n\n\n\n\n\nNov 30, 2012\n\n\n\n\n\n\n\n\n\n\n\n\nSome Homological Algebra Computations\n\n\n\n\n\n\n\n\nNov 26, 2012\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-08-13-notes-on-the-current-state-of-llm-frameworks/index.html",
    "href": "posts/2023-08-13-notes-on-the-current-state-of-llm-frameworks/index.html",
    "title": "Notes on the Current State of LLM Frameworks",
    "section": "",
    "text": "This post tries to shed some light on the rapidly changing LLM frameworks in particular, LangChain (LC) and Llama-index (LI)."
  },
  {
    "objectID": "posts/2023-08-13-notes-on-the-current-state-of-llm-frameworks/index.html#library-vs.-framework",
    "href": "posts/2023-08-13-notes-on-the-current-state-of-llm-frameworks/index.html#library-vs.-framework",
    "title": "Notes on the Current State of LLM Frameworks",
    "section": "Library vs.¬†Framework",
    "text": "Library vs.¬†Framework\nIt‚Äôs tricky to draw a clear boundary between a package/library and a framework, but for the sake of discussion, let‚Äôs look at some well-known examplesPackages:Numpy falls into this category. It provides functionality that can be adapted to various Linear Algebra problems without dictating a particular structure or methodology.Mature Frameworks: Scikit-learn, PyTorch and HuggingFace transformers. They provide high-level abstractions but take customization and non-opinionated design seriously. As a result, they have become the de facto ways of doing ML/AI.ImmatureFrameworks:**LangChain (LC) and Llama-index (LI) are examples in this category. Unlike packages, they offer higher-level abstractions and impose a specific way of building software. (Within this category, I‚Äôve found that LI provides better abstractions than LC).\nI‚Äôve been using LC and LI for seven months now, and their evolution exemplifies a broader trend in the ML/AI world. While they‚Äôve been great for quickly creating a Proof-of-Concept, their higher-level abstractions miss many details and nuances of components that are hard to include in a clean abstraction.\nTake VectorDB/VectorStore; within a few months, their numbers have grown quickly. The quality of each is up to the individual to decide and try (I have my own suggestions, perhaps for later). For a long time, none of these frameworks offered a simple, clear CRUD API. Also, each VectorDB has its nuances of handling embeddings, from storage to search. Some support async, and some don‚Äôt. Some support gRPC, and some don‚Äôt. Some support hybrid-search, and some don‚Äôt. To use them at scale for production, we need to know about these nuances to squeeze every bit of performance. So what LC and LI did was provide a base VectorStore class and implement methods like add_documents (and their async aadd_documents) and wrap them without exposing the individual nuances of VectorDBs.\nMostly in LC, almost every abstracted away component is opinionated and hard to customize. I can add async, streaming, callback-hell, agent and memory to the list. Not everything needs to be a subclass of pydantic BaseModel! I think ‚Äútrue chaining‚Äù is not there yet in LC (should admit overloading __or__ is an interesting new way, similar to Unix pipe), and the many ways of calling a chain/agent with run, __call__, apply (now with added async) are unhealthy. These criticisms of LC are mostly to the point but I also think given the rapid change of landscape both LC and LI provide values esp.¬†as a generic off-the-shelf solutions."
  },
  {
    "objectID": "posts/2023-08-13-notes-on-the-current-state-of-llm-frameworks/index.html#conclusion-updated",
    "href": "posts/2023-08-13-notes-on-the-current-state-of-llm-frameworks/index.html#conclusion-updated",
    "title": "Notes on the Current State of LLM Frameworks",
    "section": "Conclusion (Updated)",
    "text": "Conclusion (Updated)\nI don‚Äôt think these frameworks, esp.¬†LC offers anything good long term. LI has a better chance of remaining relevant but after spending some time, I came into conclusion that many of the framework decisions were taken that are hard to change and won‚Äôt work with down the road esp.¬†if you application needs much higher control over how things are done such as preserving the structure of the documents during chunking with non-flat node/graph system, granular control over embedding of different data types, handling VectorDBs components, let alone the ‚Äúdata‚Äù agent etc."
  },
  {
    "objectID": "posts/2023-11-23-the-core-of-attention-is-communication/index.html",
    "href": "posts/2023-11-23-the-core-of-attention-is-communication/index.html",
    "title": "The Core of Attention is Communication",
    "section": "",
    "text": "Over the past year, perhaps the most cited paper across the software industry is Attention is All You Need that is at the heart of ChatGPT and GPT transformer models. The first thing you will notice in the paper is the Attention formula:\n\\[\\text{Attention(Q, K, V)} = \\text{softmax}(\\frac{QK^T}{\\sqrt{d\\_k}})V\\]\nUnfortunately, very few sources have delved into where this has come from i.e.¬†the core of the attention mechanism, and most explanations provide little to no intuition. For example, the celebrated illustrated transformer says\nWell, squinting my eyes here! That‚Äôs not enough for my taste. Now we are going to put ourselves in the mind of the authors of the paper and try to rediscover such formula."
  },
  {
    "objectID": "posts/2023-11-23-the-core-of-attention-is-communication/index.html#rediscovering-attention",
    "href": "posts/2023-11-23-the-core-of-attention-is-communication/index.html#rediscovering-attention",
    "title": "The Core of Attention is Communication",
    "section": "Rediscovering Attention",
    "text": "Rediscovering Attention\nInterestingly, the of core attention hasnothingto do with Deep Learning.Attention is in fact a communication mechanism in a graph / network. Let me explain how. Given a directed graph \\(G = (N, E)\\), where \\(N\\) is the set of nodes and \\(E\\) a set of edges, we can associatetwopieces of information to each node \\(n\\);(key, query)where key is the ‚Äúidentity‚Äù of the node, query is what thenode is looking for and is interested in finding. (Note there is no value piece yet). Take the following directed graph. The red arrows from the view point of node \\(n\\). The key is the self-arrow and the rest of the arrows make up the query piece. We haven‚Äôt yet defined key and query.\n\nWe can parameterize key and query and represent them as (learnable) vectors (in PyTorch nn.Parameter). So now looking at other nodes key vectors, the node \\(n\\) can find its interest / ‚Äúattention‚Äù score by for example computing cosine similarity (cosSim) between its query and the key of every other neighbours (connected to) i.e.¬†\\[\\text{cosSim}(w\\_q, \\hat{w}\\_k)\\]\nand gathering all the similarity scores in a vector\n\\[ \\text{scores} = (\\text{cosSim}(w\\_q, \\hat{w}\\_1), \\text{cosSim}(w\\_q, \\hat{w}\\_2), ..., \\text{cosSim}(w\\_q, \\hat{w}\\_m))\\]\nSo far so good! now what to do with this similarity vector? we need to include another piece which is thevalue. This value is the internal representation of nodes so for \\(n\\) let‚Äôs denote it by \\(w\\_v\\) (note it‚Äôs parameterized to be learned). Normally, we would start with some value. For example in the NLP, node / token embedding and adjust that iteratively. But for now, we can assume the value vector is given so we update it by the discovered similarity scores for example by element-wise multiplication\n\\[\\text{scores} \\odot w\\_v\\]\nNote it doesn‚Äôt have to be element-wise multiplication but that seems to be the simplest case here. Now, recall\n\\[\\text{cosSim}(a, b) = \\frac{ab^T}{\\| a\\| \\| b \\|}\\]\nso\n\\[\\text{scores} = (\\text{cosSim}(w\\_q, \\hat{w}\\_1), \\text{cosSim}(w\\_q, \\hat{w}\\_2), ..., \\text{cosSim}(w\\_q, \\hat{w}\\_m))\\]\nisalmostequivalent to\n\\[\\text{cosSim}(w\\_q W\\_k^T)\\]\nwhere \\(W\\_k\\) is a matrix formed by stacked all keys in rows. Note here, we have to be careful about full vector-matrix multiplication because we need to only multiply \\(w\\_q\\) to the keys of its neighbours. Let‚Äôs simplify and put that details aside. So as you can see the final rediscovered attention formula from the view point of a single node \\(n\\) is\n\\[\\text{cosSim}(w\\_q W\\_k^T) w\\_v\\]\nor putting the ‚Äúweights‚Äù \\(w\\_\\star\\) down\n\\[\\text{cosSim}(q K^T) v\\]\nand going for all nodes by stacking their vectors into matrices, we will get at\n\\[\\text{cosSim}(QK^T) V\\]\nit‚Äôs not hard to see it resembles the original attention formula\n\\[\\text{softmax}(\\frac{QK^T}{\\sqrt{d\\_k}})V\\]\ndim = 64\nw_q = nn.Parameter(torch.randn(dim))\nW_k = nn.Parameter(torch.randn(dim, dim))\nw_v = nn.Parameter(torch.randn(dim))\nattention_scores = w_q @ W_k\nattention_weights = F.softmax(attention_scores, dim=0)\noutput = attention_weights @ w_v\nIn fact, \\(\\text{cosSim}\\) was replaced with \\(\\text{softmax}\\) (and the additional \\(\\sqrt{d\\_k}\\) normalization). Also it turns out the \\(\\text{cosSim}\\) version is calledContent-base attentionand is used in Neural Turing Machine. In fact, others have tried different ‚Äúadjustments‚Äù to the formula as highlighted here."
  },
  {
    "objectID": "posts/2023-11-23-the-core-of-attention-is-communication/index.html#how-does-this-formulation-map-to-graphs-from-nlp-cv-etc",
    "href": "posts/2023-11-23-the-core-of-attention-is-communication/index.html#how-does-this-formulation-map-to-graphs-from-nlp-cv-etc",
    "title": "The Core of Attention is Communication",
    "section": "How does this formulation map to graphs from NLP, CV etc?",
    "text": "How does this formulation map to graphs from NLP, CV etc?\nNLP and Vision make explicit use of it by creating the graph suitable for their tasks. For example, in (Causal) Language Model (LM) the core of (GPT) attention graph looks like\n\nIt is auto-regressive i.e.¬†only past informations / edges are allowed so with a proper mask (lower-triangular matrix) we can zero out the future information\nattention_scores = w_q @ W_k\ntril = torch.tril(torch.ones(T, T)) # lower-triangle matrix of 1s\nattention_scores = attention_scores.masked_fill(tril == 0, float('-inf')) # masks the future and set to -inf\nattention_weights = F.softmax(attention_scores, dim=0)\noutput = attention_weights @ w_v\nOr in Vision, the attention graph of the vision transformer model (ViT) from An Image is Worth 16 x 16 Words looks like this (complete graph i.e.¬†all nodes are connected to each other)"
  },
  {
    "objectID": "posts/2023-11-23-the-core-of-attention-is-communication/index.html#summary",
    "href": "posts/2023-11-23-the-core-of-attention-is-communication/index.html#summary",
    "title": "The Core of Attention is Communication",
    "section": "Summary",
    "text": "Summary\nWe tried to rediscover the attention formula and did it by putting the attention in a higher context i.e.communication in a graph. Note that depending on the task and the graph at hand we need to make adjustments like in the LM case, by masking out future information. Also the general notion of Attention has no notion of position and that is why encoding and incorporating positions i.e.¬†positional encoding in LM is important. The complexity grows from here and we can create Multi-Head Attention and even more general Cross-Attention (with encoder-decoder). But these are less important than the core intuition I wanted to give in this post. Hope this post has clarified attention and the intuition behind it. If you are interested in more detail implementation of the Attention-is-all-You-Need paper, I recommend checking out annotated implementation in labml."
  },
  {
    "objectID": "posts/2024-03-21-semantic-search-max-engine/index.html",
    "href": "posts/2024-03-21-semantic-search-max-engine/index.html",
    "title": "Semantic Search with MAX Engine",
    "section": "",
    "text": "I wrote a blog post on the Modular blog about building semantic search applications using MAX Engine.\nSemantic search goes beyond keyword matching to understand the meaning and context of queries. This article demonstrates how to leverage MAX Engine for building powerful semantic search systems.\nKey topics covered:\n\nIntroduction to semantic search concepts\nSetting up MAX Engine for embedding generation\nBuilding a semantic search pipeline\nPractical implementation examples\nPerformance considerations and best practices\n\nRead the full article: Semantic Search with MAX Engine"
  },
  {
    "objectID": "posts/2025-01-21-hands-on-with-mojo-24-6/index.html",
    "href": "posts/2025-01-21-hands-on-with-mojo-24-6/index.html",
    "title": "Hands-on with Mojo 24.6",
    "section": "",
    "text": "I wrote a blog post on the Modular blog exploring the new features introduced in Mojo 24.6.\nThis release brings significant improvements to argument conventions, memory management, and reference tracking. The article provides practical examples to help developers understand and implement these enhancements.\nKey topics covered:\n\nNew mut keyword for mutable arguments\nOrigins system for reference tracking\nImproved memory management\nNew collection types\nPractical code examples demonstrating each feature\n\nRead the full article: Hands-on with Mojo 24.6"
  },
  {
    "objectID": "posts/2012-11-30-classification-of-vector-bundles-on-elliptic-curves-2/index.html",
    "href": "posts/2012-11-30-classification-of-vector-bundles-on-elliptic-curves-2/index.html",
    "title": "Classification of Vector Bundles on Elliptic Curves",
    "section": "",
    "text": "I‚Äôm supposed to give a talk on this subject for one of my courses, so I consider this post as a ‚Äúpre-exposition.‚Äù I learned from and heavily used the great exposition ‚ÄúVector bundles on curves‚Äù by Montserrat Teixidor I Bigas in this post. I wrote up the pre-requisites here.\nIn 1957, Atiyah in his famous paper ‚ÄúVector bundles over an elliptic curve‚Äù classified indecomposable vector bundles of arbitrary rank and degree. Briefly, every vector bundle (locally free sheaf) is decomposed uniquely (up to the order) to the direct sum of indecomposable vector bundles and the set of isomorphism classes of indecomposable vector bundles of a fixed rank and degree is isomorphic to the Jacobian of the curve which the latter is isomorphic to the curve itself.\nThe isomorphisms are canonical for vector bundles of degree zero and for higher degree, they depend on the choice of a line bundle of degree one (a base point on the curve). [‚ÄúVector bundles on curves‚Äù by Montserrat Teixidor I Bigas, section 4.]\nAn elliptic curve \\(C\\) is a smooth projective curve of genus one over an algebraically closed field \\(k.\\) One may assume that \\(\\text{char}(k)=0.\\) Throughout, the words vector bundle \\(E\\) and locally free sheaf \\(\\mathcal{E}\\) are used interchangeably.\nDenote by \\(U(r,d)\\) the set of isomorphism classes of indecomposable vector bundles of rank \\(r\\) and degree \\(d\\) where the degree of a vector bundle \\(E\\) of rank \\(r\\) is defined as the degree of the associated locally free sheaf \\(\\mathcal{E}\\) which is\n\\[\\deg(\\mathcal{E})= \\chi (\\mathcal{E})-r \\chi (\\mathcal{O})=\\chi (\\mathcal{E})-r(1-g).\\]\nOne can also show that \\(\\deg(\\mathcal{E})\\) is equal to the degree of its determinant."
  },
  {
    "objectID": "posts/2012-11-30-classification-of-vector-bundles-on-elliptic-curves-2/index.html#case-1-vector-bundles-of-degree-zero-d0",
    "href": "posts/2012-11-30-classification-of-vector-bundles-on-elliptic-curves-2/index.html#case-1-vector-bundles-of-degree-zero-d0",
    "title": "Classification of Vector Bundles on Elliptic Curves",
    "section": "Case 1: Vector bundles of degree zero (\\(d=0\\))",
    "text": "Case 1: Vector bundles of degree zero (\\(d=0\\))\nFor every positive integer \\(r\\) there exists a unique (self-dual) indecomposable vector bundle \\(\\overline{E}_{r,0}\\) of rank \\(r\\) and degree zero with only one section. It turns out that any indecomposable vector bundle of rank \\(r\\) and degree zero is isomorphic to \\(\\overline{E}_{r,0} \\otimes L\\) for a unique line bundle \\(L\\) of degree zero.\nTherefore, the set (moduli space) of indecomposable vector bundles of degree zero on \\(C,\\) i.e.¬†\\(U(r,0)\\) is (canonically) isomorphic to the moduli space of degree zero line bundles on \\(C\\) which in turn by definition is the Jacobian of \\(C.\\)"
  },
  {
    "objectID": "posts/2012-11-30-classification-of-vector-bundles-on-elliptic-curves-2/index.html#case-2-vector-bundles-of-non-negative-degree-d-geq-0-general-case",
    "href": "posts/2012-11-30-classification-of-vector-bundles-on-elliptic-curves-2/index.html#case-2-vector-bundles-of-non-negative-degree-d-geq-0-general-case",
    "title": "Classification of Vector Bundles on Elliptic Curves",
    "section": "Case 2: Vector bundles of non-negative degree (\\(d \\geq 0\\), general case)",
    "text": "Case 2: Vector bundles of non-negative degree (\\(d \\geq 0\\), general case)\nIn general, let \\(L\\) be a fixed line bundle of degree one (which corresponds to fixing a base point) on \\(C\\) then there is an isomorphism \\(U(r,d) \\xrightarrow{\\sim} U(r,d+rk)\\) sending \\(E \\mapsto E \\otimes L^k.\\) Note that \\(\\deg(E \\otimes L^k)=\\text{rk} (L^k) \\deg (E)+\\text{rk} (E) \\deg(L^k)=d+rk.\\)\nIf \\(r&gt;0\\) there is an isomorphism \\(U(r,d) \\xrightarrow{\\sim} U(r+d,d)\\) sending \\(E\\) to \\(E'\\) where \\(E'\\) is given by the following extension with \\(\\mathcal{O}^d,\\)\n\\[0 \\longrightarrow \\mathcal{O}^d \\longrightarrow E' \\longrightarrow E \\longrightarrow 0\\]\nBy these two operations, we can assume that \\(d \\geq 0\\) and \\(r&gt;0.\\) Moreover, if \\(d \\geq r\\) we will have \\(U(r,d) \\cong U(r,r-d)\\) and if \\(d&lt;r\\) then \\(U(r,d) \\cong U(r-d,d).\\) Note that none of these operations changes \\(h=\\gcd(r,d).\\)\nThus, we can construct a sequence of isomorphisms \\(U(r_i,d_i) \\cong U(r_{i+1}, d_{i+1})\\) with \\(r_i&gt;0, \\; d_i \\geq 0\\) s.t. \\(\\gcd(r_i,d_i)=\\gcd(r_{i+1},d_{i+1})\\) and \\(r_i+d_i&gt; r_{i+1}+d_{i+1}.\\) This is the sequence of positive numbers, so it will terminate when \\(r_i=h\\) and \\(d_i=0.\\) Hence, we have established the isomorphism\n\\[U(r,d) \\cong U(h,0)\\]\nand by Case 1, \\(U(h,0)\\) is isomorphic to the Jacobian of \\(C\\) (and is isomorphic to the curve itself.)\nThat being said, the above isomorphism is completely determined up to the choice of a line bundle \\(L\\) of degree one.\nSo far, we have achieved what we wanted. Let‚Äôs now try to dig more. Denote by \\(E^L_{r,d}\\) the element in \\(U(r,d)\\) corresponding to \\(\\overline{E}_{h,0}\\) in \\(U(h,0).\\)\n\n\n\n\n\n\n\nNoteProposition 1\n\n\n\n\nEvery vector bundle \\(E\\) of rank \\(r\\) and degree \\(d\\) can be written as \\(E^L_{r,d} \\otimes L'\\) for some line bundle \\(L'\\) of degree zero.\nIf \\(\\gcd(r,d)=1,\\) then \\(E^L_{r,d} \\otimes (E^L_{r,d})^*\\cong \\bigoplus^{r^2}_{i=1} L_i\\) where \\(L_i\\) run over the set of line bundles of order (in the Picard group) \\(r.\\)\nIf \\(r \\geq s,\\) then \\(\\overline{E}_{r,0} \\otimes \\overline{E}_{s,0} \\cong \\overline{E}_{r-s+1,0} \\oplus \\overline{E}_{r-s+3,0} \\oplus \\cdots \\oplus \\overline{E}_{r+s-3,0} \\oplus \\overline{E}_{r+s-1,0}.\\)\nIf \\(\\gcd(r,d)=1,\\) then \\(E^L_{r,d} \\otimes \\overline{E}_{h,0}=E^L_{rh,dh}.\\)\nIf \\(\\gcd(r,r')=\\gcd(r,d)=\\gcd(r',d')=1,\\) then \\(E^L_{r,d} \\otimes E^L_{r',d'}=E^L_{rr',rd'+r'd}.\\)\n\n\n\n\nHere are some results involving stability and semi-stability of vector bundles on \\(C.\\)\n\n\n\n\n\n\nNoteProposition 2\n\n\n\n\nAn indecomposable vector bundle of degree zero is semi-stable but not stable.\nIndecomposable vector bundles are semi-stable and they are stable if and only if \\(\\gcd(r,d)=1.\\)\n\n\n\nFor detailed and complete description of vector bundles on an elliptic curve, especially when the ground field is of \\(\\text{char}(k)=p,\\) look at Atiyah‚Äôs original paper."
  },
  {
    "objectID": "posts/2017-07-12-whats-up-with-word-embedding/index.html",
    "href": "posts/2017-07-12-whats-up-with-word-embedding/index.html",
    "title": "What‚Äôs up with word embedding?",
    "section": "",
    "text": "Word embedding is one of the interesting areas of research in Natural Language Processing. There are huge amount of materials with a lot of interesting ideas. I have been studying some of them lately and in this post, I‚Äôd like to create abrief accountof the ideas I have found most interesting so far."
  },
  {
    "objectID": "posts/2017-07-12-whats-up-with-word-embedding/index.html#word2vec-yet-another-explanation",
    "href": "posts/2017-07-12-whats-up-with-word-embedding/index.html#word2vec-yet-another-explanation",
    "title": "What‚Äôs up with word embedding?",
    "section": "Word2Vec (Yet another explanation)",
    "text": "Word2Vec (Yet another explanation)\n Let‚Äôs start with the intuitive¬†distributional hypothesis &gt; Aword is characterized by the company it keepsorlinguistic items with similar distributions have similar meanings.In other words, we expect to see words used in similarcontextshave similarmeaning (semantic relations). The goal is to build a mathematical model such that given a¬†word (token)¬†\\(w,\\) we can assign real-valued vector \\(v\\_w\\) (in some space), so that interesting properties of words such as semantic relations, are preserved¬† for example, by using their inner product or¬†cosine similarity¬†of vectors as metric. Mikolov et. al. constructed such vector representations of words from natural language text that preserve semantic relations via simple quantities of vectors such as theirinner product. Theirset of modelsis called Word2Vec. Intuitively, there are two basic ideas; for a series of words (tokens) \\(w\\_1, w\\_2, \\cdots, w\\_T\\) in a corpus/Textdata, and a choice ofcontext\\(C\\) i.e.¬†awindow (set)of words, either find the probability of some word \\(w\\_i\\) given the some word in the context \\(c \\in C\\) that is, \\(p(w\\_i | c)\\)orfind the probability of observing a context given a word \\(w\\_i,\\) that is, \\(p(c|w\\_i).\\) The first model is called Continuous Bag of Words (CBOW) and the second model is called SkipGram. (If you‚Äôve never heard of these terms before, I suggest reading this note.) One of well-known tasks that Word2Vec performed well is the analogy task. Word2Vec captures the following type of relations between words: Formally, let \\(v\\_{\\text{w}}\\) be a vector representation (embedding) of a word \\(w\\) in some Euclidean vector space. Then\n¬†\\(v\\_{\\text{man}} -¬†v\\_{\\text{woman}} \\simeq v\\_{\\text{king}} -¬†v\\_{\\text{queen}}\\)\nand one such similarity measure \\(\\simeq\\) can be¬†cosine similarity¬†of vectors, for example."
  },
  {
    "objectID": "posts/2017-07-12-whats-up-with-word-embedding/index.html#skipgram-model",
    "href": "posts/2017-07-12-whats-up-with-word-embedding/index.html#skipgram-model",
    "title": "What‚Äôs up with word embedding?",
    "section": "SkipGram model",
    "text": "SkipGram model\nGiven the notations above and the conditional probabilities \\(p(c|w),\\) the goal is to find some parameters \\(\\theta\\) of the parametrized \\(p(c|w; \\theta)\\) in order to maximize the corpus probability, i.e.\n\\(\\text{argmax}\\_{\\theta} \\prod\\_{w \\in W} \\left(\\prod\\_{c \\in C(w)} p(c | w; \\theta) \\right) \\;\\;\\;\\; (\\star)\\)\nThere is an independent assumption (reflected in $_{c C(w)} p(c | w; ) $) that given a word then observing different words in a context are independent from each other and those events for each word themselves are independent (reflected in the outer \\(\\prod\\_{w \\in \\text{W}}\\)). Note also that contexts are words and SkipGram models each context given a wordindependently. One approach to model the conditional probabilities \\(p(c | w; \\theta)\\) so as to connect the word-vector representation idea, is via softmax function\n\\(p(c | w; \\theta) = \\dfrac{\\exp(\\langle v\\_c, v\\_w \\rangle)}{\\sum\\_{c' \\in C(w)}\\exp(\\langle v\\_c', v\\_w \\rangle)} \\;\\;\\;\\; (\\star \\star)\\)\nwhere \\(v\\_c, v\\_w\\) are the desired vector representations for \\(c, w,\\) and \\(\\langle v\\_c, v\\_w \\rangle\\) is the (Euclidean) inner product. Note that \\(\\theta\\) is the set of all \\(v\\_w, v\\_c\\) for all \\(w \\in W\\) and \\(c \\in C,\\) so there are \\(|W| \\times |C| \\times d\\) number of parameters where \\(d\\) is the embedding dimension. This representation can be considered as a shallow neural network with softmax output. To address some of the difficulties on the training such as finding the denominator in the softmax there‚Äôre two proposed solutions; 1)hierarchical softmax2)negative sampling. ¬†In practice negative sampling is more favorable. To get to know more I recommend reading Word2Vec explained paper and more expanded version in Word2Vec parameter learning explained.\n\nPoint-wise mutual information matrix (PMI)\nRecall that if two random outcomes \\(x, y\\) are independent, then \\(p(x,y) = p(x) p(y).\\) That is, their join distribution factorizes into their individual distributions. To have a general measure of this phenomenon, given any two (not necessarily independent) random outcomes, we can define their Point-wise Mutual Information as \\(\\text{pmi}(x, y) = \\log \\left(\\dfrac{p(x,y)}{p(x)p(y)} \\right).\\) In case of word-context pairs, we can define the PMI matrix whose entries are \\(\\text{pmi}(w, c)\\) which is \\(|W| \\times |C|\\) matrix.\nOne can use Singular Value Decomposition on the PMI matrix to get lower dimensional representations of words. Let \\(U \\Sigma V^T\\) be the SVD of the PMI matrix, then for example, the symmetric factorization \\(W = U \\Sigma^{\\frac 12}\\) and \\(C = V \\Sigma^{\\frac 12}\\) provide word and context representations, respectively. However, SVD provides the best rank \\(d\\) approximation wrt \\(L\\_2\\) matrix norm, and in practice, this is not enough!\n\nWhat‚Äôs the relation between SkipGram and PMI?### SkipGram with negative sampling (SGNS) vs.¬†PMI\n\nLevy-Goldberg¬†showed that if you take the word embedding \\(W\\) and the context embedding \\(C\\) obtained in SGNS, then \\(WC^T\\) is in fact, a factorization of (shifted)¬†PMI matrix. In other words, \\(\\langle v\\_w, v\\_c \\rangle ¬†\\simeq \\text{PMI}(w, c) - \\log k\\) where \\(k\\) is the number of negative samples. This result bridges the neural network method with the traditional vector space model of semantics. Another important advantage is thatPMI approach suffers from scalability but SGNS is very scalable, indeed.### Variants of SGNS\n\ni) NCE:\nMinh et. al¬†used noise contrastive estimation (NCE) to model the probability of a word-context coming from correct sample vs.¬†incorrect one. Levy-Goldberg also showed that this approach is equivalent to factorizing the word-context matrix whose entries are shifted \\(\\log p(w|c).\\)\n\n\nii) GloVe:\nAnother approach is described in¬†GloVe: Global vectors for word representations. ¬†The idea is to relatedinner productof desired vectors to theratioof the context-word conditional probabilities. The optimization function that is\n\\(\\sum\\_{i,j} f(X\\_{ij}) (v\\_{w\\_i}^T v\\_{c\\_j} - \\log X\\_{ij} + b\\_{w} + b\\_c)\\)\nwhere \\(X\\_{ij}\\) is the element \\((i, j)\\) of the matrix of word-word co-occurence count \\(X,\\) \\(b\\_w, b\\_c\\) are biases and \\(f\\) is some function (found empirically) with some desired properties. Despite GloVe gives more parameters to the model,its performance is quitesimilarto SGNS. In fact, by fixing the biases to be the logarithm of word and context counts, GloVe also factorizes a shifted version of PMI matrix.\n\n\n\nSGNS as weighted logistic PCA and its generalization\nLevy-Goldberg have also provided a much more clear description of the SGNS model, which briefly goes like this: After taking \\(\\log\\) from \\((\\star)\\) and using the softmax \\((\\star \\star)\\) it ¬†becomes equivalent to\n\\(\\text{argmax}\\_{\\theta} \\sum\\_{(w, c) \\in D} \\log p(c | w; \\theta) = \\sum\\_{(w, c) \\in D} (v\\_c^T v\\_w - \\log \\sum\\_{c'} \\exp(v\\_{c'}^Tv\\_w))\\)\nwhere \\(D\\) is the set of all word-context pairs from \\(\\text{Text}.\\) The role of Negative Sampling is to approximation the softmax log of probabilities. Basically, to approximate the above objective, we change it to a classification task of \\(p(D=1|w,c) = \\dfrac{1}{1 + \\exp(v\\_c^Tv\\_w)},\\) which is given a word-context \\((w, c)\\) whether it is coming from our \\(\\text{Text}\\) or not \\(p(D=0 | w,c).\\) So we need to gather some noise word-contexts \\(D'\\). Mikolov et. al, did it by randomly sampling fromsmoothedunigram distribution (i.e.¬†to power \\(0.75\\)), \\(k\\) context noises \\(c\\_{\\text{Noise}}\\) (in short \\(c\\_N\\)) for each word \\(w.\\) Note that without negative samples, the above objective can be maximizedwhen all word and context vectors become equal ¬†\\(v\\_w = v\\_c\\) with big enough inner product. Therefore, with negative sampling, the approximation goes like this;\n\\(\\text{argmax}\\_{\\theta} \\prod\\_{(w, c) \\in D} p(D=1|c,w;\\theta)\\prod\\_{(w, c) \\in D'} p(D=0|c,w ; \\theta)\\)\nAfter some standard simplifications (see Word2Vec Explained), ¬†the above objective becomes\n\\(\\text{argmax}\\_{\\theta} \\sum\\_{(w,c) \\in D} \\log \\sigma(v\\_c^Tv\\_w) + \\sum\\_{(w, c) \\in D'} \\log \\sigma (-v\\_c^Tv\\_w) \\;\\;\\;\\; (\\star \\star \\star)\\)\nwhere \\(\\sigma\\) is the Sigmoid function. In the presence of negative samples \\(c\\_N\\), for each \\((w, c) \\in D,\\) we are computing\n\\(\\log \\sigma(v\\_c^Tv\\_w) + \\sum\\_{(w, c) \\in D'} \\log \\sigma (-v\\_c^Tv\\_w) =\\log \\sigma(v\\_c^Tv\\_w) + k \\mathbb{E}\\_{c\\_N} [\\log \\sigma (-v\\_c^Tv\\_w)]\\)\nFor a given \\((w, c)\\) let \\(n\\_{w,c}\\) be the number of times they appear in \\(D.\\) (this about \\(D\\) like matrix, \\(n\\_{w,c}\\) is the entry in row \\(w\\) and column \\(c\\)). So the SGNS objective (equation \\((\\star \\star \\star)\\)) summed with multiplicities becomes\n\\(\\sum\\_{(w, c)} n\\_{w, c} (\\log \\sigma(v\\_c^Tv\\_w) + k \\mathbb{E}\\_{c\\_N} [\\log \\sigma (-v\\_c^Tv\\_w)])\\)\nLandgrad-Bellay¬†has recently provided another interpretation of the above SGNS objective and they showed that it is equivalent to theweighted logisticPCA. The generalization of this fact is captured through exponential family PCA."
  },
  {
    "objectID": "posts/2017-07-12-whats-up-with-word-embedding/index.html#enriching-the-word-embedding",
    "href": "posts/2017-07-12-whats-up-with-word-embedding/index.html#enriching-the-word-embedding",
    "title": "What‚Äôs up with word embedding?",
    "section": "Enriching the word embedding",
    "text": "Enriching the word embedding\nAnother interesting idea is to do with the recent work of¬†Avraham-Goldberg¬†to include morphological information of words with Part of Speech (POS) tagging with preprocessing of the text and consider thepair\\((w, \\text{POS})\\) instead of the word \\(w\\) alone. The result is having different vector representations for cases like \\(\\text{plant}\\_{\\text{NOUN}}\\) and¬†\\(\\text{plant}\\_{\\text{VERB}}.\\)"
  },
  {
    "objectID": "posts/2017-07-12-whats-up-with-word-embedding/index.html#geometry-of-the-word-embedding",
    "href": "posts/2017-07-12-whats-up-with-word-embedding/index.html#geometry-of-the-word-embedding",
    "title": "What‚Äôs up with word embedding?",
    "section": "Geometry of the word embedding",
    "text": "Geometry of the word embedding\nTo understand geometric structures of data, one can look into Topological Data Analysis¬†and its methods such as Persistent Homology. Zhu¬†has an introduction of such approach for Natural Language Processing. In basic algebraic topology (with enough assumption), the dimension of zeroth homology group (zeroth Betti number) of a topological space is the number of connected components and its first Betti number counts the number of holes. Given some data points, the idea of persistent homology is totrack homological classes along increasing neighborhoods of (simplicial complexes) data points. Recently Michel et. al¬†using persistent homological approach concluded that such method doesn‚Äôt have positive impact on document classification and clustering tasks. They have used Gromov-Haussdorff distance (which is insensitive to isometries) and defined two documents have the same ‚Äúgeometry‚Äù if their GH distance if zero. However, it could be argued that this definition of ‚Äúgeometry‚Äù is very limiting and doesn‚Äôt capture all existing structures in document data!"
  },
  {
    "objectID": "posts/2017-07-12-whats-up-with-word-embedding/index.html#generalization-to-graph-and-manifold-embedding",
    "href": "posts/2017-07-12-whats-up-with-word-embedding/index.html#generalization-to-graph-and-manifold-embedding",
    "title": "What‚Äôs up with word embedding?",
    "section": "Generalization to graph and manifold embedding",
    "text": "Generalization to graph and manifold embedding\nArora et. al.¬†in their work, RAND-WALK: A latent variable model approach to word embeddings¬†with generative modelling approach, provided more justifications for relations between PMI, Word2Vec and GloVe. Hashimoto-Alvarez-Melis¬†considered the task of word embedding asmetric recovery. That is, given a word embedding \\(v\\_{w\\_1}, \\cdots, v\\_{w\\_m}\\) over a document with \\(s\\) sentences with total number \\(m\\) words and \\(n\\) vocabulary (unique words), one can view \\(p(c\\_j|w\\_i)\\) (where context is a word as well and there‚Äôs no separate context embedding vectors, such as SGNS throwing away the context vectors) as a Markov (Gaussian) random walk \\(X\\_1, \\cdots, X\\_m\\) with transition function\n\\(p(X\\_t = v\\_{w\\_j} | X\\_{t-1}=v\\_{w\\_i}) = \\dfrac{\\exp(- \\|v\\_{w\\_i} - v\\_{w\\_j} \\|\\_2^2)}{\\sum\\_{k=1}^n \\exp(- \\|v\\_{w\\_i} - v\\_{w\\_k} \\|\\_2^2)}\\)\nthen there exists a sequence \\(a\\_i^m, b\\_j^m\\) such that in probability, as \\(m \\to \\infty,\\)\n\\(-\\log(C\\_{ij}) - a\\_i^m \\stackrel{p}{\\longrightarrow} \\|v\\_{w\\_i} - v\\_{w\\_j} \\|\\_2^2 + b\\_j^m\\)\nwhere \\(C = [C\\_{ij}]\\) is the co-occurence matrix over our document. (matrix version of earlier \\(D\\) with contexts as words). This describedlog-linear relation between co-occurences and distance.The metric recovery holds in more general setting for random walk overunweighted directed graphs and data manifold.Intuitively,\n\\(-\\log(\\text{co-occurence}) \\stackrel{\\text{converges}}{\\longrightarrow} \\text{geodesic}(v\\_{w\\_i}, v\\_{w\\_j})^2\\)\nfor some meaning of convergence and distance/geodesic."
  },
  {
    "objectID": "posts/2017-07-12-whats-up-with-word-embedding/index.html#hyperbolic-embedding",
    "href": "posts/2017-07-12-whats-up-with-word-embedding/index.html#hyperbolic-embedding",
    "title": "What‚Äôs up with word embedding?",
    "section": "Hyperbolic embedding",
    "text": "Hyperbolic embedding\nWe can view words as symbolic data and try to represent their relations with graphs. To learn a representation of symbolic data with hierarchical relations (with existence of power-law distribution), one well-known approach is embedding the data in anon-Euclidean space, such as \\(d\\)-dimensionalPoincar√© ball\\(\\mathbb{B}^d = \\{x \\in \\mathbb{R}^d | \\|x\\|\\_2 &lt; 1\\}\\) (or complex upper half-plane \\(\\mathbb{H}\\)) which is the Euclidean \\(d\\) dimensional ball equipped with (non-Euclidean) Riemannian metric. In two closely published papers Chamberlain et. al studied hyperbolic embedding for \\(2\\)-dimensional Poincar√© ball and Nickel-Kiela for a general \\(d\\)-dimensional Poincar√© ball. They examined such embeddings for different types of symbolic data such as text and network graphs and showed improvements as well as the ability of capturing more information in lower dimension because hyperbolic spaces are richer than flat Euclidean spaces."
  },
  {
    "objectID": "posts/2017-07-12-whats-up-with-word-embedding/index.html#encode-decoder",
    "href": "posts/2017-07-12-whats-up-with-word-embedding/index.html#encode-decoder",
    "title": "What‚Äôs up with word embedding?",
    "section": "Encode-Decoder",
    "text": "Encode-Decoder\nAnother line of ideas is related to encode-decoder (seq2seq) approach where either encoder or decoder can be any of Convolutional NN or Recurrent NN such as LSTM or GRU. The basic idea is to encode a sequence of data (sentences) and try to reconstruct them back with decoder. In the meantime, compact representations are constructed. One such successful approach has been introduced by Kiros et. al.¬†in their Skip-Thought Vectors¬†with GRU as encoder and decoder. Given a tuple of sentences \\((s\\_{i-1}, s\\_i, s\\_{i+1}),\\) let \\(w\\_i^t\\) be the \\(t\\)-th word for sentence \\(s\\_i\\) and let \\(v\\_{w\\_i^t}\\) be its embedding. ¬†The objective is to maximize the log-probabilities for theforward and backward sentences conditioned on the encoder representation:\n\\(\\sum\\_t \\log p(w\\_{i+1}^t | w\\_{i+1}^{&lt; t}, h\\_i) +\\sum\\_t \\log p(w\\_{i-1}^t | w\\_{i-1}^{&lt; t}, h\\_i)\\)\nwhere \\(w\\_{i+1}^{&lt; t}\\) is the sequence of words in sentence \\(s\\_i\\) coming before the \\(t\\)-th words and \\(h\\_i\\) is the hidden state of the encoder GRU."
  },
  {
    "objectID": "posts/2017-07-12-whats-up-with-word-embedding/index.html#conclusion",
    "href": "posts/2017-07-12-whats-up-with-word-embedding/index.html#conclusion",
    "title": "What‚Äôs up with word embedding?",
    "section": "Conclusion",
    "text": "Conclusion\nThanks for reading this post! ¬†I‚Äôm still learning and will try to update this post if I find more interesting ideas. If you have any thoughts, please comment below."
  },
  {
    "objectID": "posts/2022-09-07-announcement-releasing-smartalloc/index.html",
    "href": "posts/2022-09-07-announcement-releasing-smartalloc/index.html",
    "title": "Announcement üì¢ Releasing smartalloc",
    "section": "",
    "text": "If you happen to write unsafe code in Rust where normal static checks are not available and want better UX for detecting memory issues along side using various sanitizers, checkout my new crate smartalloc which provides idiomatic Rust binding for the original C version here.\nBeside the reason in README, note that MIRI can‚Äôt be used now since it doesn‚Äôt support FFI function call. With Rust 1.65.0-nightly running\ncargo +nightly miri run --example undetected\nresults the error\nerror: unsupported operation: can't call foreign function: sm_malloc\n --&gt; examples/undetected.rs:6:16\n  |\n5 | #[global_allocator]\n  | ------------------- in this procedural macro expansion\n6 | static GLOBAL: SmartAlloc = SmartAlloc;\n  |                ^^^^^^^^^^ can't call foreign function: sm_malloc\n  |\n  = help: this is likely not a bug in the program; it indicates that the program performed an operation that the interpreter does not support\n  = note: BACKTRACE:\n  = note: inside `_::__rg_alloc` at examples/undetected.rs:6:16\n  = note: this error originates in the attribute macro `global_allocator` (in Nightly builds, run with -Z macro-backtrace for more info)\n\nnote: some details are omitted, run with `MIRIFLAGS=-Zmiri-backtrace=full` for a verbose backtrace"
  },
  {
    "objectID": "posts/2023-12-08-modal-labs-deep-dive/index.html",
    "href": "posts/2023-12-08-modal-labs-deep-dive/index.html",
    "title": "Modal Labs Deep Dive",
    "section": "",
    "text": "In this post, we‚Äôre going to deep dive into one of my favourite tools that are revolutionizing how Python code is run in cloud and it‚Äôs especially aimed at the computing stack for Machine Learning / Deep Learning applications, called Modal which has recently gone GA!\nThere are almost no resources besides the official documents and examples. I have been using Modal for almost a year and this post sums up crucial points on using Modal and does a series of deep dives into its inner working. Finally, we end with a few best practices.\n\n\n\nIn the last couple of years, there has been a new trend aimed at simplifying code deployment in cloud. If you‚Äôve likely seen drawbacks withInfrastructure-as-Code (IaC)tools such as Terraform, Pulumi, AWS CloudFormation, etc., you‚Äôre not alone.Each tool has its own terminology, often in the form of a DSL, library/package, or YAML extension ü§ïDecoupled from the application code, meaning that using resources in code requires separately creating the necessary cloud resources\nFor certain applications and larger companies, these requirements might be manageable. However, for smaller companies and individual developers, they can be overwhelming.\nSo, the main idea ofInfrastructure-from-Code (IfC)is to simplify IaC by being as close to the application as possible. This means introducing little to no new syntax and providing a way to create and manage cloud resources needed for deploying the application as conveniently as possible, for example, with a CLI.\nIn fact,Modal is an IfC solution for the computing stack, especially for ML/DL apps.\nIf you want to know more about IfC, have a look at my awesome-infrastructure-from-code resources.\n\n\n\n\nModal‚Äôs goal is tomake running code in the cloudfeel likeyou‚Äôre running code locally. You don‚Äôt need to run any commands to rebuild, push containers, or go to a web UI to download logs.\n\n\n\n\nFirst, make sure you‚Äôre signed in modal.com. Then the installation guide\npip install modal\npython3 -m modal setup\ninstalls the modal package which comes with its own CLI. Also configures the CLI and sets up the API token once. After that, let‚Äôs create a dev environment for our deep dive\nmodal environment create dev\nmodal config set-environment dev\nNote: All codes are available in my GitHub.\n\n\n\nLet‚Äôs see what that means. First,\ngit clone https://github.com/ehsanmok/blog && cd blog/code/modal-deep-dive\nand have a look at lib/empty.py\nimport modal\nstub = modal.Stub(\"empty\")\nLet‚Äôs deploy an empty stub via\nmodal deploy lib/empty.py\n(since I‚Äôm using Poetry, I factor out the poetry run part).\n\nand the Modal dashboard view\n\nOk! so far so good. We just deployed an empty stub. You can think about it like acontainer(either in abstract sense or like docker container) and here the container is empty! We will expand on that later.\nLet‚Äôs continue with lib/hello.py\nimport modal\n\nstub = modal.Stub(\"hello\")\n\n@stub.function()\ndef hello():\n    print(\"hello, world!\")\nLet‚Äôs take a look at Stub description (emphasis is mine)\n\nA Stub is adescriptionof how to create a Modal application. The stub object principally describes Modal objects (Function, Image, Secret, etc.) associated with the application. It hasthree responsibilities:&gt;\nSyncing of identities across processes (your local Python interpreter and every Modal worker active in your application).&gt;Making Objects stay alive and not be garbage collected for as long as the app lives (see App lifetime below).&gt;*Manage log collection for everything that happens inside your code.See what happens if we deploy our hello world via\n\nmodal deploy lib/hello.py\n\nand dashboard says it‚Äôs deployed but is ‚ÄúCurrently idle‚Äù, because it‚Äôs not receiving any input\n\nin fact in order to execute the stubbed hello function we should do modal run lib/hello.py.\n\nFrom the output, we can see it\n\nCreated a mount of some sort (will expand on that later)\nCreated our application hello\nRan the app\nFinally stopped the app"
  },
  {
    "objectID": "posts/2023-12-08-modal-labs-deep-dive/index.html#prelude",
    "href": "posts/2023-12-08-modal-labs-deep-dive/index.html#prelude",
    "title": "Modal Labs Deep Dive",
    "section": "",
    "text": "In this post, we‚Äôre going to deep dive into one of my favourite tools that are revolutionizing how Python code is run in cloud and it‚Äôs especially aimed at the computing stack for Machine Learning / Deep Learning applications, called Modal which has recently gone GA!\nThere are almost no resources besides the official documents and examples. I have been using Modal for almost a year and this post sums up crucial points on using Modal and does a series of deep dives into its inner working. Finally, we end with a few best practices.\n\n\n\nIn the last couple of years, there has been a new trend aimed at simplifying code deployment in cloud. If you‚Äôve likely seen drawbacks withInfrastructure-as-Code (IaC)tools such as Terraform, Pulumi, AWS CloudFormation, etc., you‚Äôre not alone.Each tool has its own terminology, often in the form of a DSL, library/package, or YAML extension ü§ïDecoupled from the application code, meaning that using resources in code requires separately creating the necessary cloud resources\nFor certain applications and larger companies, these requirements might be manageable. However, for smaller companies and individual developers, they can be overwhelming.\nSo, the main idea ofInfrastructure-from-Code (IfC)is to simplify IaC by being as close to the application as possible. This means introducing little to no new syntax and providing a way to create and manage cloud resources needed for deploying the application as conveniently as possible, for example, with a CLI.\nIn fact,Modal is an IfC solution for the computing stack, especially for ML/DL apps.\nIf you want to know more about IfC, have a look at my awesome-infrastructure-from-code resources.\n\n\n\n\nModal‚Äôs goal is tomake running code in the cloudfeel likeyou‚Äôre running code locally. You don‚Äôt need to run any commands to rebuild, push containers, or go to a web UI to download logs.\n\n\n\n\nFirst, make sure you‚Äôre signed in modal.com. Then the installation guide\npip install modal\npython3 -m modal setup\ninstalls the modal package which comes with its own CLI. Also configures the CLI and sets up the API token once. After that, let‚Äôs create a dev environment for our deep dive\nmodal environment create dev\nmodal config set-environment dev\nNote: All codes are available in my GitHub.\n\n\n\nLet‚Äôs see what that means. First,\ngit clone https://github.com/ehsanmok/blog && cd blog/code/modal-deep-dive\nand have a look at lib/empty.py\nimport modal\nstub = modal.Stub(\"empty\")\nLet‚Äôs deploy an empty stub via\nmodal deploy lib/empty.py\n(since I‚Äôm using Poetry, I factor out the poetry run part).\n\nand the Modal dashboard view\n\nOk! so far so good. We just deployed an empty stub. You can think about it like acontainer(either in abstract sense or like docker container) and here the container is empty! We will expand on that later.\nLet‚Äôs continue with lib/hello.py\nimport modal\n\nstub = modal.Stub(\"hello\")\n\n@stub.function()\ndef hello():\n    print(\"hello, world!\")\nLet‚Äôs take a look at Stub description (emphasis is mine)\n\nA Stub is adescriptionof how to create a Modal application. The stub object principally describes Modal objects (Function, Image, Secret, etc.) associated with the application. It hasthree responsibilities:&gt;\nSyncing of identities across processes (your local Python interpreter and every Modal worker active in your application).&gt;Making Objects stay alive and not be garbage collected for as long as the app lives (see App lifetime below).&gt;*Manage log collection for everything that happens inside your code.See what happens if we deploy our hello world via\n\nmodal deploy lib/hello.py\n\nand dashboard says it‚Äôs deployed but is ‚ÄúCurrently idle‚Äù, because it‚Äôs not receiving any input\n\nin fact in order to execute the stubbed hello function we should do modal run lib/hello.py.\n\nFrom the output, we can see it\n\nCreated a mount of some sort (will expand on that later)\nCreated our application hello\nRan the app\nFinally stopped the app"
  },
  {
    "objectID": "posts/2023-12-08-modal-labs-deep-dive/index.html#lets-dive-deep",
    "href": "posts/2023-12-08-modal-labs-deep-dive/index.html#lets-dive-deep",
    "title": "Modal Labs Deep Dive",
    "section": "Let‚Äôs Dive Deep",
    "text": "Let‚Äôs Dive Deep\nWhat happens when we do modal run?\nThe official Modal document describes it as\n\nHow does it work?\nModal takes your code, puts it in a container, and executes it in the cloud.\nWhere does it run? Modal runs it in its own cloud environment. The benefit is that we solve all the hard infrastructure problems for you, so you don‚Äôt have to do anything. You don‚Äôt need to mess with Kubernetes, Docker or even an AWS account.\nOfficial Modal doc\n\nIn the outset, Modal intelligentlywraps our code and all of its dependencies (we will get to it later) with metadatacontainerizes*and finally runs / deploys it\nA few important points:the containerization isNOTvia docker. Modal has created its ownOCI compatible container servicesuitable for ML apps including heavy duty ones. An issue with the default docker container is being slow when for example including large ML model. Modal has its own container runner (compatible with runc and gvisor) and image builder.Modal has created it‚Äôs own filesystem (in Rust ü¶Ä)that maps everything into a performant container via its FS. This innovating approach makes creating and deploying large containers fast and scalable.\n\nWe decided to not build this on top of tools like Docker/Kubernetes because we want infrastructure to befast‚Ä¶ Modal has no problem building a 100GB container, and then booting up 100 of those containers ‚Äî you can do the whole thing in a few seconds. This is what it‚Äôs built for.\nEric Bernhardsson‚Äôs (Modal CEO) blog\n\nIn fact, the essence of docker is chroot and Unix FileSystem. So with their performant custom FS, Modal has successfully created a veryperformant runtime for cloud.\nTo have a better idea of what is under-the-hood, we use modal shell\nmodal shell lib/hello.py\n‚úì Initialized. View run at https://modal.com/apps/ap-t46rTYWQKvHzcvjJ2uvpGr\n‚úì Created objects.\n‚îú‚îÄ‚îÄ üî® Created mount /Users/ehsan/workspace/blog/modal-deep-dive/lib\n‚îî‚îÄ‚îÄ üî® Created hello.\nSpawning /bin/bash\nroot@modal:~#\nroot@modal:~# ls\nintro\nroot@modal:~# cd lib/\nroot@modal:~/lib# ls\n__init__.py  empty.py  hello.py\nInteresting! modal shell spawns bash and shows where our python code is placed. Let‚Äôs see what is at the root directory\nroot@modal:~# cd /\nroot@modal:/# ls\nbin  boot  dev  dummy_plug  etc  home  lib  lib64  media  mnt  modal_requirements.txt  opt  pkg  proc  root  run  sbin  srv  sys  tmp  usr  var\nroot@modal:/# cat modal_requirements.txt\n# Pins Modal dependencies installed within the container runtime.\naiohttp==3.8.3\naiostream==0.4.4\nasgiref==3.5.2\ncertifi&gt;=2022.12.07\ncloudpickle==2.0.0;python_version&lt;'3.11'\ncloudpickle==2.2.0;python_version&gt;='3.11'\nddtrace==1.5.2;python_version&lt;'3.11'\nfastapi==0.88.0\nfastprogress==1.0.0\ngrpclib==0.4.3\nimportlib_metadata==4.8.1\nipython&gt;=7.34.0\nprotobuf&gt;=3.19.0\npython-multipart&gt;=0.0.5\nrich==12.3.0\ntblib==1.7.0\ntoml==0.10.2\ntyper==0.6.1\ntypes-certifi==2021.10.8.3\ntypes-toml==0.10.4\ntypeguard&gt;=3.0.0\nSo it looks like a usual Unix FS with a few new addops like modal_requirements.txt where we can see its content.\n\ncloudpickle\nModal uses cloudpickle which extends Python pickle for sending data back and forth. This is amajor requirementthat data must be cloudpicklable. Note thatnot everything is cloudpicklable.\nHere is an example which we can run directly in Modal python environment\nroot@modal:/# python\nPython 3.10.8 (main, Dec  6 2022, 14:24:03) [GCC 10.2.1 20210110] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; import cloudpickle\n&gt;&gt;&gt;\n&gt;&gt;&gt; class Unpickleable:\n...     def __reduce__(self):\n...         raise TypeError(\"This object cannot be pickled\")\n...\n&gt;&gt;&gt; obj = Unpickleable()\n&gt;&gt;&gt;\n&gt;&gt;&gt; try:\n...     serialized_obj = cloudpickle.dumps(obj)\n... except Exception as e:\n...     print(f\"Error: {e}\")\n...\nError: This object cannot be pickled\n\n\nWhat objects are not cloudpicklable?\nObjects that cloudpicklecannotserialize include\n1.Objects with unpickleable attributes: If an object has attributes that cannot be pickled (e.g., file handles, network connections), cloudpickle will fail to serialize it. 2.Objects with custom __reduce__ methods that raise exceptions: If an object defines a custom __reduce__ method that raises exceptions, cloudpickle will not be able to serialize it. 3.Objects from C extensions or third-party libraries: Cloudpickle may not be able to serialize objects from C extensions or third-party libraries that do not provide proper serialization support. 4.Some built-in types: While cloudpickle can serialize many built-in types, there may be limitations for certain objects like open file objects, network sockets, or database connections. 5.Functions or lambda functions with closures: Cloudpickle can serialize simple functions, but functions with complex closures (nested functions with captured variables) may not be serialized correctly.\nThe takeaway is cloudpickle almost always works and if there components that cannot be used there usually is a workaround!\n\n\nFunction proto\nNow back to Modal. Among its dependencies, there aregrpclibandprotobuf. If we have look at modal-client/blob/main/modal_proto/api.proto we will see the protobuf definitions. Of particular interest is Function and other related message Function* definitions\nmessage Function {\n  string module_name = 1;\n  string function_name = 2;\n  repeated string mount_ids = 3;\n  string image_id = 4;\n  bytes function_serialized = 6;\n\n  enum DefinitionType {\n    DEFINITION_TYPE_UNSPECIFIED = 0;\n    DEFINITION_TYPE_SERIALIZED = 1;\n    DEFINITION_TYPE_FILE = 2;\n  }\n  DefinitionType definition_type = 7;\n\n  enum FunctionType {\n    FUNCTION_TYPE_UNSPECIFIED = 0;\n    FUNCTION_TYPE_GENERATOR = 1;\n    FUNCTION_TYPE_FUNCTION = 2;\n  }\n  FunctionType function_type = 8;\n\n  Resources resources = 9;\n  repeated string secret_ids = 10;\n\n  RateLimit rate_limit = 11;\n  WebhookConfig webhook_config = 15;\n\n  repeated SharedVolumeMount shared_volume_mounts = 16;\n\n  optional string proxy_id = 17;\n\n  FunctionRetryPolicy retry_policy = 18;\n\n  uint32 concurrency_limit = 19;\n\n  bool keep_warm = 20;\n\n  uint32 timeout_secs = 21;\n\n  PTYInfo pty_info = 22;\n  bytes class_serialized = 23;\n\n  uint32 task_idle_timeout_secs = 25;\n\n  CloudProvider cloud_provider = 26;\n\n  uint32 warm_pool_size = 27;\n\n  string web_url = 28;\n  WebUrlInfo web_url_info = 29;\n\n  // If set, overrides the runtime used by the function, either \"runc\" or \"gvisor\".\n  string runtime = 30;\n\n  string stub_name = 31;\n\n  repeated VolumeMount volume_mounts = 33;\n\n  uint32 allow_concurrent_inputs = 34;\n\n  repeated CustomDomainInfo custom_domain_info = 35;\n\n  string worker_id = 36; // For internal debugging use only.\n\n  bool runtime_debug = 37; // For internal debugging use only.\n\n  // TODO: combine into enum?\n  bool is_builder_function = 32;\n  bool is_auto_snapshot = 38;\n  bool is_method = 39;\n  bool is_checkpointing_function = 40;\n\n  // Checkpoint and restore\n\n  bool checkpointing_enabled = 41;\n\n  message CheckpointInfo {\n    string checksum = 1;\n    CheckpointStatus status = 2;\n  }\n  CheckpointInfo checkpoint = 42;\n  repeated ObjectDependency object_dependencies = 43;\n}\nThere is a lot going on. We can summarize it as\n1.Basic Function Information:module_name, function_name: Identifiers for the module and function.mount_ids, image_id: Refer to identifiers for file systems or images that the function can access or is associated with. 2.Function and Definition Types:DefinitionType and FunctionType enums: These provide information about how the function is defined (serialized, file) and its type (generator, regular function). 3.Execution Resources and Configuration:Resources, RateLimit, WebhookConfig, SharedVolumeMounts: Configuration for computational resources, rate limiting, webhooks for external triggers, and shared file system mounts.proxy_id, retry_policy, concurrency_limit, keep_warm, timeout_secs: Deal with network proxy settings, how to handle retries, concurrency limits, whether to keep the function active in memory, execution timeouts, and more. 4.Advanced Function Settings:PTYInfo, class_serialized, CloudProvider, runtime, stub_name: Related to Python perhaps? (PTY), class definitions, the cloud provider specifics, the runtime environment, and some stub or placeholder name.volume_mounts, custom_domain_info, worker_id: Volume mounts for additional storage, custom domain configurations for web access, and an identifier for internal debugging. 5.Concurrency and Security:allow_concurrent_inputs, secret_ids: Settings for handling concurrent inputs and identifiers for any secrets needed by the function. 6.Debugging and Special Function Types:runtime_debug, is_builder_function, is_auto_snapshot, is_method, is_checkpointing_function: These flags seem to enable debugging, specify different roles or behaviors of the function (like being a builder, supporting auto-snapshots, etc.), and indicate if the function supports checkpointing for state persistence. 7.Checkpointing and Dependencies:checkpointing_enabled, CheckpointInfo, object_dependencies: Related to the ability to checkpoint (save the state) of the function, with details about the checkpoint and dependencies on other objects."
  },
  {
    "objectID": "posts/2023-12-08-modal-labs-deep-dive/index.html#lets-come-back-up-breathe-and-dive-deep-again",
    "href": "posts/2023-12-08-modal-labs-deep-dive/index.html#lets-come-back-up-breathe-and-dive-deep-again",
    "title": "Modal Labs Deep Dive",
    "section": "Let‚Äôs Come back up, Breathe and Dive Deep Again",
    "text": "Let‚Äôs Come back up, Breathe and Dive Deep Again\nFollowing up on the hello world, what if there are multiple modal @stub.functions under one stub?\nimport modal\n\nstub = modal.Stub(\"hello\")\n\n@stub.function()\ndef hello():\n    print(\"hello, world!\")\n\n@stub.function()\ndef hello2():\n    print(\"hello, world second time!\")\nthen modal run lib/hello_local_entry.py fails with\n\nso we need to specify which one to run. We have two optionsvia CLI: modal run lib/hello_local_entry.py::helloOr specify it in code using @stub.local_entrypoint decorator and modal run lib/hello_local_entry.py (no need for ::hello)\n@stub.function()\ndef hello():\n    print(\"hello, world!\")\n\n@stub.function()\ndef hello2():\n    print(\"hello, world second time!\")\n\n@stub.local_entrypoint()\ndef main():\n    hello.local()  # semantically is `hello()`\nSimply calling hello() doesn‚Äôt work (anymore) and Modal has its own calling semantics depending on the environment*hello.local() as above\nwhich outputs\n*Or via hello.remote()\n@stub.local_entrypoint()\ndef main():\n    # hello.local()\n    hello.remote()\n\nThere is a subtle difference in their output. Did you notice it? there is an extra None after hello, world! in the .remote() case. Why?\n\n.local() vs .remote()\nWhat happens with hello.local() tells Modal to run the code locally (your local environment such as laptop) whereas hello.remote() runs the code in the cloud and returns the result back locally.\nOne important aspect of Modal is it promises simulating cloud code execution to be as similar and local as possible. It also hashot-reloading(when code is run via modal serve which will get to that later) i.e.¬†changing your code is automatically synced and rerun in the cloud (in an ephemeral environment).\n\n\nParallel run via .map() and starmap()\nModal functions can be run in parallel using .map(...)\n@stub.function()\ndef my_func(a):\n    return a ** 2\n\n@stub.local_entrypoint()\ndef main():\n    assert list(my_func.map([1, 2, 3, 4])) == [1, 4, 9, 16]\n.starmap(...) is like¬†map¬†but spreads arguments over multiple function arguments\n@stub.function()\ndef my_func(a, b):\n    return a + b\n\n@stub.local_entrypoint()\ndef main():\n    assert list(my_func.starmap([(1, 2), (3, 4)])) == [3, 7]\n\n\nLookup and Spawn Functions Remotely\nSo far, we have tested using one stub. It‚Äôs possible to havemultiple stubs, too. In that case, it‚Äôs possible to lookup a function or spawn it remotely (via a handle). In order to do that, the function that we are looking up or spawningmust be already deployed(modal deploy) whereas if it‚Äôs in the ephemeral phase it can get deleted on the fly we can look it up in different context viamodal.Function.lookup(STUB_NAME, FUNCTION_NAME)modal.Function.from_name(STUB_NAME, FUNCTION_NAME)\nFor example, lib/square.py and model deploy square.py (notemustbe deployed to be found)\nimport modal\n\nstub = modal.Stub(\"my-shared-app\")\n\n@stub.function()\ndef square(x):\n    return x * x\nand lib/cube.py can look it up via either Function.from_name or Function.lookup and then modal run cube.py\nimport modal\n\nstub = modal.Stub(\"another-app\")\nstub.square = modal.Function.from_name(\"my-shared-app\", \"square\") # &lt;-- NOTE: this must be deployed otherwise `modal run` won't find it\n\n@stub.function()\ndef cube(x):\n    return x * stub.square.remote(x)\n\n@stub.local_entrypoint()\ndef main():\n    assert cube.remote(42) == 74088\nIt‚Äôs also possible to remotely modal.Function.spawn a function which doesn‚Äôt wait for the result but returns FunctionCall object which can bepolled. This is useful esp.¬†given a job queue and (async) spawn / poll for long running jobs in the background.\nTo see it in action, run modal run lib/cube_spawn.py\nimport time\nimport modal\nfrom modal.functions import FunctionCall\n\nstub = modal.Stub(\"cube-spawn\")\nstub.square = modal.Function.from_name(\n    \"my-shared-app\", \"square\"\n)  # &lt;-- NOTE: this must be deployed otherwise `modal run` won't find it\n\n@stub.function()\ndef spawn_square(x):\n    call = stub.square.spawn(x)\n    return {\"call_id\": call.object_id}\n\n@stub.function()\ndef poll(call_id):\n    fcall = FunctionCall.from_id(call_id)\n    try:\n        # 5 seconds timeout to simulate a long running job\n        ret = fcall.get(timeout=5)\n    except TimeoutError:\n        print(\"waiting for result\")\n        return\n\n    return ret\n\n@stub.local_entrypoint()\ndef cube():\n    call = spawn_square.remote(42)\n    call_id = call[\"call_id\"]\n    assert call_id is not None\n    ret = poll.remote(call_id)\n    assert ret * 42 == 74088\nThe important point is the caller that spawns square returns a call_id which can be retrieved and polled via FunctionCall.from_id(call_id). A less contrived example is in OCR webapp example which is\n@web_app.post(\"/parse\")\nasync def parse(request: fastapi.Request):\n    parse_receipt = Function.lookup(\"example-doc-ocr-jobs\", \"parse_receipt\")\n\n    form = await request.form()\n    receipt = await form[\"receipt\"].read()  # type: ignore\n    call = parse_receipt.spawn(receipt)\n    return {\"call_id\": call.object_id}\n\n@web_app.get(\"/result/{call_id}\")\nasync def poll_results(call_id: str):\n    from modal.functions import FunctionCall\n\n    function_call = FunctionCall.from_id(call_id)\n    try:\n        result = function_call.get(timeout=0)\n    except TimeoutError:\n        return fastapi.responses.JSONResponse(content=\"\", status_code=202)\n\n    return result\nand the frontend code uses them like\nconst resp = await fetch(\"/parse\", {\n      method: \"POST\",\n      body: formData,\n    });\n\n...\n\nconst _intervalID = setInterval(async () =&gt; {\n      const resp = await fetch(`/result/${callId}`);\n      if (resp.status === 200) {\n        setResult(await resp.json());\n      }\n    }, 100);\n\n    setIntervalId(_intervalID);\n\n\nAsync Support\nModal stubbed functions can be async as well. The only thing that changes is how they are called; await is prepended as well as .aio appendedfunc.remote(‚Ä¶)‚üπawait func.remote.aio(‚Ä¶)The magic is behind Modal‚Äôssynchronicitylibrary that makes sync and async functions to be used uniformly.\n\n\nClass Support\nModal supports decorating Python classes with @stub.cls() but requires __(a)enter__ with/without __(a)exit__ methods and @modal.method decorator for the class methods. The __(a)enter__ and __(a)exit__ come with the the followingcaveat&gt; The syntax and behavior for the¬†__(a)enter__¬†and¬†__(a)exit__¬†functions are similar to¬†context managers. However, theydo nothave the exact same semantics as Python‚Äôs corresponding¬†special methods¬†with the same name. __(a)enter__ and __(a)exit__. &gt; &gt; &gt; &gt;The container entry handler is called when a new container is started. This is useful for doing one-time initialization, such as loading model weights or importing packages that are only present in that image.&gt; &gt; Mo\nfrom modal import Stub, method\n\nstub = Stub()\n\n@stub.cls()\nclass Model:\n    def __enter__(self):\n        self.model = pickle.load(open(\"model.pickle\"))\n\n    @method()\n    def predict(self, x):\n        return self.model.predict(x)\n\n@stub.local_entrypoint()\ndef main():\n    Model().predict.remote(x) # &lt;-- it's not like context manager `with ...` at all!\nIn fact, @stub.cls is more like asyntactic sugar for functionsthat needs to load an object or make a connection once and proceed with the rest of the work. The __(a)enter__ is particularly useful for example, when we want to load a large ML model once and use it for inference."
  },
  {
    "objectID": "posts/2023-12-08-modal-labs-deep-dive/index.html#build-time-dependencies-and-resources",
    "href": "posts/2023-12-08-modal-labs-deep-dive/index.html#build-time-dependencies-and-resources",
    "title": "Modal Labs Deep Dive",
    "section": "Build time Dependencies and Resources",
    "text": "Build time Dependencies and Resources\n\nmodal.Image\nSo far, we have created and run functions with no dependency. Modal supports specifying build time dependencies via the Image object. For example,\nimage = Image.debian_slim().pip_install(\"pandas\", \"numpy\")\n@stub.function(image=image)\ndef my_function():\n    import pandas as pd\n    import numpy as np\n    ...\nThe Image object is very versatile. One can do*pip install targeting GPU\nimage = (\n    Image.debian_slim(python_version=\"3.10\")\n    .pip_install(\n        \"optimum[onnxruntime-gpu]==1.7.3\",\n        \"transformers[onnx-gpu]==4.28.1\",\n        gpu=\"A10G\",\n    )\n)\n\napt install:\n\nImage.debian_slim().apt_install(\"ffmpeg\")\n\nadd custom commands:\n\nImage.debian_slim().apt_install(\"curl\").run_commands(\n        \"curl -O https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalcatface.xml\",\n)\n\nbuild from dockerhub registry with a specific python version\n\nImage.from_registry(\"huanjason/scikit-learn\", add_python=3.10)\n\nConda\n\nImage.conda().conda_install(\"theano-pymc==1.1.2\", \"pymc3==3.11.2\")\n\nPoetry\n\nimage = modal.Image.debian_slim(python_version=\"3.10\").poetry_install_from_file(\"pyproject.toml\", without=[\"dev\"])\n\nfrom local Dockerfile\n\nImage.from_dockerfile(\"Dockerfile\")\n\nOr any combinations (also supports poetry) and with custom run_function\n\ndef download_models():\n    from sentence_transformers import SentenceTransformer\n\n    SentenceTransformer(settings.EMBEDDING_MODEL_NAME, cache_folder=EMBEDDING_MODEL_DIR)\n\nimage = (\n    modal.Image.debian_slim(python_version=\"3.10\")\n    .env({\"ENV\": ENV})\n    .run_commands(\"python -m pip install --upgrade pip\")\n    .poetry_install_from_file(\"pyproject.toml\", without=[\"dev\"])\n    .pip_install(\n        \"torch==2.1.0+cu118\",\n        find_links=\"https://download.pytorch.org/whl/torch_stable.html\",\n    )\n    .env({\"HF_HUB_ENABLE_HF_TRANSFER\": \"1\"})\n    .env({\"TRANSFORMERS_CACHE\": EMBEDDING_MODEL_DIR})\n    .run_function(download_models)\n)\nIf you have suffered from Python lack of proper dependency management, you will like this very powerful approach (yes, there are some caveat in mixing but you get the point). Also we can see how much it wouldsaveus from write our own Dockerfile, downloading the modal and include it in the image.\nSo far, we have established the mental model behind Modal (stubbed) functions as containers and how dependencies can be specified but for running a full-fledge app in cloud Modal defines its own resources. To start, similar to how docker-compose or k8s have the notion of Volume and Secrets, those can be specified in the stub too.\n\n\nModal.Secret\nFor example, Secrets can either be defined and set via Modal‚Äôs dashboard or from .env file\nsecret = modal.Secret.from_dotenv(\".env\"))\n@stub.function(\n    image=image,\n    secret=secret)\ndef example():\n    ....\n\n\nModal.Volume and Modal.NetworkFileSystem\nVolume is almost the same. As of writing this post, Modal has two ways to include Volume.modal.Volume: mutable volume built for high-performance file servingmodal.NetworkFileSystem: A shared, writable file system accessible by one or more Modal functions.\nBoth must be attached to the stub like stub.volume and mapped in @stub.function runtime\nimport pathlib\nimport modal\n\np = pathlib.Path(\"/root/foo/bar.txt\")\n\nstub = modal.Stub()\nstub.volume = modal.Volume.new()\n\n@stub.function(volumes={\"/root/foo\": stub.volume})\ndef f():\n    p.write_text(\"hello\")\n    print(f\"Created {p=}\")\n    stub.volume.commit()  # Persist changes\n    print(f\"Committed {p=}\")\nand\nimport modal\n\nstub = modal.Stub()\nvolume = modal.NetworkFileSystem.new()\n\n@stub.function(network_file_systems={\"/root/foo\": volume})\ndef f():\n    pass\n\n@stub.function(network_file_systems={\"/root/goo\": volume})\ndef g():\n    pass\nBoth can also be examined with CLI too, modal volume and modal nfs.\nOther resources like CPU/GPU, memory and more runtime configurations are also supported. For example,\n@stub.function(\n    image=image,\n    secret=secret,\n    concurrency_limit=4,\n    cpu=8,\n    gpu=\"t4\",\n    memory=1024 * 8\n    timeout=5 * 60,\n    keep_warm=True,\n    cloud=\"aws\", # as of now, 'aws', 'gcp' and oracle cloud are supported\n)\n@modal.asgi_app(label=\"server\")\ndef main() -&gt; Callable:\n    app = get_app()\n    return app\nBilling is also pay as you go.\n\n\nBuild time vs Runtime Mental Model\nUnderstanding the distinctions is very important for writing working apps. Anything that is definedoutsideof decorated stub using Modal constructs such asImage, Secret etc. are build timewhereas anything that‚Äôs specifiedinside @stub.function(...) or @stub.cls(...) is for runtime.\nFor example,\nsecret = modal.Secret.from_dotenv(\".env\"))\nimage = (\n    modal.Image.debian_slim(python_version=\"3.10\")\n    .env({\"ENV\": ENV})\n    .run_commands(\"python -m pip install --upgrade pip\")\n    .poetry_install_from_file(\"pyproject.toml\", without=[\"dev\"])\n)\nare happening at build time so ENV and .env must be available at the build time (including the imports) and the runtime specifications is done as usual. At the runtime, Modal allocates cloud resources, injects image, secrets etc. and runs our application.\n@stub.function(\n    image=image,\n    secret=secret,\n    cpu=8,\n    gpu=\"any\",\n    memory=1024 * 8,\n    cloud=\"aws\",\n)\ndef main():\n    ...\n\n\nWeb Endpoints\nAnother advantage of Modal is it can turnany functionto a web endpoint via @web_enpoint. It basically wraps the function in a FastAPI app. It can also work with any WSGI and ASGI compliant frameworks as well, including Flask, FastAPI, Sanic etc.\nfrom modal import Stub, web_endpoint\n\nstub = Stub()\n\n@stub.function()\n@web_endpoint()\ndef f():\n    return \"Hello world!\"\nFor development, we can use modal serve lib/hello_server.py which supports hot-reloading of local changes automatically. And for deployment the usual modal deploy just works. See official docs.\nThis works nicely esp.¬†for rapid development, however, in my opinion when the number of endpoints grows it‚Äôs better to use the normal FastAPI or Flask way and hand it to Modal for serving via @asgi_app or @wsgi_app, respectively.\nfrom fastapi import FastAPI, Request\nfrom fastapi.responses import HTMLResponse\n\nfrom modal import Image, Stub, asgi_app\n\nweb_app = FastAPI()\nstub = Stub()\nimage = Image.debian_slim().pip_install(\"boto3\")\n\n@web_app.post(\"/foo\")\nasync def foo(request: Request):\n    body = await request.json()\n    return body\n\n@web_app.get(\"/bar\")\nasync def bar(arg=\"world\"):\n    return HTMLResponse(f\"&lt;h1&gt;Hello Fast {arg}!&lt;/h1&gt;\")\n\n@stub.function(image=image)\n@asgi_app()\ndef fastapi_app():\n    return web_app\n\n\nModal Supports Crob-jobs, Tunnel, Sandbox and more\nThere are more resource objects available. Make sure to check out the official guide and reference doc. Some of the important ones areDict: distributed dictionaryQueue: distributed queue backed by RedisVolumeNetwork Filesystem (NFS)Period (for Cron jobs)SecretsTunnel‚Ä¶"
  },
  {
    "objectID": "posts/2023-12-08-modal-labs-deep-dive/index.html#modal-launch-jupytervscode",
    "href": "posts/2023-12-08-modal-labs-deep-dive/index.html#modal-launch-jupytervscode",
    "title": "Modal Labs Deep Dive",
    "section": "modal launch jupyter/vscode",
    "text": "modal launch jupyter/vscode\nModal CLI has the option oflaunching jupyter or vscode from modal by also configuring CPU/GPU and memory as needed. This essentially makes manual SSHing and to EC2 instancesobsolete. Jupyter is a great tool for exploratory part of ML such astraining. The vscode is just another cherry on top. Here I have launched vscode with T4 GPU using\nmodal launch vscode --gpu T4\n\nThe CLI command is basically running the following which you can customize as well.\n# Copyright Modal Labs 2023\n# type: ignore\nimport os\nimport secrets\nimport socket\nimport subprocess\nimport threading\nimport time\nimport webbrowser\nfrom typing import Any\n\nfrom modal import Image, Queue, Stub, forward\n\nargs: Any = {'cpu': 8, 'memory': 32768, 'gpu': 'T4', 'timeout': 3600}\n\nstub = Stub()\nstub.image = Image.from_registry(\"codercom/code-server\", add_python=\"3.11\").dockerfile_commands(\"ENTRYPOINT []\")\nstub.q = Queue.new()\n\ndef wait_for_port(data):\n    start_time = time.monotonic()\n    while True:\n        try:\n            with socket.create_connection((\"localhost\", 8080), timeout=15.0):\n                break\n        except OSError as exc:\n            time.sleep(0.01)\n            if time.monotonic() - start_time &gt;= 15.0:\n                raise TimeoutError(\"Waited too long for port 8080 to accept connections\") from exc\n    stub.q.put(data)\n\n@stub.function(cpu=args[\"cpu\"], memory=args[\"memory\"], gpu=args[\"gpu\"], timeout=args[\"timeout\"])\ndef run_jupyter():\n    os.chdir(\"/home/coder\")\n    token = secrets.token_urlsafe(13)\n    with forward(8080) as tunnel:\n        url = tunnel.url\n        threading.Thread(target=wait_for_port, args=((url, token),)).start()\n        subprocess.run(\n            [\"/usr/bin/entrypoint.sh\", \"--bind-addr\", \"0.0.0.0:8080\", \".\"],\n            env={**os.environ, \"SHELL\": \"/bin/bash\", \"PASSWORD\": token},\n        )\n    stub.q.put(\"done\")\n\n@stub.local_entrypoint()\ndef main():\n    stub.run_jupyter.spawn()\n    url, token = stub.q.get()\n    time.sleep(1)  # Give Jupyter a chance to start up\n    print(\"\\nVS Code on Modal, opening in browser...\")\n    print(f\"   -&gt; {url}\")\n    print(f\"   -&gt; password: {token}\\n\")\n    webbrowser.open(url)\n    assert stub.q.get() == \"done\""
  },
  {
    "objectID": "posts/2023-12-08-modal-labs-deep-dive/index.html#best-practices",
    "href": "posts/2023-12-08-modal-labs-deep-dive/index.html#best-practices",
    "title": "Modal Labs Deep Dive",
    "section": "Best Practices",
    "text": "Best Practices\nSince Modal is new there are almost no written best practices available. Here are my suggestions after using Modal for almost a year and trying many things\n\nSingle stub vs Multi-stub\nInitially there seems to be a tendency to create onenewstub per Modal function or class and use CLI to run / deploy each. If the application is simple and only a couple of stubs are used, is fine but using a single stub is not only possible also definitely more convenient for running, testing and deploying apps. In fact,a single stub can contain any number of functions and each scale independently. Sodeploy all together and scale independentlyis the idea here. At the root of my Python applications, I have\nmyapp /\n    __init__.py\n    stub.py\n    app /\n        api.py\n    services /\n        service1.py\n        service2.py\nwhere stub.py defined a global stub like\nimport modal\nfrom myapp.settings import get_settings\n\nsettings = get_settings()\nstub = modal.Stub(settings.STUB_NAME)\nstub.queue = modal.Queue.new().persisted(label=settings.STUB_QUEUE_NAME)\nand I importallthe modules that use the stub in the root __init__.py\nfrom myapp.api.app import *\nfrom myapp.services.service1 import *\nfrom myapp.services.service2 import *\nIn Python, this is not necessarily a good practice for libraries but for Modal apps isfine.\nHaving this allows running the entire application with a single command modal deploy myapp without specifying which stub to be deployed.\n\n\nTesting via @stub.local_entrypoint()\nWe can test each individual Modal function via @stub.local_entrypoint(). For example, in services/service1.py, we can have a embedding service and test it using\nENV=test modal run myapp/services/service1.py\nfrom pathlib import Path\nfrom typing import List\nimport numpy as np\nimport tqdm\nimport modal\n\nfrom myapp.settings import get_settings\nfrom myapp.stub import stub\n\nsettings = get_settings()\nsecret = modal.Secret.from_dotenv( \".env\"))\n\ndef download_models():\n    ...\n\nimage = (\n    modal.Image.debian_slim(python_version=\"3.10\")\n    ...\n    .run_function(download_models)\n)\n\n@stub.cls(image=image, secret=secret)\nclass BatchEmbedding:\n    def __enter__(self):\n        from sentence_transformers import SentenceTransformer\n\n        self.model = SentenceTransformer(\n            settings.EMBEDDING_MODEL_NAME,\n            cache_folder=EMBEDDING_MODEL_DIR,\n        )\n        return self\n\n    @modal.method()\n    def embed(self, texts) -&gt; np.ndarray:\n        vectors = self.model.encode(\n            texts,\n            show_progress_bar=True,\n            batch_size=settings.EMBED_BATCH_SIZE,\n        )\n        return vector\n\nclass DocumentEmbeddingFn:\n    def __call__(self, document: Document) -&gt; List[np.ndarray]:\n        if not len(document):\n            return []\n\n        batches: List[List[TextDoc]] = [\n            document.chunks[i : i + settings.EMBED_BATCH_SIZE]\n            for i in range(0, len(document), settings.EMBED_BATCH_SIZE)\n        ]\n        chunked_batches: List[List[str]] = [[chunk.text for chunk in batch] for batch in batches if len(batch)]\n        vectors: List[np.ndarray] = []\n        for batch in tqdm(list(BatchEmbedding().embed.map(chunked_batches))):\n            vectors.extend(batch)\n\n        return vectors\n\nif settings.ENV == \"test\":\n    import io\n    from pathlib import Path\n    from fastapi import UploadFile\n\n    @stub.local_entrypoint()\n    async def test_embedding():\n        logger.info(\"Testing embedding service with sample PDF file\")\n        data = Path(__file__) / \"tests/data.pdf\"\n        with open(str(data), \"rb\") as fin:\n            file = UploadFile(\n                file=io.BytesIO(fin.read()),\n                filename=\"data.pdf\",\n            )\n        doc = ..\n        embeddings = DocumentEmbeddingFn()(doc)\n        assert len(embeddings) == len(doc)\n\n\nManaging Dependencies\nIf you‚Äôre like me who want all dependencies to be in the same place as much as possible, I recommend using Poetry and grouping your dependencies. The simplest default case is including dev dependencies. Modal Image can build them using\npoetry_install_from_file(\"pyproject.toml\", without=[\"dev\"])\nIt‚Äôs also possible to have a fine-grained grouping like the following in pyproject.toml\n[tool.poetry.dependencies]\npython = \"&gt;=3.10,&lt;3.12\"\nmodal = \"^0.52.3366\"\n\n[tool.poetry.core.dependencies]\nnumpy = \"^1.20.0\"\npydantic = \"^2.3.0\"\n\n[tool.poetry.db.dependencies]\npymongo = { version = \"^4.5\", extras = [\"srv\"] }\n\n[tool.poetry.service1.dependencies]\n...\n\n[tool.poetry.service2.dependencies]\n...\nand build each stubbed function or class image using\npoetry_install_from_file(\"pyproject.toml\", with_=[\"core\", \"db\", \"service1\"])\nthis way centralizes the dependencies and makes upgrading and managing them easier esp.¬†for medium to large projects than inlining them via Image.debian_image().pip_install(...).\n\n\nObservability\nModal offers granular logs for each functions as well as usage. I‚Äôve been using Sentry along side Modal and hasn‚Äôt disappointed me all at. I‚Äôve found its distributed tracing and performance monitoring adequate which help debugging eps. at times when finding the relevant stacktraces is hard from the Modal logs."
  },
  {
    "objectID": "posts/2023-12-08-modal-labs-deep-dive/index.html#acknowledgement",
    "href": "posts/2023-12-08-modal-labs-deep-dive/index.html#acknowledgement",
    "title": "Modal Labs Deep Dive",
    "section": "Acknowledgement",
    "text": "Acknowledgement\nI want to thank Modal engineers in particular Akshat Bubna (Modal‚Äôs co-founder), Jonathon Belotti and Luis Capelo for their immense help, patient and being very responsive to my queries throughout using Modal. From my experience, Modal‚Äôs customer service is fantastic despite it being run by small developer team."
  },
  {
    "objectID": "posts/2020-06-08-create-your-own-programming-language-with-rust/index.html",
    "href": "posts/2020-06-08-create-your-own-programming-language-with-rust/index.html",
    "title": "Announcement üì¢ Create your own programming language with Rust",
    "section": "",
    "text": "After almost a year from my last blog post, in this short post I‚Äôm very happy to announce that I‚Äôm writing a free online book where early chapters are available now. I‚Äôve explained my motivations and goals in the introduction. The accompanying codes are also available on my GitHub.\nFeedbacks are welcome and happy learning. If you‚Äôve found the book useful, please consider donating to any of the organizations listed at either GitHub readme or at the end of the introduction chapter."
  },
  {
    "objectID": "posts/2012-11-26-some-homological-algebra-computation/index.html",
    "href": "posts/2012-11-26-some-homological-algebra-computation/index.html",
    "title": "Some Homological Algebra Computations",
    "section": "",
    "text": "In this post, I‚Äôm going to write down the detailed proofs of some of the exercises in Rotman‚Äôs Homological Algebra. They were asked in ML and then answered by me.\n\n\n\n\n\n\nNoteProblem 1\n\n\n\nLet \\(A\\) be a torsion abelian group. Then \\(\\text{Ext}^1_\\mathbb{Z}(A, \\mathbb{Z}) \\cong \\text{Hom}_\\mathbb{Z}(A,S^1)\\), where \\(S^1\\) is the unit circle.\n\n\nOne point is that the structure of the circle group is \\(\\mathbb{Q}/\\mathbb{Z} \\oplus \\mathbb{R}.\\) Now, since \\(A\\) is torsion, then \\(\\text{Hom}(A,\\mathbb{R})=0.\\) Therefore, it is enough to show that \\(\\text{Ext}^1_\\mathbb{Z}(A, \\mathbb{Z}) \\cong \\text{Hom}_\\mathbb{Z}(A,\\mathbb{Q}/\\mathbb{Z}).\\)\nWe know that \\(\\text{Ext}(A,-)\\) functor is the right derived functor of the left exact (covariant) functor \\(\\text{Hom}(A,-).\\) Consider the following exact sequence of abelian groups (injective resolution)\n\\[0 \\longrightarrow \\mathbb{Z} \\longrightarrow \\mathbb{Q} \\longrightarrow \\mathbb{Q}/\\mathbb{Z} \\longrightarrow 0\\]\nwhere obviously, the second arrow is inclusion and the third one is the projection.\nApplying our left exact (covariant) functor \\(\\text{Hom}(A, -)\\) to the above exact sequence, we will obtain the following long exact sequence\n\\[0 \\longrightarrow \\text{Hom}(A,\\mathbb{Z}) \\longrightarrow \\text{Hom}(A,\\mathbb{Q}) \\longrightarrow \\text{Hom}(A,\\mathbb{Q}/\\mathbb{Z}) \\longrightarrow \\text{Ext}^1(A, \\mathbb{Z}) \\longrightarrow 0\\]\nNotice that, \\(\\mathbb{Q}\\) and \\(\\mathbb{Q}/\\mathbb{Z}\\) are divisible groups, hence are injective \\(\\mathbb{Z}\\)-modules, thus, \\(\\text{Ext}^i(A, \\mathbb{Q})=\\text{Ext}^i(A, \\mathbb{Q}/\\mathbb{Z})=0\\) for \\(i&gt;0.\\)\nSimilarly, since \\(A\\) is torsion then \\(\\text{Hom}(A,\\mathbb{Z})=\\text{Hom}(A,\\mathbb{Q})=0.\\) Hence,\n\\[\\text{Ext}^1(A, \\mathbb{Z}) \\cong \\text{Hom}(A,\\mathbb{Q}/\\mathbb{Z})\\]\nas desired.\n\n\n\n\n\n\n\nNoteProblem 2\n\n\n\nLet \\(A, B\\) be finite abelian groups. Prove that \\(\\text{Ext}^1_\\mathbb{Z} (A,B) \\cong A \\otimes_\\mathbb{Z} B.\\)\n\n\nBy the classification of finitely generated abelian groups, we can assume that \\(A \\cong \\mathbb{Z}/q_1 \\oplus \\mathbb{Z}/q_2 \\oplus \\cdots \\oplus \\mathbb{Z}/q_k\\) and \\(B\\cong \\mathbb{Z}/p_1 \\oplus \\mathbb{Z}/p_2 \\oplus \\cdots \\oplus \\mathbb{Z}/p_l\\) where \\(q_i, p_j\\) are (not necessarily distinct) prime numbers.\nLemma: Let \\(A\\) be a finite abelian group, then\n\\[\\text{Ext}^1(A, \\mathbb{Z}/m) \\cong \\frac{\\text{Hom}(A, \\mathbb{Q}/\\mathbb{Z})}{m \\cdot \\text{Hom}(A, \\mathbb{Q}/\\mathbb{Z})}.\\]\nProof of the lemma: Consider the following injective resolution for \\(\\mathbb{Z}/m\\)\n\\[0 \\longrightarrow \\mathbb{Z}/m \\longrightarrow \\mathbb{Q}/\\mathbb{Z} \\longrightarrow \\mathbb{Q}/\\mathbb{Z} \\longrightarrow 0\\]\nwhere the second arrow is multiplication by \\(1/m\\) and the third one is multiplication by \\(m.\\)\nApplying \\(\\text{Hom}(A, -)\\) to the above short exact sequence, we will get\n\\[0 \\longrightarrow \\text{Hom}(A, \\mathbb{Z}/m) \\longrightarrow \\text{Hom}(A,\\mathbb{Q}/\\mathbb{Z}) \\longrightarrow \\text{Hom}(A,\\mathbb{Q}/\\mathbb{Z}) \\longrightarrow \\text{Ext}^1(A, \\mathbb{Z}/m) \\to 0\\]\nNote that, injectivity of \\(\\mathbb{Q}/\\mathbb{Z},\\) implies that \\(\\text{Ext}^i(A,\\mathbb{Q}/\\mathbb{Z})=0\\) for \\(i&gt;0.\\) Hence, the required isomorphism. (Notice that the third arrow is the induced multiplication by \\(m\\) from our resolution.)\nIn particular, setting \\(A=\\mathbb{Z}/n\\) in the lemma and using the result of Problem 1, we get\n\\[\\text{Ext}^1(\\mathbb{Z}/n, \\mathbb{Z}/m) \\cong \\frac{\\text{Hom}(\\mathbb{Z}/n, \\mathbb{Q}/\\mathbb{Z})}{m \\cdot \\text{Hom}(\\mathbb{Z}/n, \\mathbb{Q}/\\mathbb{Z})} \\cong \\frac{\\text{Ext}^1(\\mathbb{Z}/n, \\mathbb{Z})}{m \\cdot \\text{Ext}^1(\\mathbb{Z}/n, \\mathbb{Z})} \\cong \\frac{ \\mathbb{Z}/n}{m \\cdot \\mathbb{Z}/n} \\cong \\mathbb{Z}/d\\]\nNote that, \\(\\text{Ext}^1(\\mathbb{Z}/n, \\mathbb{Z}) \\cong \\mathbb{Z}/n\\) and \\(\\mathbb{Z}/d \\cong \\mathbb{Z}/n \\otimes \\mathbb{Z}/m\\) where \\(d=\\gcd(n,m).\\) Therefore,\n\\[\\text{Ext}^1(\\mathbb{Z}/n, \\mathbb{Z}/m) \\cong \\mathbb{Z}/n \\otimes \\mathbb{Z}/m \\quad (\\star)\\]\nUtilizing the commutativity of \\(\\text{Ext}\\) and tensor product with (finite) direct sum, and \\((\\star),\\) we obtain the required isomorphism,\n\\[\\text{Ext}^1(A,B) \\cong \\bigoplus_{i,j} \\text{Ext}^1(\\mathbb{Z}/q_i, \\mathbb{Z}/p_j) \\cong \\bigoplus_{i,j} (\\mathbb{Z}/q_i \\otimes \\mathbb{Z}/p_j) \\cong A \\otimes B.\\]\n\n\n\n\n\n\n\nNoteProblem 3\n\n\n\nLet \\(_RF\\) be a flat left \\(R\\)-module and \\(P\\) be its projective covering, i.e.¬†there is some \\(R\\)-module epimorphism \\(p: P \\rightarrow F\\). Prove that if \\(P\\) is flat, then \\(\\text{Ker}(p)\\) is flat.\n\n\nRotman‚Äôs definition of projective cover of a module \\(F\\) is indeed, an ordered pair \\((P, p),\\) where \\(P\\) is projective and \\(p:P \\to F\\) is a surjective morphism with \\(\\text{ker}(p)\\) a superfluous submodule of \\(P.\\)\nI will use the following homological characterization for (left) flat \\(R\\)-modules,\n\\[\\text{Tor}^R_n(-,M)=0 \\;\\; \\text{for all} \\;\\; n \\geq 1 \\iff {}_RM \\;\\; \\text{is a flat left } R\\text{-module}\\]\nBy assumption, \\(0 \\longrightarrow M \\hookrightarrow P \\longrightarrow F \\longrightarrow 0\\) is a short exact sequence of left \\(R\\)-modules, where \\(M:=\\text{ker}(p).\\)\nLet \\(A\\) be an arbitrary right \\(R\\)-module, then after applying the right exact functor \\(A \\otimes_R -\\) to the above exact sequence, we will have\n\\[0 \\to \\text{Tor}^R_1(A,M) \\to \\text{Tor}^R_1(A,P) \\to \\text{Tor}^R_1(A,F) \\to A \\otimes_R M \\to A \\otimes_R P \\to A \\otimes_R F \\to 0\\]\nBy the characterization, \\(\\text{Tor}^R_n(-,P)=\\text{Tor}^R_n(-,F)=0\\) for \\(n\\geq 1,\\) therefore, \\(\\text{Tor}^R_1(A,M)=0,\\) as well as \\(\\text{Tor}^R_n(-,M)\\) for \\(n\\geq 1.\\) Hence, \\(_RM\\) is a flat module."
  },
  {
    "objectID": "posts/2025-10-29-mojo-gpu-puzzles-edition-1/index.html",
    "href": "posts/2025-10-29-mojo-gpu-puzzles-edition-1/index.html",
    "title": "Mojo GPU Puzzles Edition 1",
    "section": "",
    "text": "I‚Äôm excited to announce the release of Mojo GPU Puzzles Edition 1, a hands-on guide that teaches GPU programming through 34 progressive challenges.\nGPU programming has become essential infrastructure, but most resources bury you in theory before you write a single line of code. Mojo GPU Puzzles flips this approach - you start coding immediately. No abstract lectures, just practical problems that build real understanding."
  },
  {
    "objectID": "posts/2025-10-29-mojo-gpu-puzzles-edition-1/index.html#why-mojo-for-gpu-programming",
    "href": "posts/2025-10-29-mojo-gpu-puzzles-edition-1/index.html#why-mojo-for-gpu-programming",
    "title": "Mojo GPU Puzzles Edition 1",
    "section": "Why Mojo for GPU Programming?",
    "text": "Why Mojo for GPU Programming?\n\nPython-like syntax with systems performance\nZero-cost abstractions\nStrong type system catching errors at compile time\nCross-hardware portability (NVIDIA, AMD, Apple GPUs)\nBuilt-in tensor support\nDirect access to GPU intrinsics\nSafer than C/CUDA\nWrite once, run on multiple GPU vendors"
  },
  {
    "objectID": "posts/2025-10-29-mojo-gpu-puzzles-edition-1/index.html#what-youll-learn",
    "href": "posts/2025-10-29-mojo-gpu-puzzles-edition-1/index.html#what-youll-learn",
    "title": "Mojo GPU Puzzles Edition 1",
    "section": "What You‚Äôll Learn",
    "text": "What You‚Äôll Learn\nThe same Mojo code works across NVIDIA, AMD, and Apple GPUs (partial support). Through these puzzles, you‚Äôll understand both the ‚Äúwhy‚Äù and the ‚Äúhow‚Äù of GPU programming.\nAll 34 puzzles are live and ready. Start with Puzzle 1, work through thread indexing, and build your way to tensor cores.\nLearn GPU programming by actually programming GPUs.\nStart now: puzzles.modular.com"
  },
  {
    "objectID": "posts/2019-05-13-state-of-machine-learning-in-rust/index.html",
    "href": "posts/2019-05-13-state-of-machine-learning-in-rust/index.html",
    "title": "The State of Machine Learning in Rust",
    "section": "",
    "text": "Every once in a while this topic comes up on a social media or Rust user channel. I‚Äôd like to describe briefly the way I see where things are going by a little bit of history as well as some information about existing flux of Machine Learning/Deep Learning frameworks and major recent trends."
  },
  {
    "objectID": "posts/2019-05-13-state-of-machine-learning-in-rust/index.html#brief-history-and-where-we-are-now",
    "href": "posts/2019-05-13-state-of-machine-learning-in-rust/index.html#brief-history-and-where-we-are-now",
    "title": "The State of Machine Learning in Rust",
    "section": "Brief history and where we are now?",
    "text": "Brief history and where we are now?\nExisting ML/DL ecosystems are huge because they are the combinations of High Performance Computing, Mathematical Optimization, System and Compiler Engineering, etc. etc. So for the sake of simplicity, if we go by the common breakdown of ML intotraditional MLvs.DL(overlap included), then rusty-machine, rustlearn vs.¬†leaf comes in front of our eyes. They have done very interesting and bold developments, in particular, leaf at their time, but eventually they were mostly abandoned because of the huge task of creating a complete open-source ML/DL framework which requiresVariouslanguage supports(will get into in a bit)Mature baselinear algebra and statistic crates*Acommunityof ML specialists who happen to know Rust and are willing to contribute\nDominant existing ML libraries (mostly in Python/Cython or C++) have been developed with all these supports and Rust is no exception.\n\nLanguage support and crates\nA while agoGonzalohas put up a list of HPC requirements which as of now, we can say Rust supports most of the items as language (stable/unsable) features or in crates and hopefully by the end of this year we will see more and more supports. Still constant-generics (good array support), stable std::simd and native GPU, async etc. supports are work-in-progress. Some workarounds and existing solutions namely are; generic-array (using typenum), packed simd, RustaCUDA. ForMPI, there‚Äôs an MPI-binding and forOpenMP, there‚Äôs rayon.\n\n\nLinear algebra base\nAre we learning yet? is tracking most of the signals in this area and a simple search over crates.io will tell you that we have a lot of things to cover, so when in comes to production Rust is not there yet!\nThanks toblusswho initiated ndarray and various contributors, ndarray has become the numpy of Rust i.e.¬†the base linear algebra crate (though still a lot to be done). Note that, this is very fundamental and simply wrapping BLAS/BLIS, LAPACK etc. are not enough!\nndarray has become the base for Rust ML ecosystem where others are building upon for example, ndarray-linalg, ndarray-stats.\n\n\nCommunity\nLooking back, it is fair to say people have been, more or less, experimenting with Rust for ML. I think the experimental phase is getting into its final stage, once Rust pushes the immediate requirements such as const-generic, GAT, std::simd, GPU support. I think the community is getting bigger and considering the collective efforts of the authors and contributors of the aforementioned crates, the number of ML specialists and enthusiasts is approx. where we can all get together to do interesting things by learning from and assessing existing ones (in particular in Python) to create our own curated Rust ecosystem. I think it is time to create anML Working Groupor at least for now, if you‚Äôre interested you can join rust-ml group to see how things would turn out."
  },
  {
    "objectID": "posts/2019-05-13-state-of-machine-learning-in-rust/index.html#what-about-deep-learning",
    "href": "posts/2019-05-13-state-of-machine-learning-in-rust/index.html#what-about-deep-learning",
    "title": "The State of Machine Learning in Rust",
    "section": "What about Deep Learning?",
    "text": "What about Deep Learning?\nThis is the area I‚Äôm mostly passionate about. DLfrontiersare pushing more and more into systems and compiler so that harder computations, graph level optimizations,differentiation(aka differentiable programming), efficient codegen and kernel generations etc. to happen at thecompile time. Major frontiers are; TVM, tensorflow/swift, pytorch/glow (also pytorch with TVM backend). So when it comes to Rust, all these effortscannot be ignored.\nTherefore, a (short term) solution is creating bindings. That‚Äôs what I did for TVM. Basically, we can train (mostly vision tasks now) using any DL frameworks (TensorFlow, PyTorch, MXNet) or bridge some with ONNX, then compile using TVM on varieties of supported hardwares, and forinference, we can use our beloved Rust. I should also mention the existing bindings such as tensorflow/rust and tch-rs. The major problem with these bindings is they‚Äôre limited. For example, tenorflow/rust does not have the higher abstractions that Python has now and tch-rs isfar from being safe.\nInference, in particular on edge devices, is one of the hottest areas. Another very interesting project which uses Rust for inference is tract which has good support for TF and ONNX ops. I should mention that Google‚Äôs TFLite, Tencent‚Äôs NCNN or FeatherCNN, Xiaomi‚Äôs MACE and Microsoft‚Äôs ELL are all trying to push their own solutions, but frankly, they‚Äôre still limited to certain well-known tasks and are painful to use for varieties of other tasks.\nYou might ask,how about creating a DL framework in Rust from scratch?I‚Äôd say, first read the source code of any major DL framework and try to catch up on the compiler development. Then you‚Äôll see the pieces are moving fast and haven‚Äôt even converged to a relatively complete solution. Though it could work out as averylong term solution, personally I‚Äôm not interested now."
  },
  {
    "objectID": "posts/2019-05-13-state-of-machine-learning-in-rust/index.html#final-words",
    "href": "posts/2019-05-13-state-of-machine-learning-in-rust/index.html#final-words",
    "title": "The State of Machine Learning in Rust",
    "section": "Final words",
    "text": "Final words\nI love Rust because of two main reasonsIt is very community driven and offering solutions never/less seen before by keeping the community healthy whereno-egorules and any inputs are welcomeThe community and in particular theleadershavehigh EQwhich in my opinion, is one of the most neglected cohesive forces in fruitful long lasting open-source communities\nI would love to see Rust flourishing in ML/DL domains. There are still areas that it lacks a decent crate such as aVisualizationscrate for ML type of workloads, but my bet is on Rust. I hope this post has cleared up where Rust is when it comes to ML/DL. For inputs from other people, please see the rust-ml discussion."
  },
  {
    "objectID": "posts/2024-06-25-whats-new-in-max-24-4/index.html",
    "href": "posts/2024-06-25-whats-new-in-max-24-4/index.html",
    "title": "What‚Äôs New in MAX 24.4? MAX on macOS, Fast Local Llama3, Native Quantization and GGUF Support",
    "section": "",
    "text": "I wrote a blog post on the Modular blog announcing the new features in MAX 24.4.\nThis release brings exciting capabilities including native macOS support, fast local Llama3 inference, native quantization, and GGUF format support.\nKey topics covered:\n\nMAX now available on macOS\nFast local Llama3 inference capabilities\nNative quantization support for efficient model deployment\nGGUF format support for broader model compatibility\nPerformance improvements and benchmarks\n\nRead the full article: What‚Äôs New in MAX 24.4?"
  },
  {
    "objectID": "posts/2025-01-30-agentic-building-blocks/index.html",
    "href": "posts/2025-01-30-agentic-building-blocks/index.html",
    "title": "Agentic Building Blocks: Creating AI Agents with MAX Serve and OpenAI Function Calling",
    "section": "",
    "text": "I wrote a blog post on the Modular blog about building AI agents using MAX Serve and OpenAI‚Äôs function calling capabilities.\nAI agents are becoming increasingly important in modern applications. This article provides a practical guide to creating dynamic and responsive AI agents using Modular‚Äôs MAX Serve infrastructure.\nKey topics covered:\n\nIntroduction to agentic AI patterns\nSetting up MAX Serve for agent workflows\nIntegrating OpenAI function calling\nStep-by-step guide to building your first agent\nBest practices for production deployments\n\nRead the full article: Agentic Building Blocks: Creating AI Agents with MAX Serve and OpenAI Function Calling"
  },
  {
    "objectID": "posts/2016-05-05-monty-hall-simulation/index.html",
    "href": "posts/2016-05-05-monty-hall-simulation/index.html",
    "title": "General Monty Hall Simulation",
    "section": "",
    "text": "The idea of this post came up to my mind last night. I‚Äôm assuming you have already heard about the famous¬†Monty Hall Problem¬†(if you haven‚Äôt, watch the quicker intro in Numberphile clip). Here I‚Äôd like to demonstrate a simulation taking thegeneral caseinto account, i.e.¬†assume we have \\(n\\) bins (boxes or doors, whatever) and there‚Äôs a prize in one of them and you don‚Äôt know which one has the prize. You pick one of those bins at random and since I‚Äôm thehostand I know where the prize is located, I‚Äôd choose \\(k\\) boxes and discard them from the game (obviously not the prize and not your first choice, so \\(1 \\leq k \\leq n - 2\\)). Then, I‚Äôd ask you whether you want toswitchto another box or want to stick to your first choice. Finally, I reveal your choice and see if it contains the prize or not. ¬†It‚Äôs not hard to compute the probability of winning if you do switch. That‚Äôs in fact, \\(P(\\text{Winning if switching}) = \\dfrac{n-1}{n(n-k-1)} &gt; \\dfrac{1}{n}=P(\\text{Winning if not switching})\\) Thus, thebest strategyis to alwaysswitch! Now, I‚Äôd like to confirm this withdataby doing simulation in Python.Note:All codes are available in my github¬†and¬†in nbviewer. So first, I create a MontyHall class representing my simulation object as follows:\nNow, for example, we can confirm the famous Monty Hall result by defining\nand then calling¬†print(simulation_proba(100000, 3, 1, switch=True))¬†we‚Äôll get \\(0.6665 \\simeq \\dfrac 23\\) as the winning probability if you switch, if you were to play the game \\(100,000\\) times and record all the results for the case where \\(n = 3, k = 1.\\) Let‚Äôs see the results for \\(n = 4, k = 1, 2\\)\nOkay! to better see the results, let‚Äôs plot the probabilities against the number of bins.\nor even with \\(20\\) discard options!"
  },
  {
    "objectID": "posts/2016-05-05-monty-hall-simulation/index.html#monty-hall-surface",
    "href": "posts/2016-05-05-monty-hall-simulation/index.html#monty-hall-surface",
    "title": "General Monty Hall Simulation",
    "section": "Monty Hall Surface",
    "text": "Monty Hall Surface\nNow, let‚Äôs see what the 3D surface plot looks like.\nfrom matplotlib import cm\nfrom matplotlib.ticker import LinearLocator, FormatStrFormatter\n\ndef simulation_3dplot(n_games, max_bins, max_discards, switch):\n    \"\"\" Simulation 3D plot\"\"\"\n    X = np.array(range(3, max_bins))\n    Y = np.array(range(1, max_discards))\n    X_grid, Y_grid = np.meshgrid(X, Y)\n    triu_idx = np.triu_indices(n=max_discards-1)\n    X_grid_utri, Y_grid_utri = X_grid[triu_idx], Y_grid[triu_idx]\n\n    vect_simulation_proba = np.vectorize(simulation_proba)\n    Z = vect_simulation_proba(n_games, X_grid_utri, Y_grid_utri, switch)\n    nZ = np.zeros((max_discards-1, max_discards-1))\n    nZ[triu_idx] = Z\n\n    fig = plt.figure(figsize=(8, 6))\n    ax = fig.gca(projection='3d')\n    surf = ax.plot_surface(X_grid, Y_grid, nZ, rstride=1, cstride=1, cmap=cm.coolwarm, linewidth=0, antialiased=False)\n    ax.set_zlim = (0.0, 1.0)\n    ax.set_xlabel('Number of Bins')\n    ax.set_ylabel('Number of Discards')\n    if switch:\n        ax.set_zlabel('Winning probability after switching')\n    else:\n        ax.set_zlabel('Winning probability if not switching')\n    ax.zaxis.set_major_locator(LinearLocator(5))\n    ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n    ax.set_title('Monty Hall Probability Surface for %d games' % n_games)\n\n    fig.colorbar(surf, shrink=0.5, aspect=5)\n\n    fig.savefig('3d_simulation.png', dpi=300)\n    plt.show()\n\nsimulation_3dplot(100, 11, 9, switch=True)\n\n\n\n3d_simulation"
  },
  {
    "objectID": "posts/2019-03-16-variance-in-rust-an-intuitive-explanation/index.html",
    "href": "posts/2019-03-16-variance-in-rust-an-intuitive-explanation/index.html",
    "title": "Variance in Rust: An intuitive explanation",
    "section": "",
    "text": "Recently I‚Äôve made a presentation aboutsubtyping and variance in Rustfor our local Vancouver Rust meetup, but I still think intuition was rather lost in the formalism, so here‚Äôs my shot at explaining it asintuitivelyas I can. For more succinct definitions, please checkout the presentation or the resources at the end.\nFirst, two important points that we‚Äôre going to talk about\nThings can get confusing because a lifetime parameter is actually a generic parameter, so variance and subtyping aretiedtogether."
  },
  {
    "objectID": "posts/2019-03-16-variance-in-rust-an-intuitive-explanation/index.html#intuition",
    "href": "posts/2019-03-16-variance-in-rust-an-intuitive-explanation/index.html#intuition",
    "title": "Variance in Rust: An intuitive explanation",
    "section": "Intuition",
    "text": "Intuition\nVariance in Rust is about allowing a lifetime parameter to be ok (i.e.¬†approximated) with a\n\nShorter lifetime:co-variance\nLonger lifetime:contra-variance\nNeither shorter nor longer lifetime:in-variance\n\nThese are actually theassumptionswe need to make so that we can be sure our implementation issound.Note: Wherever you see variance in Rust, bydefaultit means covariance.\nNow here‚Äôs a classic example:\nTry to guess the output first. (Playground link)\nstruct MyCell&lt;T&gt; {\n    value: T\n}\n\nimpl&lt;T: Copy&gt; MyCell&lt;T&gt; {\n    fn new(value: T) -&gt; Self {\n        MyCell { value }\n    }\n\n    fn get(&self) -&gt; T {\n        self.value\n    }\n\n    fn set(&self, new_value: T) {\n        // signature: pub unsafe fn write&lt;T&gt;(dst: *mut T, src: T)\n        // Overwrites a memory location with the given value without reading\n        // or dropping the old value\n        unsafe { std::ptr::write(&self.value as *const T as *mut T, new_value); }\n    }\n}\n\nfn foo(rcell: &MyCell&lt;&i32&gt;) {\n    let val: i32 = 13;\n    rcell.set(&val);\n    println!(\"foo set value: {}\", rcell.value);\n}\n\nfn main() {\n    static X: i32 = 10;\n    let cell = MyCell::new(&X);\n    foo(&cell);\n    println!(\"end value: {}\", cell.value);\n}\nAnd the output is\nfoo set value: 13\nend value: 32766 // ???\nIf you could guess theend valuewill be non-sense you might skip to the end and if it‚Äôs unsettling and you were hoping the compiler will guide us here, please keep reading.\nWell, before going into more details here‚Äôs the example using Cell&lt;T&gt;. Can you guess the output now? (Run in playground)\nuse std::cell::Cell;\n\nfn foo(rcell: &Cell&lt;&i32&gt;) {\n    let val: i32 = 13;\n    rcell.set(&val);\n}\n\nfn main() {\n    static X: i32 = 10;\n    let cell = Cell::new(&X);\n    foo(&cell);\n}\nAnd it doesn‚Äôt compile because of\nerror[E0597]: `val` does not live long enough\n --&gt; src/main.rs:7:15\n  |\n5 | fn foo(rcell: &Cell&lt;&i32&gt;) {\n  |                     - let's call the lifetime of this reference `'1`\n6 |     let val: i32 = 13;\n7 |     rcell.set(&val);\n  |     ----------^^^^-\n  |     |         |\n  |     |         borrowed value does not live long enough\n  |     argument requires that `val` is borrowed for `'1`\n8 | }\n  | - `val` dropped here while still borrowed\n\nerror: aborting due to previous error\nOk! this is what we expect from the compiler, right?\nTo understand where the root of the problem is in MyCell&lt;T&gt;, let‚Äôs try to analyze using nomicon‚Äôs visual representation of lifetime\nstatic X: i32 = 10;\n'a: {\n    let cell = MyCell::new(&'a X);\n    'b: {\n        // foo(&'b cell) more or less is:\n        let val: i32 = 13; // &lt;-- created here\n        'c: {\n            rcell.set(&'c val);\n            println!(\"foo set value: {}\", rcell.value);\n        }\n    } // &lt;-- val is dropped here\n    println!(\"end value: {}\", cell.value);\n}\nThe problem occurs when we‚Äôve allowed change/mutation to be ok for shorter lifetime `c than `a (co-variant assumption). However, clearly this is not sound, because val exists in `b and is dropped at the end of `b‚Äôs scope, that‚Äôs why printing the cell.value will be nonsense (content of a freed pointer!).\nClaim: It doesn‚Äôt matter how you implement set for MyCell&lt;T&gt; it‚Äôll always be unsound.\nPretty powerful claim! The reason is that the essence of MyCell&lt;T&gt; doesn‚Äôt put any restrictions on not allowing shorter lifetimes to mess up with its value. In other words, we‚Äôre kind of forgetting about any particular lifetime constraints, meaning that our MyCell&lt;T&gt; is co-variant wrt T where for our case Tis &'a i32 so is co-variant wrt `a.\nTo be able to make such claims, we need to have type-level knowledge (more succinct treatment through type constructor) and value-level knowledge is not enough. We can be pretty sure that these issues have been taken care of when using types provided for us in standard library."
  },
  {
    "objectID": "posts/2019-03-16-variance-in-rust-an-intuitive-explanation/index.html#but-how-does-cellt-enforce-in-variance",
    "href": "posts/2019-03-16-variance-in-rust-an-intuitive-explanation/index.html#but-how-does-cellt-enforce-in-variance",
    "title": "Variance in Rust: An intuitive explanation",
    "section": "But how does Cell<T> enforce in-variance?",
    "text": "But how does Cell&lt;T&gt; enforce in-variance?\nHere‚Äôs the stripped down definition of Cell&lt;T&gt;\npub struct Cell&lt;T: ?Sized&gt; {\n    value: UnsafeCell&lt;T&gt;,\n}\n\n#[lang = \"unsafe_cell\"] // --&gt; known to the compiler\npub struct UnsafeCell&lt;T: ?Sized&gt; {\n    value: T,\n}\nWait what? what‚Äôs the difference? o_0\nThe answer has deep root in compiler with the attribute #[lang = ‚Äúunsafe_cell‚Äù].\n\nCan we fix MyCell&lt;T&gt; somehow?\nIf you find yourself in a situation that need to make your type in-variant, you can include any in-variant type, such as Cell&lt;T&gt; or UnsafeCell&lt;T&gt; in PhantomData, for example with PhantomData&lt;Cell&lt;T&gt;&gt; (Exercise: Can you find other in-variant types?). Checkout how it fixes the issue.\nI hope you can see how important variance is and how the compiler is handling it for us in most cases. Complete understanding of this matter becomes really important when writing unsafe code in FFI for example.\nYou might ask how using a longer lifetime could be ok? Well, it‚Äôs kind of rare in fact. For example, forfn(&'a i32) it‚Äôs ok to use fn(&'static i32) so it iscontra-variant wrt its arguments(and fn is co-variant wrt return types in general).\nLastly, for the sake of completeness, there‚Äôs a fourth case such as most of primitive types that arebi-variant, meaning they‚Äôrebothco-variant and contra-variant."
  },
  {
    "objectID": "posts/2019-03-16-variance-in-rust-an-intuitive-explanation/index.html#resources",
    "href": "posts/2019-03-16-variance-in-rust-an-intuitive-explanation/index.html#resources",
    "title": "Variance in Rust: An intuitive explanation",
    "section": "Resources",
    "text": "Resources\nIf you‚Äôre interested in knowing more there are some great resources\n\nFelix Klock presentation\nRustonomicon.\nOther resources at the end of my presentation here.\nThe Variance RFC is excellent but don‚Äôt get confused with some historic changes.\nRust compiler guide."
  },
  {
    "objectID": "posts/2019-07-03-rust-std-study-series-alloc/index.html",
    "href": "posts/2019-07-03-rust-std-study-series-alloc/index.html",
    "title": "Rust std study series: alloc",
    "section": "",
    "text": "Let‚Äôs get deep into std::alloc!"
  },
  {
    "objectID": "posts/2019-07-03-rust-std-study-series-alloc/index.html#memory-allocator-101",
    "href": "posts/2019-07-03-rust-std-study-series-alloc/index.html#memory-allocator-101",
    "title": "Rust std study series: alloc",
    "section": "Memory allocator 101",
    "text": "Memory allocator 101\nThe very basic need for any program to compile and execute is having access to either physical memory or virtual memory. An allocator is responsible for providing such an access. You can think of an allocator as aservice, taking some sort of requests and either giving back a (pointer) toblock of memoryor some errors. In Rust, a request is a Layout i.e.¬†some meta-data about how the memory we want is supposed to take up the space.\nA memory layout is made up ofsize (in bytes)alignment (in bytes andmust be power of two): the CPU alwaysreads memoryat itsword size(4 byteson32-bitsystem or8 byteson64-bitsystem). See what‚Äôs the purpose of memory alignment.\n\nstd::alloc::Layout\n#[lang = \"alloc_layout\"]\npub struct Layout {\n    // size of the requested block of memory, measured in bytes.\n    size_: usize,\n\n    // alignment of the requested block of memory, measured in bytes.\n    // we ensure that this is always a power-of-two ...\n    align_: NonZeroUsize,\n}\nFor example, to check the size and alignment ofatypevs avalue**, we can do (playpen)\nprintln!(\"type alignment of i32 {}\", mem::align_of::&lt;i32&gt;()); // --&gt; 4 bytes\nprintln!(\"val alignment 1i32 {}\", mem::align_of_val(&1i32)); // --&gt; 4 bytes\nprintln!(\"type size i32 {}\", mem::size_of::&lt;i32&gt;()); // --&gt; 4 bytes\nprintln!(\"val size 1i32 {}\", mem::size_of_val(&1i32)); // --&gt; 4 bytes\n\n// empty struct\nstruct A;\nlet val = A;\nprintln!(\"type alignment {}\", mem::align_of::&lt;A&gt;()); // --&gt; 1 byte\nprintln!(\"val alignment {}\", mem::align_of_val(&val)); // --&gt; 1 byte\nprintln!(\"type size {}\", mem::size_of::&lt;A&gt;()); // --&gt; 0 bytes\nprintln!(\"val size {}\", mem::size_of_val(&val)); // --&gt; 0 bytes\n\n// also with\nprintln!(\"Layout of A: {:?}\", Layout::new::&lt;A&gt;()); // --&gt; Layout { size_: 0, align_: 1 }\n\n\nAre there any relations between size and align?\nLooking into std::alloc::from_size_align indicates three requirements (invariants) that must be held when constructing a layout given size and alignment:align must benon-zeroalign must be apower of twosize whenrounded up*to thenearest multipleof align,must not overflow(i.e., the rounded value must be less than usize::MAX) i.e.¬†\\(\\text{size} + \\text{align} - 1 \\leq \\text{usize::MAX}\\).\nWe can compute the theround up sizewith \\((size + align - 1) \\;\\; \\& \\;\\; !(align - 1).\\) For example, if we want to create a layout with size \\(6\\) bytes and alignment \\(8\\) bytes, then the round up size will be \\(8\\), but for size of \\(10\\) bytes and the same \\(8\\) alignment, the round up size will be \\(16\\) bytes. The difference between the rounded up size and the given size is the amount of padding needed for that alignment which are \\(2\\) and \\(6,\\) respectively. (playpen)\nWe saw in the earlier example that we can create a layout for a type T using Layout::new::&lt;T&gt;(). Moreover, given a reference &T, we can create the desirable layout with std::alloc::for_value. Basically, they are\nimpl&lt;T&gt; Layout&lt;T&gt; {\n    pub fn new&lt;T&gt;() -&gt; Self {\n        let (size, align) = (mem::size_of::&lt;T&gt;(), mem::align_of::&lt;T&gt;());\n        // Note that the align is guaranteed by rustc to be a power of two and\n        // the size+align combo is guaranteed to fit in our address space. As a\n        // result use the unchecked constructor here to avoid inserting code\n        // that panics if it isn't optimized well enough.\n        debug_assert!(Layout::from_size_align(size, align).is_ok());\n        unsafe {\n            Layout::from_size_align_unchecked(size, align)\n        }\n    }\n\n    pub fn for_value&lt;T: ?Sized&gt;(t: &T) -&gt; Self {\n        let (size, align) = (mem::size_of_val(t), mem::align_of_val(t));\n        // See rationale in `new` for why this us using an unsafe variant below\n        debug_assert!(Layout::from_size_align(size, align).is_ok());\n        unsafe {\n            Layout::from_size_align_unchecked(size, align)\n        }\n    }\n}\nSo far we have covered the layout (request part of the allocator service). Let‚Äôs look closer into the main constituent of the service."
  },
  {
    "objectID": "posts/2019-07-03-rust-std-study-series-alloc/index.html#stdallocglobalalloc",
    "href": "posts/2019-07-03-rust-std-study-series-alloc/index.html#stdallocglobalalloc",
    "title": "Rust std study series: alloc",
    "section": "std::alloc::GlobalAlloc",
    "text": "std::alloc::GlobalAlloc\npub unsafe trait GlobalAlloc {\n    // required methods\n    unsafe fn alloc(&self, layout: Layout) -&gt; *mut u8;\n    unsafe fn dealloc(&self, ptr: *mut u8, layout: Layout);\n    // provided methods\n    unsafe fn alloc_zeroed(&self, layout: Layout) -&gt; *mut u8 { ... }\n    unsafe fn realloc(\n        &self, \n        ptr: *mut u8, \n        layout: Layout, \n        new_size: usize\n    ) -&gt; *mut u8 { ... }\n}\nImplementing this trait for a type, must be followed by using the attribute #[global_allocator] so that we can register it as theglobal std allocator. For a given layout, the alloc method returns*a pointerto a* block memoryor a null pointer in case ofOut-Of-Memory (OOM) or invalid layout.The allocated block of memory may or may not be initialized**.\nUp to writing these notes, the experimental variant of GlobalAlloc trait is std::alloc::Alloc with more functionalities. Note that the default allocator provided by the OS is std::alloc::System which in case of alloc provides either anon-nullpointer (to some block of memory) or some AllocErr. This is configured as\npub struct System;\n\n// The Alloc impl just forwards to the GlobalAlloc impl, which is in `std::sys::*::alloc`.\n#[unstable(feature = \"allocator_api\", issue = \"32838\")]\nunsafe impl Alloc for System {\n    #[inline]\n    unsafe fn alloc(&mut self, layout: Layout) -&gt; Result&lt;NonNull&lt;u8&gt;, AllocErr&gt; {\n        NonNull::new(GlobalAlloc::alloc(self, layout)).ok_or(AllocErr)\n    }\n\n    #[inline]\n    unsafe fn alloc_zeroed(&mut self, layout: Layout) -&gt; Result&lt;NonNull&lt;u8&gt;, AllocErr&gt; {\n        NonNull::new(GlobalAlloc::alloc_zeroed(self, layout)).ok_or(AllocErr)\n    }\n\n    #[inline]\n    unsafe fn dealloc(&mut self, ptr: NonNull&lt;u8&gt;, layout: Layout) {\n        GlobalAlloc::dealloc(self, ptr.as_ptr(), layout)\n    }\n\n    #[inline]\n    unsafe fn realloc(&mut self,\n                      ptr: NonNull&lt;u8&gt;,\n                      layout: Layout,\n                      new_size: usize) -&gt; Result&lt;NonNull&lt;u8&gt;, AllocErr&gt; {\n        NonNull::new(GlobalAlloc::realloc(self, ptr.as_ptr(), layout, new_size)).ok_or(AllocErr)\n    }\n}\nAnd for example on unix, the System allocator is defined using libc::malloc satisfying the condition below\nunsafe impl GlobalAlloc for System {\n    #[inline]\n    unsafe fn alloc(&self, layout: Layout) -&gt; *mut u8 {\n        if layout.align() &lt;= MIN_ALIGN && layout.align() &lt;= layout.size() {\n            libc::malloc(layout.size()) as *mut u8\n        } else {\n            #[cfg(target_os = \"macos\")]\n            {\n                if layout.align() &gt; (1 &lt;&lt; 31) {\n                    return ptr::null_mut()\n                }\n            }\n            aligned_malloc(&layout) // --&gt; more or less is: libc::memalign(layout.align(), layout.size()) as *mut u8\n        }\n    }\n    ...\nThat‚Äôs it for now! we have covered most of the basics and important aspects of std::alloc."
  },
  {
    "objectID": "posts/2013-09-09-hilbert-scheme-mckay-correspondence/index.html",
    "href": "posts/2013-09-09-hilbert-scheme-mckay-correspondence/index.html",
    "title": "My Math M.Sc. Thesis: Hilbert Schemes and the Derived McKay Correspondence",
    "section": "",
    "text": "The full title is ‚ÄúThe behavior of the Hilbert scheme of points under the derived McKay correspondence‚Äù and it‚Äôs now in the UBC Library Open Collections."
  },
  {
    "objectID": "posts/2013-09-09-hilbert-scheme-mckay-correspondence/index.html#the-math-thesis",
    "href": "posts/2013-09-09-hilbert-scheme-mckay-correspondence/index.html#the-math-thesis",
    "title": "My Math M.Sc. Thesis: Hilbert Schemes and the Derived McKay Correspondence",
    "section": "",
    "text": "The full title is ‚ÄúThe behavior of the Hilbert scheme of points under the derived McKay correspondence‚Äù and it‚Äôs now in the UBC Library Open Collections."
  },
  {
    "objectID": "posts/2013-09-09-hilbert-scheme-mckay-correspondence/index.html#the-thesis",
    "href": "posts/2013-09-09-hilbert-scheme-mckay-correspondence/index.html#the-thesis",
    "title": "My Math M.Sc. Thesis: Hilbert Schemes and the Derived McKay Correspondence",
    "section": "The Thesis",
    "text": "The Thesis\n\n\n\n\nCan‚Äôt see the embed? View the thesis on UBC Library ‚Üí"
  },
  {
    "objectID": "posts/2013-09-09-hilbert-scheme-mckay-correspondence/index.html#whats-it-about",
    "href": "posts/2013-09-09-hilbert-scheme-mckay-correspondence/index.html#whats-it-about",
    "title": "My Math M.Sc. Thesis: Hilbert Schemes and the Derived McKay Correspondence",
    "section": "What‚Äôs It About?",
    "text": "What‚Äôs It About?\nThis one is pure algebraic geometry. Let me try to explain the main ideas without losing everyone.\n\nThe Setup\nStart with \\(\\mathbb{C}^2\\) (the complex plane, two copies) and quotient it by a cyclic group \\(\\mathbb{Z}/n\\mathbb{Z}\\). This creates a singularity, a Kleinian quotient singularity to be precise. These singularities have a beautiful structure and are classified by ADE Dynkin diagrams.\nNow, you can ‚Äúresolve‚Äù this singularity, meaning you replace the bad point with something smooth. The minimal crepant resolution \\(Y\\) is particularly nice because it doesn‚Äôt introduce any extra ‚Äúcanonical‚Äù badness.\n\n\nThe McKay Correspondence\nThe McKay correspondence is one of those magical results in math that connects seemingly unrelated things:\n\nRepresentation theory of the group \\(\\mathbb{Z}/n\\mathbb{Z}\\)\nGeometry of the resolution \\(Y\\)\nCombinatorics of Dynkin diagrams\n\nThe derived McKay correspondence takes this further. It says there‚Äôs an equivalence of categories:\n\\[D^b(\\text{Coh}(Y)) \\cong D^b_{\\mathbb{Z}/n}(\\text{Coh}(\\mathbb{C}^2))\\]\nThe derived category of coherent sheaves on \\(Y\\) is equivalent to the equivariant derived category on \\(\\mathbb{C}^2\\). This equivalence is given by a Fourier-Mukai transform, a powerful tool in algebraic geometry.\n\n\nThe Main Result\nMy thesis completely determines what happens to the Hilbert scheme of points under this equivalence.\nSpecifically, I look at structure sheaves of zero-dimensional, torus-invariant closed subschemes on \\(Y\\) and track their images under the Fourier-Mukai transform.\nThe punchline? There‚Äôs a beautiful combinatorial correspondence:\nPartitions ‚ÜîÔ∏é \\(\\mathbb{Z}/n\\)-colored skew partitions\nIf you know about Young diagrams and partition combinatorics, this is a satisfying result. The geometry translates into pure combinatorics."
  },
  {
    "objectID": "posts/2013-09-09-hilbert-scheme-mckay-correspondence/index.html#why-i-love-this-stuff",
    "href": "posts/2013-09-09-hilbert-scheme-mckay-correspondence/index.html#why-i-love-this-stuff",
    "title": "My Math M.Sc. Thesis: Hilbert Schemes and the Derived McKay Correspondence",
    "section": "Why I Love This Stuff",
    "text": "Why I Love This Stuff\nThere‚Äôs something deeply satisfying about derived categories. They‚Äôre abstract, sure. But they reveal structure that you can‚Äôt see otherwise.\nThe McKay correspondence is a perfect example. On the surface, you have a group acting on a space. Underneath, there‚Äôs this rich tapestry connecting algebra, geometry, and combinatorics. The derived perspective makes it all fit together."
  },
  {
    "objectID": "posts/2013-09-09-hilbert-scheme-mckay-correspondence/index.html#the-journey",
    "href": "posts/2013-09-09-hilbert-scheme-mckay-correspondence/index.html#the-journey",
    "title": "My Math M.Sc. Thesis: Hilbert Schemes and the Derived McKay Correspondence",
    "section": "The Journey",
    "text": "The Journey\nThis was my first thesis. I spent months reading papers, filling notebooks with diagrams, and trying to understand Bridgeland‚Äôs work on derived categories. There were weeks where I felt completely lost.\nBut when it clicked? When I finally saw how the Fourier-Mukai kernel worked, how the exceptional divisors corresponded to representations, how the combinatorics emerged from the geometry? That was worth every confused hour.\nMath is like that. Long periods of confusion punctuated by moments of clarity."
  },
  {
    "objectID": "posts/2013-09-09-hilbert-scheme-mckay-correspondence/index.html#links",
    "href": "posts/2013-09-09-hilbert-scheme-mckay-correspondence/index.html#links",
    "title": "My Math M.Sc. Thesis: Hilbert Schemes and the Derived McKay Correspondence",
    "section": "Links",
    "text": "Links\n\nFull thesis on UBC Library\nDOI: 10.14288/1.0074268"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Coming soon‚Ä¶"
  }
]