<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Ehsan M. Kermani">
<meta name="dcterms.date" content="2017-07-12">

<title>What’s up with word embedding? – Ehsan's Blog</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-597958c53c93a607afca12fd375c57ed.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-7ac2f9da8c2617a4fdd15004b4601015.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-597958c53c93a607afca12fd375c57ed.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-0f54ebcecf29f09aa746c60f2b36b484.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-a59265f28323c50649b21528bab209e7.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../site_libs/bootstrap/bootstrap-0f54ebcecf29f09aa746c60f2b36b484.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    processEscapes: true
  }
};
</script>
<script>
// Add reading time calculation
document.addEventListener('DOMContentLoaded', function() {
  const article = document.querySelector('.page-columns');
  if (article) {
    const text = article.textContent;
    const wordCount = text.trim().split(/\s+/).length;
    const readingTime = Math.ceil(wordCount / 200);
    const metadata = document.querySelector('.quarto-title-meta');
    if (metadata && readingTime > 0) {
      const timeDiv = document.createElement('div');
      timeDiv.className = 'quarto-title-meta-heading';
      timeDiv.innerHTML = '<div class="quarto-title-meta-heading">Reading Time</div><div class="quarto-title-meta-contents"><p class="article-reading-time">' + readingTime + ' min read</p></div>';
      metadata.appendChild(timeDiv);
    }
  }
  // Add GRR formula to navbar (centered)
  const navbarContainer = document.querySelector('.navbar > .container-fluid');
  if (navbarContainer) {
    const grr = document.createElement('span');
    grr.className = 'grr-formula';
    grr.innerHTML = 'ch(f<sub>!</sub>ℱ) = f<sub>*</sub>(ch(ℱ) · td(T<sub>f</sub>))';
    navbarContainer.appendChild(grr);
  }
});
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Ehsan’s Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts.html"> 
<span class="menu-text">Posts</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://x.com/ehsanmok"> <i class="bi bi-twitter" role="img" aria-label="X">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/ehsanmok"> <i class="bi bi-github" role="img" aria-label="GitHub">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/ehsanmkermani/"> <i class="bi bi-linkedin" role="img" aria-label="LinkedIn">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../posts.xml"> <i class="bi bi-rss" role="img" aria-label="RSS Feed">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#word2vec-yet-another-explanation" id="toc-word2vec-yet-another-explanation" class="nav-link active" data-scroll-target="#word2vec-yet-another-explanation">Word2Vec (Yet another explanation)</a></li>
  <li><a href="#skipgram-model" id="toc-skipgram-model" class="nav-link" data-scroll-target="#skipgram-model">SkipGram model</a>
  <ul class="collapse">
  <li><a href="#point-wise-mutual-information-matrix-pmi" id="toc-point-wise-mutual-information-matrix-pmi" class="nav-link" data-scroll-target="#point-wise-mutual-information-matrix-pmi">Point-wise mutual information matrix (PMI)</a></li>
  <li><a href="#sgns-as-weighted-logistic-pca-and-its-generalization" id="toc-sgns-as-weighted-logistic-pca-and-its-generalization" class="nav-link" data-scroll-target="#sgns-as-weighted-logistic-pca-and-its-generalization">SGNS as weighted logistic PCA and its generalization</a></li>
  </ul></li>
  <li><a href="#enriching-the-word-embedding" id="toc-enriching-the-word-embedding" class="nav-link" data-scroll-target="#enriching-the-word-embedding">Enriching the word embedding</a></li>
  <li><a href="#geometry-of-the-word-embedding" id="toc-geometry-of-the-word-embedding" class="nav-link" data-scroll-target="#geometry-of-the-word-embedding">Geometry of the word embedding</a></li>
  <li><a href="#generalization-to-graph-and-manifold-embedding" id="toc-generalization-to-graph-and-manifold-embedding" class="nav-link" data-scroll-target="#generalization-to-graph-and-manifold-embedding">Generalization to graph and manifold embedding</a></li>
  <li><a href="#hyperbolic-embedding" id="toc-hyperbolic-embedding" class="nav-link" data-scroll-target="#hyperbolic-embedding">Hyperbolic embedding</a></li>
  <li><a href="#encode-decoder" id="toc-encode-decoder" class="nav-link" data-scroll-target="#encode-decoder">Encode-Decoder</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">What’s up with word embedding?</h1>
  <div class="quarto-categories">
    <div class="quarto-category">Deep Learning</div>
    <div class="quarto-category">Machine Learning</div>
    <div class="quarto-category">NLProc</div>
  </div>
  </div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Ehsan M. Kermani </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">July 12, 2017</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p>Word embedding is one of the interesting areas of research in Natural Language Processing. There are huge amount of materials with a lot of interesting ideas. I have been studying some of them lately and in this post, I’d like to create a<em>brief account</em>of the ideas I have found most interesting so far.</p>
<section id="word2vec-yet-another-explanation" class="level2">
<h2 class="anchored" data-anchor-id="word2vec-yet-another-explanation">Word2Vec (Yet another explanation)</h2>
<p><img src="images/youtho.jpg" class="img-fluid" alt="youtho"> Let’s start with the intuitive&nbsp;<a href="https://en.wikipedia.org/wiki/Distributional_semantics#Distributional_hypothesis">distributional hypothesis</a> &gt; A<em>word is characterized by the company it keeps</em>or<em>linguistic items with similar distributions have similar meanings.</em>In other words, we expect to see words used in similar<em>contexts</em>have similar<em>meaning (semantic relations)</em>. The goal is to build a mathematical model such that given a&nbsp;word (token)&nbsp;<span class="math inline">\(w,\)</span> we can assign real-valued vector <span class="math inline">\(v\_w\)</span> (in some space), so that interesting properties of words such as semantic relations, are preserved&nbsp; for example, by using their inner product or&nbsp;<a href="https://en.wikipedia.org/wiki/Cosine_similarity">cosine similarity</a>&nbsp;of vectors as metric. <a href="https://arxiv.org/abs/1301.3781">Mikolov et. al.</a> constructed such vector representations of words from natural language text that preserve semantic relations via simple quantities of vectors such as their<em>inner product</em>. Their<em>set of models</em>is called Word2Vec. Intuitively, there are two basic ideas; for a series of words (tokens) <span class="math inline">\(w\_1, w\_2, \cdots, w\_T\)</span> in a corpus/<em>Text</em>data, and a choice of<em>context</em><span class="math inline">\(C\)</span> i.e.&nbsp;a<em>window (set)</em>of words, either find the probability of some word <span class="math inline">\(w\_i\)</span> given the some word in the context <span class="math inline">\(c \in C\)</span> that is, <span class="math inline">\(p(w\_i | c)\)</span><em>or</em>find the probability of observing a context given a word <span class="math inline">\(w\_i,\)</span> that is, <span class="math inline">\(p(c|w\_i).\)</span> The first model is called Continuous Bag of Words (CBOW) and the second model is called SkipGram. (If you’ve never heard of these terms before, I suggest reading <a href="https://web.stanford.edu/class/cs224n/lecture_notes/cs224n-2017-notes1.pdf">this note</a>.) One of well-known tasks that Word2Vec performed well is the analogy task. Word2Vec captures the following type of relations between words: Formally, let <span class="math inline">\(v\_{\text{w}}\)</span> be a vector representation (embedding) of a word <span class="math inline">\(w\)</span> in some Euclidean vector space. Then</p>
<p>&nbsp;<span class="math inline">\(v\_{\text{man}} -&nbsp;v\_{\text{woman}} \simeq v\_{\text{king}} -&nbsp;v\_{\text{queen}}\)</span></p>
<p>and one such similarity measure <span class="math inline">\(\simeq\)</span> can be&nbsp;<a href="https://en.wikipedia.org/wiki/Cosine_similarity">cosine similarity</a>&nbsp;of vectors, for example.</p>
</section>
<section id="skipgram-model" class="level2">
<h2 class="anchored" data-anchor-id="skipgram-model">SkipGram model</h2>
<p>Given the notations above and the conditional probabilities <span class="math inline">\(p(c|w),\)</span> the goal is to find some parameters <span class="math inline">\(\theta\)</span> of the parametrized <span class="math inline">\(p(c|w; \theta)\)</span> in order to maximize the corpus probability, i.e.</p>
<p><span class="math inline">\(\text{argmax}\_{\theta} \prod\_{w \in W} \left(\prod\_{c \in C(w)} p(c | w; \theta) \right) \;\;\;\; (\star)\)</span></p>
<p>There is an independent assumption (reflected in $_{c C(w)} p(c | w; ) $) that given a word then observing different words in a context are independent from each other and those events for each word themselves are independent (reflected in the outer <span class="math inline">\(\prod\_{w \in \text{W}}\)</span>). Note also that contexts are words and SkipGram models each context given a word<em>independently</em>. One approach to model the conditional probabilities <span class="math inline">\(p(c | w; \theta)\)</span> so as to connect the word-vector representation idea, is via softmax function</p>
<p><span class="math inline">\(p(c | w; \theta) = \dfrac{\exp(\langle v\_c, v\_w \rangle)}{\sum\_{c' \in C(w)}\exp(\langle v\_c', v\_w \rangle)} \;\;\;\; (\star \star)\)</span></p>
<p>where <span class="math inline">\(v\_c, v\_w\)</span> are the desired vector representations for <span class="math inline">\(c, w,\)</span> and <span class="math inline">\(\langle v\_c, v\_w \rangle\)</span> is the (Euclidean) inner product. Note that <span class="math inline">\(\theta\)</span> is the set of all <span class="math inline">\(v\_w, v\_c\)</span> for all <span class="math inline">\(w \in W\)</span> and <span class="math inline">\(c \in C,\)</span> so there are <span class="math inline">\(|W| \times |C| \times d\)</span> number of parameters where <span class="math inline">\(d\)</span> is the embedding dimension. This representation can be considered as a shallow neural network with softmax output. To address some of the difficulties on the training such as finding the denominator in the softmax there’re two proposed solutions; 1)<em>hierarchical softmax</em>2)<em>negative sampling</em>. &nbsp;In practice negative sampling is more favorable. To get to know more I recommend reading <a href="https://arxiv.org/abs/1402.3722">Word2Vec explained</a> paper and more expanded version in <a href="https://arxiv.org/abs/1411.2738">Word2Vec parameter learning explained</a>.</p>
<section id="point-wise-mutual-information-matrix-pmi" class="level3">
<h3 class="anchored" data-anchor-id="point-wise-mutual-information-matrix-pmi">Point-wise mutual information matrix (PMI)</h3>
<p>Recall that if two random outcomes <span class="math inline">\(x, y\)</span> are independent, then <span class="math inline">\(p(x,y) = p(x) p(y).\)</span> That is, their join distribution factorizes into their individual distributions. To have a general measure of this phenomenon, given any two (not necessarily independent) random outcomes, we can define their Point-wise Mutual Information as <span class="math inline">\(\text{pmi}(x, y) = \log \left(\dfrac{p(x,y)}{p(x)p(y)} \right).\)</span> In case of word-context pairs, we can define the PMI matrix whose entries are <span class="math inline">\(\text{pmi}(w, c)\)</span> which is <span class="math inline">\(|W| \times |C|\)</span> matrix.</p>
<p>One can use Singular Value Decomposition on the PMI matrix to get lower dimensional representations of words. Let <span class="math inline">\(U \Sigma V^T\)</span> be the SVD of the PMI matrix, then for example, the symmetric factorization <span class="math inline">\(W = U \Sigma^{\frac 12}\)</span> and <span class="math inline">\(C = V \Sigma^{\frac 12}\)</span> provide word and context representations, respectively. However, SVD provides the best rank <span class="math inline">\(d\)</span> approximation wrt <span class="math inline">\(L\_2\)</span> matrix norm, and in practice, this is not enough!</p>
<blockquote class="blockquote">
<p><em>What’s the relation between SkipGram and PMI?</em>### SkipGram with negative sampling (SGNS) vs.&nbsp;PMI</p>
</blockquote>
<p><a href="https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf">Levy-Goldberg</a>&nbsp;showed that if you take the word embedding <span class="math inline">\(W\)</span> and the context embedding <span class="math inline">\(C\)</span> obtained in SGNS, then <span class="math inline">\(WC^T\)</span> is in fact, a factorization of (shifted)&nbsp;<a href="https://en.wikipedia.org/wiki/Pointwise_mutual_information">PMI</a> matrix. In other words, <span class="math inline">\(\langle v\_w, v\_c \rangle &nbsp;\simeq \text{PMI}(w, c) - \log k\)</span> where <span class="math inline">\(k\)</span> is the number of negative samples. This result bridges the neural network method with the traditional <a href="https://arxiv.org/pdf/1003.1141.pdf">vector space model of semantics</a>. Another important advantage is that<strong>PMI approach suffers from scalability but SGNS is very scalable, indeed.</strong>### Variants of SGNS</p>
<section id="i-nce" class="level4">
<h4 class="anchored" data-anchor-id="i-nce">i) NCE:</h4>
<p><a href="http://papers.nips.cc/paper/5165-learning-word-embeddings-efficiently-with-noise-contrastive-estimation.pdf">Minh et. al</a>&nbsp;used noise contrastive estimation (NCE) to model the probability of a word-context coming from correct sample vs.&nbsp;incorrect one. Levy-Goldberg also showed that this approach is equivalent to factorizing the word-context matrix whose entries are shifted <span class="math inline">\(\log p(w|c).\)</span></p>
</section>
<section id="ii-glove" class="level4">
<h4 class="anchored" data-anchor-id="ii-glove">ii) GloVe:</h4>
<p>Another approach is described in&nbsp;<a href="https://nlp.stanford.edu/pubs/glove.pdf">GloVe: Global vectors for word representations</a>. &nbsp;The idea is to related<em>inner product</em>of desired vectors to the<em>ratio</em>of the context-word conditional probabilities. The optimization function that is</p>
<p><span class="math inline">\(\sum\_{i,j} f(X\_{ij}) (v\_{w\_i}^T v\_{c\_j} - \log X\_{ij} + b\_{w} + b\_c)\)</span></p>
<p>where <span class="math inline">\(X\_{ij}\)</span> is the element <span class="math inline">\((i, j)\)</span> of the matrix of word-word co-occurence count <span class="math inline">\(X,\)</span> <span class="math inline">\(b\_w, b\_c\)</span> are biases and <span class="math inline">\(f\)</span> is some function (found empirically) with some desired properties. Despite GloVe gives more parameters to the model,<strong>its performance is quite<em>similar</em>to SGNS</strong>. In fact, by fixing the biases to be the logarithm of word and context counts, GloVe also factorizes a shifted version of PMI matrix.</p>
</section>
</section>
<section id="sgns-as-weighted-logistic-pca-and-its-generalization" class="level3">
<h3 class="anchored" data-anchor-id="sgns-as-weighted-logistic-pca-and-its-generalization">SGNS as weighted logistic PCA and its generalization</h3>
<p>Levy-Goldberg have also provided a much more clear description of the SGNS model, which briefly goes like this: After taking <span class="math inline">\(\log\)</span> from <span class="math inline">\((\star)\)</span> and using the softmax <span class="math inline">\((\star \star)\)</span> it &nbsp;becomes equivalent to</p>
<p><span class="math inline">\(\text{argmax}\_{\theta} \sum\_{(w, c) \in D} \log p(c | w; \theta) = \sum\_{(w, c) \in D} (v\_c^T v\_w - \log \sum\_{c'} \exp(v\_{c'}^Tv\_w))\)</span></p>
<p>where <span class="math inline">\(D\)</span> is the set of all word-context pairs from <span class="math inline">\(\text{Text}.\)</span> The role of Negative Sampling is to approximation the softmax log of probabilities. Basically, to approximate the above objective, we change it to a classification task of <span class="math inline">\(p(D=1|w,c) = \dfrac{1}{1 + \exp(v\_c^Tv\_w)},\)</span> which is given a word-context <span class="math inline">\((w, c)\)</span> whether it is coming from our <span class="math inline">\(\text{Text}\)</span> or not <span class="math inline">\(p(D=0 | w,c).\)</span> So we need to gather some noise word-contexts <span class="math inline">\(D'\)</span>. Mikolov et. al, did it by randomly sampling from<em>smoothed</em>unigram distribution (i.e.&nbsp;to power <span class="math inline">\(0.75\)</span>), <span class="math inline">\(k\)</span> context noises <span class="math inline">\(c\_{\text{Noise}}\)</span> (in short <span class="math inline">\(c\_N\)</span>) for each word <span class="math inline">\(w.\)</span> Note that without negative samples, the above objective can be maximized<em>when all word and context vectors become equal &nbsp;<span class="math inline">\(v\_w = v\_c\)</span> with big enough inner product</em>. Therefore, with negative sampling, the approximation goes like this;</p>
<p><span class="math inline">\(\text{argmax}\_{\theta} \prod\_{(w, c) \in D} p(D=1|c,w;\theta)\prod\_{(w, c) \in D'} p(D=0|c,w ; \theta)\)</span></p>
<p>After some standard simplifications (see <a href="https://arxiv.org/abs/1402.3722">Word2Vec Explained</a>), &nbsp;the above objective becomes</p>
<p><span class="math inline">\(\text{argmax}\_{\theta} \sum\_{(w,c) \in D} \log \sigma(v\_c^Tv\_w) + \sum\_{(w, c) \in D'} \log \sigma (-v\_c^Tv\_w) \;\;\;\; (\star \star \star)\)</span></p>
<p>where <span class="math inline">\(\sigma\)</span> is the Sigmoid function. In the presence of negative samples <span class="math inline">\(c\_N\)</span>, for each <span class="math inline">\((w, c) \in D,\)</span> we are computing</p>
<p><span class="math inline">\(\log \sigma(v\_c^Tv\_w) + \sum\_{(w, c) \in D'} \log \sigma (-v\_c^Tv\_w) =\log \sigma(v\_c^Tv\_w) + k \mathbb{E}\_{c\_N} [\log \sigma (-v\_c^Tv\_w)]\)</span></p>
<p>For a given <span class="math inline">\((w, c)\)</span> let <span class="math inline">\(n\_{w,c}\)</span> be the number of times they appear in <span class="math inline">\(D.\)</span> (this about <span class="math inline">\(D\)</span> like matrix, <span class="math inline">\(n\_{w,c}\)</span> is the entry in row <span class="math inline">\(w\)</span> and column <span class="math inline">\(c\)</span>). So the SGNS objective (equation <span class="math inline">\((\star \star \star)\)</span>) summed with multiplicities becomes</p>
<p><span class="math inline">\(\sum\_{(w, c)} n\_{w, c} (\log \sigma(v\_c^Tv\_w) + k \mathbb{E}\_{c\_N} [\log \sigma (-v\_c^Tv\_w)])\)</span></p>
<p><a href="https://arxiv.org/abs/1705.09755">Landgrad-Bellay</a>&nbsp;has recently provided another interpretation of the above SGNS objective and they showed that it is equivalent to the<em>weighted logistic</em>PCA. The generalization of this fact is captured through <a href="https://www.cs.jhu.edu/~jason/papers/cotterell+al.eacl17.pdf">exponential family PCA</a>.</p>
</section>
</section>
<section id="enriching-the-word-embedding" class="level2">
<h2 class="anchored" data-anchor-id="enriching-the-word-embedding">Enriching the word embedding</h2>
<p>Another interesting idea is to do with the recent work of&nbsp;<a href="https://arxiv.org/pdf/1704.01938.pdf">Avraham-Goldberg</a>&nbsp;to include morphological information of words with Part of Speech (POS) tagging with preprocessing of the text and consider the<em>pair</em><span class="math inline">\((w, \text{POS})\)</span> instead of the word <span class="math inline">\(w\)</span> alone. The result is having different vector representations for cases like <span class="math inline">\(\text{plant}\_{\text{NOUN}}\)</span> and&nbsp;<span class="math inline">\(\text{plant}\_{\text{VERB}}.\)</span></p>
</section>
<section id="geometry-of-the-word-embedding" class="level2">
<h2 class="anchored" data-anchor-id="geometry-of-the-word-embedding">Geometry of the word embedding</h2>
<p>To understand geometric structures of data, one can look into <a href="https://en.wikipedia.org/wiki/Topological_data_analysis">Topological Data Analysis</a>&nbsp;and its methods such as <a href="https://en.wikipedia.org/wiki/Persistent_homology">Persistent Homology</a>. <a href="http://pages.cs.wisc.edu/~jerryzhu/pub/homology.pdf">Zhu</a>&nbsp;has an introduction of such approach for Natural Language Processing. In basic algebraic topology (with enough assumption), the dimension of zeroth homology group (zeroth Betti number) of a topological space is the number of connected components and its first Betti number counts the number of holes. Given some data points, the idea of persistent homology is to<em>track homological classes along increasing neighborhoods of (simplicial complexes) data points</em>. Recently <a href="https://arxiv.org/abs/1705.10900">Michel et. al</a>&nbsp;using persistent homological approach concluded that such method doesn’t have positive impact on document classification and clustering tasks. They have used Gromov-Haussdorff distance (which is insensitive to isometries) and defined two documents have the same “geometry” if their GH distance if zero. However, it could be argued that this definition of “geometry” is very limiting and doesn’t capture all existing structures in document data!</p>
</section>
<section id="generalization-to-graph-and-manifold-embedding" class="level2">
<h2 class="anchored" data-anchor-id="generalization-to-graph-and-manifold-embedding">Generalization to graph and manifold embedding</h2>
<p>Arora et. al.&nbsp;in their work, <a href="https://arxiv.org/abs/1502.03520">RAND-WALK: A latent variable model approach to word embeddings</a>&nbsp;with generative modelling approach, provided more justifications for relations between PMI, Word2Vec and GloVe. <a href="https://arxiv.org/abs/1509.05808">Hashimoto-Alvarez-Melis</a>&nbsp;considered the task of word embedding as<em>metric recovery</em>. That is, given a word embedding <span class="math inline">\(v\_{w\_1}, \cdots, v\_{w\_m}\)</span> over a document with <span class="math inline">\(s\)</span> sentences with total number <span class="math inline">\(m\)</span> words and <span class="math inline">\(n\)</span> vocabulary (unique words), one can view <span class="math inline">\(p(c\_j|w\_i)\)</span> (where context is a word as well and there’s no separate context embedding vectors, such as SGNS throwing away the context vectors) as a Markov (Gaussian) random walk <span class="math inline">\(X\_1, \cdots, X\_m\)</span> with transition function</p>
<p><span class="math inline">\(p(X\_t = v\_{w\_j} | X\_{t-1}=v\_{w\_i}) = \dfrac{\exp(- \|v\_{w\_i} - v\_{w\_j} \|\_2^2)}{\sum\_{k=1}^n \exp(- \|v\_{w\_i} - v\_{w\_k} \|\_2^2)}\)</span></p>
<p>then there exists a sequence <span class="math inline">\(a\_i^m, b\_j^m\)</span> such that in probability, as <span class="math inline">\(m \to \infty,\)</span></p>
<p><span class="math inline">\(-\log(C\_{ij}) - a\_i^m \stackrel{p}{\longrightarrow} \|v\_{w\_i} - v\_{w\_j} \|\_2^2 + b\_j^m\)</span></p>
<p>where <span class="math inline">\(C = [C\_{ij}]\)</span> is the co-occurence matrix over our document. (matrix version of earlier <span class="math inline">\(D\)</span> with contexts as words). This described<strong>log-linear relation between co-occurences and distance.</strong>The metric recovery holds in more general setting for random walk over<em>unweighted directed graphs and data manifold.</em>Intuitively,</p>
<p><span class="math inline">\(-\log(\text{co-occurence}) \stackrel{\text{converges}}{\longrightarrow} \text{geodesic}(v\_{w\_i}, v\_{w\_j})^2\)</span></p>
<p>for some meaning of convergence and distance/geodesic.</p>
</section>
<section id="hyperbolic-embedding" class="level2">
<h2 class="anchored" data-anchor-id="hyperbolic-embedding">Hyperbolic embedding</h2>
<p>We can view words as symbolic data and try to represent their relations with graphs. To learn a representation of symbolic data with hierarchical relations (with existence of power-law distribution), one well-known approach is embedding the data in a<strong>non-Euclidean space</strong>, such as <span class="math inline">\(d\)</span>-dimensional<em>Poincaré ball</em><span class="math inline">\(\mathbb{B}^d = \{x \in \mathbb{R}^d | \|x\|\_2 &lt; 1\}\)</span> (or complex upper half-plane <span class="math inline">\(\mathbb{H}\)</span>) which is the Euclidean <span class="math inline">\(d\)</span> dimensional ball equipped with (non-Euclidean) Riemannian metric. In two closely published papers <a href="https://arxiv.org/abs/1705.10359">Chamberlain et. al</a> studied hyperbolic embedding for <span class="math inline">\(2\)</span>-dimensional Poincaré ball and <a href="https://arxiv.org/abs/1705.08039">Nickel-Kiela</a> for a general <span class="math inline">\(d\)</span>-dimensional Poincaré ball. They examined such embeddings for different types of symbolic data such as text and network graphs and showed improvements as well as the ability of capturing more information in lower dimension because hyperbolic spaces are richer than flat Euclidean spaces.</p>
</section>
<section id="encode-decoder" class="level2">
<h2 class="anchored" data-anchor-id="encode-decoder">Encode-Decoder</h2>
<p>Another line of ideas is related to encode-decoder (seq2seq) approach where either encoder or decoder can be any of Convolutional NN or Recurrent NN such as LSTM or GRU. The basic idea is to encode a sequence of data (sentences) and try to reconstruct them back with decoder. In the meantime, compact representations are constructed. One such successful approach has been introduced by Kiros et. al.&nbsp;in their <a href="https://arxiv.org/abs/1506.06726">Skip-Thought Vectors</a>&nbsp;with GRU as encoder and decoder. Given a tuple of sentences <span class="math inline">\((s\_{i-1}, s\_i, s\_{i+1}),\)</span> let <span class="math inline">\(w\_i^t\)</span> be the <span class="math inline">\(t\)</span>-th word for sentence <span class="math inline">\(s\_i\)</span> and let <span class="math inline">\(v\_{w\_i^t}\)</span> be its embedding. &nbsp;The objective is to maximize the log-probabilities for the<strong>forward and backward sentences conditioned on the encoder representation</strong>:</p>
<p><span class="math inline">\(\sum\_t \log p(w\_{i+1}^t | w\_{i+1}^{&lt; t}, h\_i) +\sum\_t \log p(w\_{i-1}^t | w\_{i-1}^{&lt; t}, h\_i)\)</span></p>
<p>where <span class="math inline">\(w\_{i+1}^{&lt; t}\)</span> is the sequence of words in sentence <span class="math inline">\(s\_i\)</span> coming before the <span class="math inline">\(t\)</span>-th words and <span class="math inline">\(h\_i\)</span> is the hidden state of the encoder GRU.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Thanks for reading this post! &nbsp;I’m still learning and will try to update this post if I find more interesting ideas. If you have any thoughts, please comment below.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/ehsanmkermani\.com");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<script src="https://utteranc.es/client.js" repo="ehsanmok/blog-comments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>© 2025 Ehsan M. Kermani</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>